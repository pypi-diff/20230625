# Comparing `tmp/pyqlib-0.9.1-cp38-cp38-win_amd64.whl.zip` & `tmp/pyqlib-0.9.2-cp38-cp38-win_amd64.whl.zip`

## zipinfo {}

```diff
@@ -1,233 +1,235 @@
-Zip file size: 640042 bytes, number of entries: 231
--rw-rw-rw-  2.0 fat    11959 b- defN 23-Jan-29 10:55 qlib/__init__.py
--rw-rw-rw-  2.0 fat    18102 b- defN 23-Jan-29 10:55 qlib/config.py
--rw-rw-rw-  2.0 fat      521 b- defN 23-Jan-29 10:55 qlib/constant.py
--rw-rw-rw-  2.0 fat     7864 b- defN 23-Jan-29 10:55 qlib/log.py
--rw-rw-rw-  2.0 fat     2058 b- defN 23-Jan-29 10:55 qlib/typehint.py
--rw-rw-rw-  2.0 fat    12144 b- defN 23-Jan-29 10:55 qlib/backtest/__init__.py
--rw-rw-rw-  2.0 fat    18379 b- defN 23-Jan-29 10:55 qlib/backtest/account.py
--rw-rw-rw-  2.0 fat     4142 b- defN 23-Jan-29 10:55 qlib/backtest/backtest.py
--rw-rw-rw-  2.0 fat    22447 b- defN 23-Jan-29 10:55 qlib/backtest/decision.py
--rw-rw-rw-  2.0 fat    45054 b- defN 23-Jan-29 10:55 qlib/backtest/exchange.py
--rw-rw-rw-  2.0 fat    27198 b- defN 23-Jan-29 10:55 qlib/backtest/executor.py
--rw-rw-rw-  2.0 fat    23892 b- defN 23-Jan-29 10:55 qlib/backtest/high_performance_ds.py
--rw-rw-rw-  2.0 fat    20569 b- defN 23-Jan-29 10:55 qlib/backtest/position.py
--rw-rw-rw-  2.0 fat    15326 b- defN 23-Jan-29 10:55 qlib/backtest/profit_attribution.py
--rw-rw-rw-  2.0 fat    28201 b- defN 23-Jan-29 10:55 qlib/backtest/report.py
--rw-rw-rw-  2.0 fat     4108 b- defN 23-Jan-29 10:55 qlib/backtest/signal.py
--rw-rw-rw-  2.0 fat    10812 b- defN 23-Jan-29 10:55 qlib/backtest/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-29 10:55 qlib/contrib/__init__.py
--rw-rw-rw-  2.0 fat    14471 b- defN 23-Jan-29 10:55 qlib/contrib/evaluate.py
--rw-rw-rw-  2.0 fat     6639 b- defN 23-Jan-29 10:55 qlib/contrib/evaluate_portfolio.py
--rw-rw-rw-  2.0 fat     1105 b- defN 23-Jan-29 10:55 qlib/contrib/torch.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-29 10:55 qlib/contrib/data/__init__.py
--rw-rw-rw-  2.0 fat     2185 b- defN 23-Jan-29 10:55 qlib/contrib/data/data.py
--rw-rw-rw-  2.0 fat    13957 b- defN 23-Jan-29 10:55 qlib/contrib/data/dataset.py
--rw-rw-rw-  2.0 fat    20635 b- defN 23-Jan-29 10:55 qlib/contrib/data/handler.py
--rw-rw-rw-  2.0 fat    19031 b- defN 23-Jan-29 10:55 qlib/contrib/data/highfreq_handler.py
--rw-rw-rw-  2.0 fat     3151 b- defN 23-Jan-29 10:55 qlib/contrib/data/highfreq_processor.py
--rw-rw-rw-  2.0 fat    12540 b- defN 23-Jan-29 10:55 qlib/contrib/data/highfreq_provider.py
--rw-rw-rw-  2.0 fat     4571 b- defN 23-Jan-29 10:55 qlib/contrib/data/processor.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-29 10:55 qlib/contrib/data/utils/__init__.py
--rw-rw-rw-  2.0 fat     7163 b- defN 23-Jan-29 10:55 qlib/contrib/data/utils/sepdf.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-29 10:55 qlib/contrib/eva/__init__.py
--rw-rw-rw-  2.0 fat     6847 b- defN 23-Jan-29 10:55 qlib/contrib/eva/alpha.py
--rw-rw-rw-  2.0 fat      207 b- defN 23-Jan-29 10:55 qlib/contrib/meta/__init__.py
--rw-rw-rw-  2.0 fat      219 b- defN 23-Jan-29 10:55 qlib/contrib/meta/data_selection/__init__.py
--rw-rw-rw-  2.0 fat    14695 b- defN 23-Jan-29 10:55 qlib/contrib/meta/data_selection/dataset.py
--rw-rw-rw-  2.0 fat     6496 b- defN 23-Jan-29 10:55 qlib/contrib/meta/data_selection/model.py
--rw-rw-rw-  2.0 fat     2824 b- defN 23-Jan-29 10:55 qlib/contrib/meta/data_selection/net.py
--rw-rw-rw-  2.0 fat     3506 b- defN 23-Jan-29 10:55 qlib/contrib/meta/data_selection/utils.py
--rw-rw-rw-  2.0 fat     1754 b- defN 23-Jan-29 10:55 qlib/contrib/model/__init__.py
--rw-rw-rw-  2.0 fat     3878 b- defN 23-Jan-29 10:55 qlib/contrib/model/catboost_model.py
--rw-rw-rw-  2.0 fat    12459 b- defN 23-Jan-29 10:55 qlib/contrib/model/double_ensemble.py
--rw-rw-rw-  2.0 fat     5061 b- defN 23-Jan-29 10:55 qlib/contrib/model/gbdt.py
--rw-rw-rw-  2.0 fat     6810 b- defN 23-Jan-29 10:55 qlib/contrib/model/highfreq_gdbt_model.py
--rw-rw-rw-  2.0 fat     3700 b- defN 23-Jan-29 10:55 qlib/contrib/model/linear.py
--rw-rw-rw-  2.0 fat    28733 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_adarnn.py
--rw-rw-rw-  2.0 fat    22112 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_add.py
--rw-rw-rw-  2.0 fat    11690 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_alstm.py
--rw-rw-rw-  2.0 fat    11925 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_alstm_ts.py
--rw-rw-rw-  2.0 fat    13097 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_gats.py
--rw-rw-rw-  2.0 fat    13548 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_gats_ts.py
--rw-rw-rw-  2.0 fat     9973 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_gru.py
--rw-rw-rw-  2.0 fat    10187 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_gru_ts.py
--rw-rw-rw-  2.0 fat    19174 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_hist.py
--rw-rw-rw-  2.0 fat    16293 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_igmtf.py
--rw-rw-rw-  2.0 fat    11054 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_localformer.py
--rw-rw-rw-  2.0 fat    10377 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_localformer_ts.py
--rw-rw-rw-  2.0 fat     9728 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_lstm.py
--rw-rw-rw-  2.0 fat     9974 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_lstm_ts.py
--rw-rw-rw-  2.0 fat    17924 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_nn.py
--rw-rw-rw-  2.0 fat    16377 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_sfm.py
--rw-rw-rw-  2.0 fat    23506 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_tabnet.py
--rw-rw-rw-  2.0 fat     9903 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_tcn.py
--rw-rw-rw-  2.0 fat     9374 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_tcn_ts.py
--rw-rw-rw-  2.0 fat    14727 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_tcts.py
--rw-rw-rw-  2.0 fat    35174 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_tra.py
--rw-rw-rw-  2.0 fat     9881 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_transformer.py
--rw-rw-rw-  2.0 fat     9179 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_transformer_ts.py
--rw-rw-rw-  2.0 fat     1234 b- defN 23-Jan-29 10:55 qlib/contrib/model/pytorch_utils.py
--rw-rw-rw-  2.0 fat     2682 b- defN 23-Jan-29 10:55 qlib/contrib/model/tcn.py
--rw-rw-rw-  2.0 fat     3170 b- defN 23-Jan-29 10:55 qlib/contrib/model/xgboost.py
--rw-rw-rw-  2.0 fat      607 b- defN 23-Jan-29 10:55 qlib/contrib/online/__init__.py
--rw-rw-rw-  2.0 fat     5636 b- defN 23-Jan-29 10:55 qlib/contrib/online/manager.py
--rw-rw-rw-  2.0 fat     1149 b- defN 23-Jan-29 10:55 qlib/contrib/online/online_model.py
--rw-rw-rw-  2.0 fat    13138 b- defN 23-Jan-29 10:55 qlib/contrib/online/operator.py
--rw-rw-rw-  2.0 fat     3057 b- defN 23-Jan-29 10:55 qlib/contrib/online/user.py
--rw-rw-rw-  2.0 fat     3177 b- defN 23-Jan-29 10:55 qlib/contrib/online/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-29 10:55 qlib/contrib/ops/__init__.py
--rw-rw-rw-  2.0 fat     8202 b- defN 23-Jan-29 10:55 qlib/contrib/ops/high_freq.py
--rw-rw-rw-  2.0 fat      367 b- defN 23-Jan-29 10:55 qlib/contrib/report/__init__.py
--rw-rw-rw-  2.0 fat    11574 b- defN 23-Jan-29 10:55 qlib/contrib/report/graph.py
--rw-rw-rw-  2.0 fat     2541 b- defN 23-Jan-29 10:55 qlib/contrib/report/utils.py
--rw-rw-rw-  2.0 fat      185 b- defN 23-Jan-29 10:55 qlib/contrib/report/analysis_model/__init__.py
--rw-rw-rw-  2.0 fat    11719 b- defN 23-Jan-29 10:55 qlib/contrib/report/analysis_model/analysis_model_performance.py
--rw-rw-rw-  2.0 fat      415 b- defN 23-Jan-29 10:55 qlib/contrib/report/analysis_position/__init__.py
--rw-rw-rw-  2.0 fat     9725 b- defN 23-Jan-29 10:55 qlib/contrib/report/analysis_position/cumulative_return.py
--rw-rw-rw-  2.0 fat     6734 b- defN 23-Jan-29 10:55 qlib/contrib/report/analysis_position/parse_position.py
--rw-rw-rw-  2.0 fat     4450 b- defN 23-Jan-29 10:55 qlib/contrib/report/analysis_position/rank_label.py
--rw-rw-rw-  2.0 fat     8661 b- defN 23-Jan-29 10:55 qlib/contrib/report/analysis_position/report.py
--rw-rw-rw-  2.0 fat    10946 b- defN 23-Jan-29 10:55 qlib/contrib/report/analysis_position/risk_analysis.py
--rw-rw-rw-  2.0 fat     2641 b- defN 23-Jan-29 10:55 qlib/contrib/report/analysis_position/score_ic.py
--rw-rw-rw-  2.0 fat      131 b- defN 23-Jan-29 10:55 qlib/contrib/report/data/__init__.py
--rw-rw-rw-  2.0 fat     6810 b- defN 23-Jan-29 10:55 qlib/contrib/report/data/ana.py
--rw-rw-rw-  2.0 fat      967 b- defN 23-Jan-29 10:55 qlib/contrib/report/data/base.py
--rw-rw-rw-  2.0 fat      540 b- defN 23-Jan-29 10:55 qlib/contrib/strategy/__init__.py
--rw-rw-rw-  2.0 fat     3928 b- defN 23-Jan-29 10:55 qlib/contrib/strategy/cost_control.py
--rw-rw-rw-  2.0 fat     8478 b- defN 23-Jan-29 10:55 qlib/contrib/strategy/order_generator.py
--rw-rw-rw-  2.0 fat    30048 b- defN 23-Jan-29 10:55 qlib/contrib/strategy/rule_strategy.py
--rw-rw-rw-  2.0 fat    23023 b- defN 23-Jan-29 10:55 qlib/contrib/strategy/signal_strategy.py
--rw-rw-rw-  2.0 fat      295 b- defN 23-Jan-29 10:55 qlib/contrib/strategy/optimizer/__init__.py
--rw-rw-rw-  2.0 fat      326 b- defN 23-Jan-29 10:55 qlib/contrib/strategy/optimizer/base.py
--rw-rw-rw-  2.0 fat     6717 b- defN 23-Jan-29 10:55 qlib/contrib/strategy/optimizer/enhanced_indexing.py
--rw-rw-rw-  2.0 fat     8906 b- defN 23-Jan-29 10:55 qlib/contrib/strategy/optimizer/optimizer.py
--rw-rw-rw-  2.0 fat       37 b- defN 23-Jan-29 10:55 qlib/contrib/tuner/__init__.py
--rw-rw-rw-  2.0 fat     3720 b- defN 23-Jan-29 10:55 qlib/contrib/tuner/config.py
--rw-rw-rw-  2.0 fat      858 b- defN 23-Jan-29 10:55 qlib/contrib/tuner/launcher.py
--rw-rw-rw-  2.0 fat     3540 b- defN 23-Jan-29 10:55 qlib/contrib/tuner/pipeline.py
--rw-rw-rw-  2.0 fat      453 b- defN 23-Jan-29 10:55 qlib/contrib/tuner/space.py
--rw-rw-rw-  2.0 fat     8231 b- defN 23-Jan-29 10:55 qlib/contrib/tuner/tuner.py
--rw-rw-rw-  2.0 fat      213 b- defN 23-Jan-29 10:55 qlib/contrib/workflow/__init__.py
--rw-rw-rw-  2.0 fat     3454 b- defN 23-Jan-29 10:55 qlib/contrib/workflow/record_temp.py
--rw-rw-rw-  2.0 fat     1476 b- defN 23-Jan-29 10:55 qlib/data/__init__.py
--rw-rw-rw-  2.0 fat     8668 b- defN 23-Jan-29 10:55 qlib/data/base.py
--rw-rw-rw-  2.0 fat    48454 b- defN 23-Jan-29 10:55 qlib/data/cache.py
--rw-rw-rw-  2.0 fat     3852 b- defN 23-Jan-29 10:55 qlib/data/client.py
--rw-rw-rw-  2.0 fat    50784 b- defN 23-Jan-29 10:55 qlib/data/data.py
--rw-rw-rw-  2.0 fat    14298 b- defN 23-Jan-29 10:55 qlib/data/filter.py
--rw-rw-rw-  2.0 fat      620 b- defN 23-Jan-29 10:55 qlib/data/inst_processor.py
--rw-rw-rw-  2.0 fat    47127 b- defN 23-Jan-29 10:55 qlib/data/ops.py
--rw-rw-rw-  2.0 fat     3304 b- defN 23-Jan-29 10:55 qlib/data/pit.py
--rw-rw-rw-  2.0 fat       75 b- defN 23-Jan-29 10:55 qlib/data/_libs/__init__.py
--rw-rw-rw-  2.0 fat   122880 b- defN 23-Jan-29 10:57 qlib/data/_libs/expanding.cp38-win_amd64.pyd
--rw-rw-rw-  2.0 fat     4303 b- defN 23-Jan-29 10:55 qlib/data/_libs/expanding.pyx
--rw-rw-rw-  2.0 fat    86016 b- defN 23-Jan-29 10:57 qlib/data/_libs/rolling.cp38-win_amd64.pyd
--rw-rw-rw-  2.0 fat     6318 b- defN 23-Jan-29 10:55 qlib/data/_libs/rolling.pyx
--rw-rw-rw-  2.0 fat    27922 b- defN 23-Jan-29 10:55 qlib/data/dataset/__init__.py
--rw-rw-rw-  2.0 fat    25489 b- defN 23-Jan-29 10:55 qlib/data/dataset/handler.py
--rw-rw-rw-  2.0 fat    12421 b- defN 23-Jan-29 10:55 qlib/data/dataset/loader.py
--rw-rw-rw-  2.0 fat    13267 b- defN 23-Jan-29 10:55 qlib/data/dataset/processor.py
--rw-rw-rw-  2.0 fat     6507 b- defN 23-Jan-29 10:55 qlib/data/dataset/storage.py
--rw-rw-rw-  2.0 fat     4281 b- defN 23-Jan-29 10:55 qlib/data/dataset/utils.py
--rw-rw-rw-  2.0 fat      781 b- defN 23-Jan-29 10:55 qlib/data/dataset/weight.py
--rw-rw-rw-  2.0 fat      276 b- defN 23-Jan-29 10:55 qlib/data/storage/__init__.py
--rw-rw-rw-  2.0 fat    14768 b- defN 23-Jan-29 10:55 qlib/data/storage/file_storage.py
--rw-rw-rw-  2.0 fat    15156 b- defN 23-Jan-29 10:55 qlib/data/storage/storage.py
--rw-rw-rw-  2.0 fat      158 b- defN 23-Jan-29 10:55 qlib/model/__init__.py
--rw-rw-rw-  2.0 fat     3881 b- defN 23-Jan-29 10:55 qlib/model/base.py
--rw-rw-rw-  2.0 fat    23384 b- defN 23-Jan-29 10:55 qlib/model/trainer.py
--rw-rw-rw-  2.0 fat      605 b- defN 23-Jan-29 10:55 qlib/model/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-29 10:55 qlib/model/ens/__init__.py
--rw-rw-rw-  2.0 fat     4669 b- defN 23-Jan-29 10:55 qlib/model/ens/ensemble.py
--rw-rw-rw-  2.0 fat     4027 b- defN 23-Jan-29 10:55 qlib/model/ens/group.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-29 10:55 qlib/model/interpret/__init__.py
--rw-rw-rw-  2.0 fat     1178 b- defN 23-Jan-29 10:55 qlib/model/interpret/base.py
--rw-rw-rw-  2.0 fat      190 b- defN 23-Jan-29 10:55 qlib/model/meta/__init__.py
--rw-rw-rw-  2.0 fat     2856 b- defN 23-Jan-29 10:55 qlib/model/meta/dataset.py
--rw-rw-rw-  2.0 fat     2430 b- defN 23-Jan-29 10:55 qlib/model/meta/model.py
--rw-rw-rw-  2.0 fat     1708 b- defN 23-Jan-29 10:55 qlib/model/meta/task.py
--rw-rw-rw-  2.0 fat      351 b- defN 23-Jan-29 10:55 qlib/model/riskmodel/__init__.py
--rw-rw-rw-  2.0 fat     5182 b- defN 23-Jan-29 10:55 qlib/model/riskmodel/base.py
--rw-rw-rw-  2.0 fat     3290 b- defN 23-Jan-29 10:55 qlib/model/riskmodel/poet.py
--rw-rw-rw-  2.0 fat    10702 b- defN 23-Jan-29 10:55 qlib/model/riskmodel/shrink.py
--rw-rw-rw-  2.0 fat     3895 b- defN 23-Jan-29 10:55 qlib/model/riskmodel/structured.py
--rw-rw-rw-  2.0 fat      347 b- defN 23-Jan-29 10:55 qlib/rl/__init__.py
--rw-rw-rw-  2.0 fat     1165 b- defN 23-Jan-29 10:55 qlib/rl/aux_info.py
--rw-rw-rw-  2.0 fat     5376 b- defN 23-Jan-29 10:55 qlib/rl/interpreter.py
--rw-rw-rw-  2.0 fat     2788 b- defN 23-Jan-29 10:55 qlib/rl/reward.py
--rw-rw-rw-  2.0 fat      351 b- defN 23-Jan-29 10:55 qlib/rl/seed.py
--rw-rw-rw-  2.0 fat     3106 b- defN 23-Jan-29 10:55 qlib/rl/simulator.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-29 10:55 qlib/rl/contrib/__init__.py
--rw-rw-rw-  2.0 fat    13632 b- defN 23-Jan-29 10:55 qlib/rl/contrib/backtest.py
--rw-rw-rw-  2.0 fat     3448 b- defN 23-Jan-29 10:55 qlib/rl/contrib/naive_config_parser.py
--rw-rw-rw-  2.0 fat     9086 b- defN 23-Jan-29 10:55 qlib/rl/contrib/train_onpolicy.py
--rw-rw-rw-  2.0 fat      865 b- defN 23-Jan-29 10:55 qlib/rl/contrib/utils.py
--rw-rw-rw-  2.0 fat      253 b- defN 23-Jan-29 10:55 qlib/rl/data/__init__.py
--rw-rw-rw-  2.0 fat     1819 b- defN 23-Jan-29 10:55 qlib/rl/data/base.py
--rw-rw-rw-  2.0 fat     6358 b- defN 23-Jan-29 10:55 qlib/rl/data/integration.py
--rw-rw-rw-  2.0 fat     4595 b- defN 23-Jan-29 10:55 qlib/rl/data/native.py
--rw-rw-rw-  2.0 fat    10824 b- defN 23-Jan-29 10:55 qlib/rl/data/pickle_styled.py
--rw-rw-rw-  2.0 fat     1046 b- defN 23-Jan-29 10:55 qlib/rl/order_execution/__init__.py
--rw-rw-rw-  2.0 fat     9412 b- defN 23-Jan-29 10:55 qlib/rl/order_execution/interpreter.py
--rw-rw-rw-  2.0 fat     4970 b- defN 23-Jan-29 10:55 qlib/rl/order_execution/network.py
--rw-rw-rw-  2.0 fat     5516 b- defN 23-Jan-29 10:55 qlib/rl/order_execution/policy.py
--rw-rw-rw-  2.0 fat     1752 b- defN 23-Jan-29 10:55 qlib/rl/order_execution/reward.py
--rw-rw-rw-  2.0 fat     4908 b- defN 23-Jan-29 10:55 qlib/rl/order_execution/simulator_qlib.py
--rw-rw-rw-  2.0 fat    13727 b- defN 23-Jan-29 10:55 qlib/rl/order_execution/simulator_simple.py
--rw-rw-rw-  2.0 fat     3791 b- defN 23-Jan-29 10:55 qlib/rl/order_execution/state.py
--rw-rw-rw-  2.0 fat    20828 b- defN 23-Jan-29 10:55 qlib/rl/order_execution/strategy.py
--rw-rw-rw-  2.0 fat     1973 b- defN 23-Jan-29 10:55 qlib/rl/order_execution/utils.py
--rw-rw-rw-  2.0 fat      159 b- defN 23-Jan-29 10:55 qlib/rl/strategy/__init__.py
--rw-rw-rw-  2.0 fat     1030 b- defN 23-Jan-29 10:55 qlib/rl/strategy/single_order.py
--rw-rw-rw-  2.0 fat      483 b- defN 23-Jan-29 10:55 qlib/rl/trainer/__init__.py
--rw-rw-rw-  2.0 fat     3768 b- defN 23-Jan-29 10:55 qlib/rl/trainer/api.py
--rw-rw-rw-  2.0 fat    11867 b- defN 23-Jan-29 10:55 qlib/rl/trainer/callbacks.py
--rw-rw-rw-  2.0 fat    13831 b- defN 23-Jan-29 10:55 qlib/rl/trainer/trainer.py
--rw-rw-rw-  2.0 fat    10103 b- defN 23-Jan-29 10:55 qlib/rl/trainer/vessel.py
--rw-rw-rw-  2.0 fat      548 b- defN 23-Jan-29 10:55 qlib/rl/utils/__init__.py
--rw-rw-rw-  2.0 fat     6534 b- defN 23-Jan-29 10:55 qlib/rl/utils/data_queue.py
--rw-rw-rw-  2.0 fat    10073 b- defN 23-Jan-29 10:55 qlib/rl/utils/env_wrapper.py
--rw-rw-rw-  2.0 fat    13736 b- defN 23-Jan-29 10:55 qlib/rl/utils/finite_env.py
--rw-rw-rw-  2.0 fat    19057 b- defN 23-Jan-29 10:55 qlib/rl/utils/log.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-29 10:55 qlib/run/__init__.py
--rw-rw-rw-  2.0 fat      185 b- defN 23-Jan-29 10:55 qlib/run/get_data.py
--rw-rw-rw-  2.0 fat       75 b- defN 23-Jan-29 10:55 qlib/strategy/__init__.py
--rw-rw-rw-  2.0 fat    11451 b- defN 23-Jan-29 10:55 qlib/strategy/base.py
--rw-rw-rw-  2.0 fat    12511 b- defN 23-Jan-29 10:55 qlib/tests/__init__.py
--rw-rw-rw-  2.0 fat     5001 b- defN 23-Jan-29 10:55 qlib/tests/config.py
--rw-rw-rw-  2.0 fat     6901 b- defN 23-Jan-29 10:55 qlib/tests/data.py
--rw-rw-rw-  2.0 fat    35054 b- defN 23-Jan-29 10:55 qlib/utils/__init__.py
--rw-rw-rw-  2.0 fat     1456 b- defN 23-Jan-29 10:55 qlib/utils/data.py
--rw-rw-rw-  2.0 fat      457 b- defN 23-Jan-29 10:55 qlib/utils/exceptions.py
--rw-rw-rw-  2.0 fat     5928 b- defN 23-Jan-29 10:55 qlib/utils/file.py
--rw-rw-rw-  2.0 fat    22537 b- defN 23-Jan-29 10:55 qlib/utils/index_data.py
--rw-rw-rw-  2.0 fat     3435 b- defN 23-Jan-29 10:55 qlib/utils/objm.py
--rw-rw-rw-  2.0 fat     9433 b- defN 23-Jan-29 10:55 qlib/utils/paral.py
--rw-rw-rw-  2.0 fat     9626 b- defN 23-Jan-29 10:55 qlib/utils/resam.py
--rw-rw-rw-  2.0 fat     6263 b- defN 23-Jan-29 10:55 qlib/utils/serial.py
--rw-rw-rw-  2.0 fat    12155 b- defN 23-Jan-29 10:55 qlib/utils/time.py
--rw-rw-rw-  2.0 fat    25250 b- defN 23-Jan-29 10:55 qlib/workflow/__init__.py
--rw-rw-rw-  2.0 fat     2035 b- defN 23-Jan-29 10:55 qlib/workflow/cli.py
--rw-rw-rw-  2.0 fat    15191 b- defN 23-Jan-29 10:55 qlib/workflow/exp.py
--rw-rw-rw-  2.0 fat    18007 b- defN 23-Jan-29 10:55 qlib/workflow/expm.py
--rw-rw-rw-  2.0 fat    22416 b- defN 23-Jan-29 10:55 qlib/workflow/record_temp.py
--rw-rw-rw-  2.0 fat    18255 b- defN 23-Jan-29 10:55 qlib/workflow/recorder.py
--rw-rw-rw-  2.0 fat     1663 b- defN 23-Jan-29 10:55 qlib/workflow/utils.py
--rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-29 10:55 qlib/workflow/online/__init__.py
--rw-rw-rw-  2.0 fat    17748 b- defN 23-Jan-29 10:55 qlib/workflow/online/manager.py
--rw-rw-rw-  2.0 fat     8620 b- defN 23-Jan-29 10:55 qlib/workflow/online/strategy.py
--rw-rw-rw-  2.0 fat    10884 b- defN 23-Jan-29 10:55 qlib/workflow/online/update.py
--rw-rw-rw-  2.0 fat     6662 b- defN 23-Jan-29 10:55 qlib/workflow/online/utils.py
--rw-rw-rw-  2.0 fat      548 b- defN 23-Jan-29 10:55 qlib/workflow/task/__init__.py
--rw-rw-rw-  2.0 fat    10315 b- defN 23-Jan-29 10:55 qlib/workflow/task/collect.py
--rw-rw-rw-  2.0 fat    12119 b- defN 23-Jan-29 10:55 qlib/workflow/task/gen.py
--rw-rw-rw-  2.0 fat    18968 b- defN 23-Jan-29 10:55 qlib/workflow/task/manage.py
--rw-rw-rw-  2.0 fat     8890 b- defN 23-Jan-29 10:55 qlib/workflow/task/utils.py
--rw-rw-rw-  2.0 fat     1162 b- defN 23-Jan-29 10:57 pyqlib-0.9.1.dist-info/LICENSE
--rw-rw-rw-  2.0 fat    38902 b- defN 23-Jan-29 10:57 pyqlib-0.9.1.dist-info/METADATA
--rw-rw-rw-  2.0 fat      100 b- defN 23-Jan-29 10:57 pyqlib-0.9.1.dist-info/WHEEL
--rw-rw-rw-  2.0 fat       48 b- defN 23-Jan-29 10:56 pyqlib-0.9.1.dist-info/entry_points.txt
--rw-rw-rw-  2.0 fat        5 b- defN 23-Jan-29 10:56 pyqlib-0.9.1.dist-info/top_level.txt
-?rw-rw-r--  2.0 fat    19980 b- defN 23-Jan-29 10:57 pyqlib-0.9.1.dist-info/RECORD
-231 files, 2224185 bytes uncompressed, 608746 bytes compressed:  72.6%
+Zip file size: 653261 bytes, number of entries: 233
+-rw-rw-rw-  2.0 fat    11959 b- defN 23-Jun-25 16:04 qlib/__init__.py
+-rw-rw-rw-  2.0 fat    18131 b- defN 23-Jun-25 16:04 qlib/config.py
+-rw-rw-rw-  2.0 fat      521 b- defN 23-Jun-25 16:04 qlib/constant.py
+-rw-rw-rw-  2.0 fat     7864 b- defN 23-Jun-25 16:04 qlib/log.py
+-rw-rw-rw-  2.0 fat     2058 b- defN 23-Jun-25 16:04 qlib/typehint.py
+-rw-rw-rw-  2.0 fat    12165 b- defN 23-Jun-25 16:04 qlib/backtest/__init__.py
+-rw-rw-rw-  2.0 fat    18416 b- defN 23-Jun-25 16:04 qlib/backtest/account.py
+-rw-rw-rw-  2.0 fat     4149 b- defN 23-Jun-25 16:04 qlib/backtest/backtest.py
+-rw-rw-rw-  2.0 fat    22466 b- defN 23-Jun-25 16:04 qlib/backtest/decision.py
+-rw-rw-rw-  2.0 fat    45143 b- defN 23-Jun-25 16:04 qlib/backtest/exchange.py
+-rw-rw-rw-  2.0 fat    27240 b- defN 23-Jun-25 16:04 qlib/backtest/executor.py
+-rw-rw-rw-  2.0 fat    23892 b- defN 23-Jun-25 16:04 qlib/backtest/high_performance_ds.py
+-rw-rw-rw-  2.0 fat    20612 b- defN 23-Jun-25 16:04 qlib/backtest/position.py
+-rw-rw-rw-  2.0 fat    15326 b- defN 23-Jun-25 16:04 qlib/backtest/profit_attribution.py
+-rw-rw-rw-  2.0 fat    28314 b- defN 23-Jun-25 16:04 qlib/backtest/report.py
+-rw-rw-rw-  2.0 fat     4108 b- defN 23-Jun-25 16:04 qlib/backtest/signal.py
+-rw-rw-rw-  2.0 fat    10826 b- defN 23-Jun-25 16:04 qlib/backtest/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-25 16:04 qlib/contrib/__init__.py
+-rw-rw-rw-  2.0 fat    14471 b- defN 23-Jun-25 16:04 qlib/contrib/evaluate.py
+-rw-rw-rw-  2.0 fat     6639 b- defN 23-Jun-25 16:04 qlib/contrib/evaluate_portfolio.py
+-rw-rw-rw-  2.0 fat     1105 b- defN 23-Jun-25 16:04 qlib/contrib/torch.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-25 16:04 qlib/contrib/data/__init__.py
+-rw-rw-rw-  2.0 fat     2185 b- defN 23-Jun-25 16:04 qlib/contrib/data/data.py
+-rw-rw-rw-  2.0 fat    13957 b- defN 23-Jun-25 16:04 qlib/contrib/data/dataset.py
+-rw-rw-rw-  2.0 fat    20641 b- defN 23-Jun-25 16:04 qlib/contrib/data/handler.py
+-rw-rw-rw-  2.0 fat    19277 b- defN 23-Jun-25 16:04 qlib/contrib/data/highfreq_handler.py
+-rw-rw-rw-  2.0 fat     3151 b- defN 23-Jun-25 16:04 qlib/contrib/data/highfreq_processor.py
+-rw-rw-rw-  2.0 fat    12588 b- defN 23-Jun-25 16:04 qlib/contrib/data/highfreq_provider.py
+-rw-rw-rw-  2.0 fat     4571 b- defN 23-Jun-25 16:04 qlib/contrib/data/processor.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-25 16:04 qlib/contrib/data/utils/__init__.py
+-rw-rw-rw-  2.0 fat     7163 b- defN 23-Jun-25 16:04 qlib/contrib/data/utils/sepdf.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-25 16:04 qlib/contrib/eva/__init__.py
+-rw-rw-rw-  2.0 fat     6847 b- defN 23-Jun-25 16:04 qlib/contrib/eva/alpha.py
+-rw-rw-rw-  2.0 fat      207 b- defN 23-Jun-25 16:04 qlib/contrib/meta/__init__.py
+-rw-rw-rw-  2.0 fat      219 b- defN 23-Jun-25 16:04 qlib/contrib/meta/data_selection/__init__.py
+-rw-rw-rw-  2.0 fat    18148 b- defN 23-Jun-25 16:04 qlib/contrib/meta/data_selection/dataset.py
+-rw-rw-rw-  2.0 fat     6615 b- defN 23-Jun-25 16:04 qlib/contrib/meta/data_selection/model.py
+-rw-rw-rw-  2.0 fat     3098 b- defN 23-Jun-25 16:04 qlib/contrib/meta/data_selection/net.py
+-rw-rw-rw-  2.0 fat     4075 b- defN 23-Jun-25 16:04 qlib/contrib/meta/data_selection/utils.py
+-rw-rw-rw-  2.0 fat     1754 b- defN 23-Jun-25 16:04 qlib/contrib/model/__init__.py
+-rw-rw-rw-  2.0 fat     3878 b- defN 23-Jun-25 16:04 qlib/contrib/model/catboost_model.py
+-rw-rw-rw-  2.0 fat    12459 b- defN 23-Jun-25 16:04 qlib/contrib/model/double_ensemble.py
+-rw-rw-rw-  2.0 fat     5061 b- defN 23-Jun-25 16:04 qlib/contrib/model/gbdt.py
+-rw-rw-rw-  2.0 fat     6810 b- defN 23-Jun-25 16:04 qlib/contrib/model/highfreq_gdbt_model.py
+-rw-rw-rw-  2.0 fat     4315 b- defN 23-Jun-25 16:04 qlib/contrib/model/linear.py
+-rw-rw-rw-  2.0 fat    28733 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_adarnn.py
+-rw-rw-rw-  2.0 fat    22112 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_add.py
+-rw-rw-rw-  2.0 fat    11690 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_alstm.py
+-rw-rw-rw-  2.0 fat    11925 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_alstm_ts.py
+-rw-rw-rw-  2.0 fat    13097 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_gats.py
+-rw-rw-rw-  2.0 fat    13548 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_gats_ts.py
+-rw-rw-rw-  2.0 fat     9973 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_gru.py
+-rw-rw-rw-  2.0 fat    10187 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_gru_ts.py
+-rw-rw-rw-  2.0 fat    19174 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_hist.py
+-rw-rw-rw-  2.0 fat    16293 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_igmtf.py
+-rw-rw-rw-  2.0 fat    16228 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_krnn.py
+-rw-rw-rw-  2.0 fat    11054 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_localformer.py
+-rw-rw-rw-  2.0 fat    10377 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_localformer_ts.py
+-rw-rw-rw-  2.0 fat     9728 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_lstm.py
+-rw-rw-rw-  2.0 fat     9974 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_lstm_ts.py
+-rw-rw-rw-  2.0 fat    17587 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_nn.py
+-rw-rw-rw-  2.0 fat    12042 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_sandwich.py
+-rw-rw-rw-  2.0 fat    16377 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_sfm.py
+-rw-rw-rw-  2.0 fat    23506 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_tabnet.py
+-rw-rw-rw-  2.0 fat     9903 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_tcn.py
+-rw-rw-rw-  2.0 fat     9468 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_tcn_ts.py
+-rw-rw-rw-  2.0 fat    14727 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_tcts.py
+-rw-rw-rw-  2.0 fat    35174 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_tra.py
+-rw-rw-rw-  2.0 fat     9881 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_transformer.py
+-rw-rw-rw-  2.0 fat     9179 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_transformer_ts.py
+-rw-rw-rw-  2.0 fat     1234 b- defN 23-Jun-25 16:04 qlib/contrib/model/pytorch_utils.py
+-rw-rw-rw-  2.0 fat     2682 b- defN 23-Jun-25 16:04 qlib/contrib/model/tcn.py
+-rw-rw-rw-  2.0 fat     3170 b- defN 23-Jun-25 16:04 qlib/contrib/model/xgboost.py
+-rw-rw-rw-  2.0 fat      607 b- defN 23-Jun-25 16:04 qlib/contrib/online/__init__.py
+-rw-rw-rw-  2.0 fat     5636 b- defN 23-Jun-25 16:04 qlib/contrib/online/manager.py
+-rw-rw-rw-  2.0 fat     1149 b- defN 23-Jun-25 16:04 qlib/contrib/online/online_model.py
+-rw-rw-rw-  2.0 fat    13138 b- defN 23-Jun-25 16:04 qlib/contrib/online/operator.py
+-rw-rw-rw-  2.0 fat     3057 b- defN 23-Jun-25 16:04 qlib/contrib/online/user.py
+-rw-rw-rw-  2.0 fat     3177 b- defN 23-Jun-25 16:04 qlib/contrib/online/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-25 16:04 qlib/contrib/ops/__init__.py
+-rw-rw-rw-  2.0 fat     8428 b- defN 23-Jun-25 16:04 qlib/contrib/ops/high_freq.py
+-rw-rw-rw-  2.0 fat      367 b- defN 23-Jun-25 16:04 qlib/contrib/report/__init__.py
+-rw-rw-rw-  2.0 fat    11574 b- defN 23-Jun-25 16:04 qlib/contrib/report/graph.py
+-rw-rw-rw-  2.0 fat     2541 b- defN 23-Jun-25 16:04 qlib/contrib/report/utils.py
+-rw-rw-rw-  2.0 fat      185 b- defN 23-Jun-25 16:04 qlib/contrib/report/analysis_model/__init__.py
+-rw-rw-rw-  2.0 fat    11719 b- defN 23-Jun-25 16:04 qlib/contrib/report/analysis_model/analysis_model_performance.py
+-rw-rw-rw-  2.0 fat      415 b- defN 23-Jun-25 16:04 qlib/contrib/report/analysis_position/__init__.py
+-rw-rw-rw-  2.0 fat     9725 b- defN 23-Jun-25 16:04 qlib/contrib/report/analysis_position/cumulative_return.py
+-rw-rw-rw-  2.0 fat     6734 b- defN 23-Jun-25 16:04 qlib/contrib/report/analysis_position/parse_position.py
+-rw-rw-rw-  2.0 fat     4450 b- defN 23-Jun-25 16:04 qlib/contrib/report/analysis_position/rank_label.py
+-rw-rw-rw-  2.0 fat     8661 b- defN 23-Jun-25 16:04 qlib/contrib/report/analysis_position/report.py
+-rw-rw-rw-  2.0 fat    10946 b- defN 23-Jun-25 16:04 qlib/contrib/report/analysis_position/risk_analysis.py
+-rw-rw-rw-  2.0 fat     2641 b- defN 23-Jun-25 16:04 qlib/contrib/report/analysis_position/score_ic.py
+-rw-rw-rw-  2.0 fat      131 b- defN 23-Jun-25 16:04 qlib/contrib/report/data/__init__.py
+-rw-rw-rw-  2.0 fat     6810 b- defN 23-Jun-25 16:04 qlib/contrib/report/data/ana.py
+-rw-rw-rw-  2.0 fat      967 b- defN 23-Jun-25 16:04 qlib/contrib/report/data/base.py
+-rw-rw-rw-  2.0 fat      540 b- defN 23-Jun-25 16:04 qlib/contrib/strategy/__init__.py
+-rw-rw-rw-  2.0 fat     3928 b- defN 23-Jun-25 16:04 qlib/contrib/strategy/cost_control.py
+-rw-rw-rw-  2.0 fat     8478 b- defN 23-Jun-25 16:04 qlib/contrib/strategy/order_generator.py
+-rw-rw-rw-  2.0 fat    30045 b- defN 23-Jun-25 16:04 qlib/contrib/strategy/rule_strategy.py
+-rw-rw-rw-  2.0 fat    23023 b- defN 23-Jun-25 16:04 qlib/contrib/strategy/signal_strategy.py
+-rw-rw-rw-  2.0 fat      295 b- defN 23-Jun-25 16:04 qlib/contrib/strategy/optimizer/__init__.py
+-rw-rw-rw-  2.0 fat      326 b- defN 23-Jun-25 16:04 qlib/contrib/strategy/optimizer/base.py
+-rw-rw-rw-  2.0 fat     6717 b- defN 23-Jun-25 16:04 qlib/contrib/strategy/optimizer/enhanced_indexing.py
+-rw-rw-rw-  2.0 fat     8906 b- defN 23-Jun-25 16:04 qlib/contrib/strategy/optimizer/optimizer.py
+-rw-rw-rw-  2.0 fat       37 b- defN 23-Jun-25 16:04 qlib/contrib/tuner/__init__.py
+-rw-rw-rw-  2.0 fat     3720 b- defN 23-Jun-25 16:04 qlib/contrib/tuner/config.py
+-rw-rw-rw-  2.0 fat      858 b- defN 23-Jun-25 16:04 qlib/contrib/tuner/launcher.py
+-rw-rw-rw-  2.0 fat     3540 b- defN 23-Jun-25 16:04 qlib/contrib/tuner/pipeline.py
+-rw-rw-rw-  2.0 fat      453 b- defN 23-Jun-25 16:04 qlib/contrib/tuner/space.py
+-rw-rw-rw-  2.0 fat     8231 b- defN 23-Jun-25 16:04 qlib/contrib/tuner/tuner.py
+-rw-rw-rw-  2.0 fat      213 b- defN 23-Jun-25 16:04 qlib/contrib/workflow/__init__.py
+-rw-rw-rw-  2.0 fat     3454 b- defN 23-Jun-25 16:04 qlib/contrib/workflow/record_temp.py
+-rw-rw-rw-  2.0 fat     1476 b- defN 23-Jun-25 16:04 qlib/data/__init__.py
+-rw-rw-rw-  2.0 fat     8668 b- defN 23-Jun-25 16:04 qlib/data/base.py
+-rw-rw-rw-  2.0 fat    48454 b- defN 23-Jun-25 16:04 qlib/data/cache.py
+-rw-rw-rw-  2.0 fat     3852 b- defN 23-Jun-25 16:04 qlib/data/client.py
+-rw-rw-rw-  2.0 fat    50822 b- defN 23-Jun-25 16:04 qlib/data/data.py
+-rw-rw-rw-  2.0 fat    14298 b- defN 23-Jun-25 16:04 qlib/data/filter.py
+-rw-rw-rw-  2.0 fat      620 b- defN 23-Jun-25 16:04 qlib/data/inst_processor.py
+-rw-rw-rw-  2.0 fat    47127 b- defN 23-Jun-25 16:04 qlib/data/ops.py
+-rw-rw-rw-  2.0 fat     3304 b- defN 23-Jun-25 16:04 qlib/data/pit.py
+-rw-rw-rw-  2.0 fat       75 b- defN 23-Jun-25 16:04 qlib/data/_libs/__init__.py
+-rw-rw-rw-  2.0 fat   122880 b- defN 23-Jun-25 16:06 qlib/data/_libs/expanding.cp38-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     4303 b- defN 23-Jun-25 16:04 qlib/data/_libs/expanding.pyx
+-rw-rw-rw-  2.0 fat    86528 b- defN 23-Jun-25 16:06 qlib/data/_libs/rolling.cp38-win_amd64.pyd
+-rw-rw-rw-  2.0 fat     6318 b- defN 23-Jun-25 16:04 qlib/data/_libs/rolling.pyx
+-rw-rw-rw-  2.0 fat    27919 b- defN 23-Jun-25 16:04 qlib/data/dataset/__init__.py
+-rw-rw-rw-  2.0 fat    27480 b- defN 23-Jun-25 16:04 qlib/data/dataset/handler.py
+-rw-rw-rw-  2.0 fat    12690 b- defN 23-Jun-25 16:04 qlib/data/dataset/loader.py
+-rw-rw-rw-  2.0 fat    14801 b- defN 23-Jun-25 16:04 qlib/data/dataset/processor.py
+-rw-rw-rw-  2.0 fat     6507 b- defN 23-Jun-25 16:04 qlib/data/dataset/storage.py
+-rw-rw-rw-  2.0 fat     4330 b- defN 23-Jun-25 16:04 qlib/data/dataset/utils.py
+-rw-rw-rw-  2.0 fat      781 b- defN 23-Jun-25 16:04 qlib/data/dataset/weight.py
+-rw-rw-rw-  2.0 fat      276 b- defN 23-Jun-25 16:04 qlib/data/storage/__init__.py
+-rw-rw-rw-  2.0 fat    14768 b- defN 23-Jun-25 16:04 qlib/data/storage/file_storage.py
+-rw-rw-rw-  2.0 fat    15156 b- defN 23-Jun-25 16:04 qlib/data/storage/storage.py
+-rw-rw-rw-  2.0 fat      158 b- defN 23-Jun-25 16:04 qlib/model/__init__.py
+-rw-rw-rw-  2.0 fat     3881 b- defN 23-Jun-25 16:04 qlib/model/base.py
+-rw-rw-rw-  2.0 fat    23384 b- defN 23-Jun-25 16:04 qlib/model/trainer.py
+-rw-rw-rw-  2.0 fat      605 b- defN 23-Jun-25 16:04 qlib/model/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-25 16:04 qlib/model/ens/__init__.py
+-rw-rw-rw-  2.0 fat     4669 b- defN 23-Jun-25 16:04 qlib/model/ens/ensemble.py
+-rw-rw-rw-  2.0 fat     4027 b- defN 23-Jun-25 16:04 qlib/model/ens/group.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-25 16:04 qlib/model/interpret/__init__.py
+-rw-rw-rw-  2.0 fat     1178 b- defN 23-Jun-25 16:04 qlib/model/interpret/base.py
+-rw-rw-rw-  2.0 fat      190 b- defN 23-Jun-25 16:04 qlib/model/meta/__init__.py
+-rw-rw-rw-  2.0 fat     2856 b- defN 23-Jun-25 16:04 qlib/model/meta/dataset.py
+-rw-rw-rw-  2.0 fat     2430 b- defN 23-Jun-25 16:04 qlib/model/meta/model.py
+-rw-rw-rw-  2.0 fat     1708 b- defN 23-Jun-25 16:04 qlib/model/meta/task.py
+-rw-rw-rw-  2.0 fat      351 b- defN 23-Jun-25 16:04 qlib/model/riskmodel/__init__.py
+-rw-rw-rw-  2.0 fat     5182 b- defN 23-Jun-25 16:04 qlib/model/riskmodel/base.py
+-rw-rw-rw-  2.0 fat     3290 b- defN 23-Jun-25 16:04 qlib/model/riskmodel/poet.py
+-rw-rw-rw-  2.0 fat    10702 b- defN 23-Jun-25 16:04 qlib/model/riskmodel/shrink.py
+-rw-rw-rw-  2.0 fat     3895 b- defN 23-Jun-25 16:04 qlib/model/riskmodel/structured.py
+-rw-rw-rw-  2.0 fat      347 b- defN 23-Jun-25 16:04 qlib/rl/__init__.py
+-rw-rw-rw-  2.0 fat     1165 b- defN 23-Jun-25 16:04 qlib/rl/aux_info.py
+-rw-rw-rw-  2.0 fat     5376 b- defN 23-Jun-25 16:04 qlib/rl/interpreter.py
+-rw-rw-rw-  2.0 fat     2788 b- defN 23-Jun-25 16:04 qlib/rl/reward.py
+-rw-rw-rw-  2.0 fat      351 b- defN 23-Jun-25 16:04 qlib/rl/seed.py
+-rw-rw-rw-  2.0 fat     3106 b- defN 23-Jun-25 16:04 qlib/rl/simulator.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-25 16:04 qlib/rl/contrib/__init__.py
+-rw-rw-rw-  2.0 fat    13614 b- defN 23-Jun-25 16:04 qlib/rl/contrib/backtest.py
+-rw-rw-rw-  2.0 fat     3494 b- defN 23-Jun-25 16:04 qlib/rl/contrib/naive_config_parser.py
+-rw-rw-rw-  2.0 fat    10373 b- defN 23-Jun-25 16:04 qlib/rl/contrib/train_onpolicy.py
+-rw-rw-rw-  2.0 fat      865 b- defN 23-Jun-25 16:04 qlib/rl/contrib/utils.py
+-rw-rw-rw-  2.0 fat      253 b- defN 23-Jun-25 16:04 qlib/rl/data/__init__.py
+-rw-rw-rw-  2.0 fat     1819 b- defN 23-Jun-25 16:04 qlib/rl/data/base.py
+-rw-rw-rw-  2.0 fat     3193 b- defN 23-Jun-25 16:04 qlib/rl/data/integration.py
+-rw-rw-rw-  2.0 fat     7645 b- defN 23-Jun-25 16:04 qlib/rl/data/native.py
+-rw-rw-rw-  2.0 fat    10840 b- defN 23-Jun-25 16:04 qlib/rl/data/pickle_styled.py
+-rw-rw-rw-  2.0 fat     1046 b- defN 23-Jun-25 16:04 qlib/rl/order_execution/__init__.py
+-rw-rw-rw-  2.0 fat     9926 b- defN 23-Jun-25 16:04 qlib/rl/order_execution/interpreter.py
+-rw-rw-rw-  2.0 fat     4970 b- defN 23-Jun-25 16:04 qlib/rl/order_execution/network.py
+-rw-rw-rw-  2.0 fat     7245 b- defN 23-Jun-25 16:04 qlib/rl/order_execution/policy.py
+-rw-rw-rw-  2.0 fat     3648 b- defN 23-Jun-25 16:04 qlib/rl/order_execution/reward.py
+-rw-rw-rw-  2.0 fat     4906 b- defN 23-Jun-25 16:04 qlib/rl/order_execution/simulator_qlib.py
+-rw-rw-rw-  2.0 fat    15104 b- defN 23-Jun-25 16:04 qlib/rl/order_execution/simulator_simple.py
+-rw-rw-rw-  2.0 fat     3791 b- defN 23-Jun-25 16:04 qlib/rl/order_execution/state.py
+-rw-rw-rw-  2.0 fat    21728 b- defN 23-Jun-25 16:04 qlib/rl/order_execution/strategy.py
+-rw-rw-rw-  2.0 fat     1683 b- defN 23-Jun-25 16:04 qlib/rl/order_execution/utils.py
+-rw-rw-rw-  2.0 fat      159 b- defN 23-Jun-25 16:04 qlib/rl/strategy/__init__.py
+-rw-rw-rw-  2.0 fat     1082 b- defN 23-Jun-25 16:04 qlib/rl/strategy/single_order.py
+-rw-rw-rw-  2.0 fat      483 b- defN 23-Jun-25 16:04 qlib/rl/trainer/__init__.py
+-rw-rw-rw-  2.0 fat     3768 b- defN 23-Jun-25 16:04 qlib/rl/trainer/api.py
+-rw-rw-rw-  2.0 fat    11867 b- defN 23-Jun-25 16:04 qlib/rl/trainer/callbacks.py
+-rw-rw-rw-  2.0 fat    13831 b- defN 23-Jun-25 16:04 qlib/rl/trainer/trainer.py
+-rw-rw-rw-  2.0 fat    10103 b- defN 23-Jun-25 16:04 qlib/rl/trainer/vessel.py
+-rw-rw-rw-  2.0 fat      548 b- defN 23-Jun-25 16:04 qlib/rl/utils/__init__.py
+-rw-rw-rw-  2.0 fat     6785 b- defN 23-Jun-25 16:04 qlib/rl/utils/data_queue.py
+-rw-rw-rw-  2.0 fat    10094 b- defN 23-Jun-25 16:04 qlib/rl/utils/env_wrapper.py
+-rw-rw-rw-  2.0 fat    13736 b- defN 23-Jun-25 16:04 qlib/rl/utils/finite_env.py
+-rw-rw-rw-  2.0 fat    19064 b- defN 23-Jun-25 16:04 qlib/rl/utils/log.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-25 16:04 qlib/run/__init__.py
+-rw-rw-rw-  2.0 fat      185 b- defN 23-Jun-25 16:04 qlib/run/get_data.py
+-rw-rw-rw-  2.0 fat       75 b- defN 23-Jun-25 16:04 qlib/strategy/__init__.py
+-rw-rw-rw-  2.0 fat    11451 b- defN 23-Jun-25 16:04 qlib/strategy/base.py
+-rw-rw-rw-  2.0 fat    12511 b- defN 23-Jun-25 16:04 qlib/tests/__init__.py
+-rw-rw-rw-  2.0 fat     5001 b- defN 23-Jun-25 16:04 qlib/tests/config.py
+-rw-rw-rw-  2.0 fat     8192 b- defN 23-Jun-25 16:04 qlib/tests/data.py
+-rw-rw-rw-  2.0 fat    35251 b- defN 23-Jun-25 16:04 qlib/utils/__init__.py
+-rw-rw-rw-  2.0 fat     3190 b- defN 23-Jun-25 16:04 qlib/utils/data.py
+-rw-rw-rw-  2.0 fat      457 b- defN 23-Jun-25 16:04 qlib/utils/exceptions.py
+-rw-rw-rw-  2.0 fat     5928 b- defN 23-Jun-25 16:04 qlib/utils/file.py
+-rw-rw-rw-  2.0 fat    22537 b- defN 23-Jun-25 16:04 qlib/utils/index_data.py
+-rw-rw-rw-  2.0 fat     3435 b- defN 23-Jun-25 16:04 qlib/utils/objm.py
+-rw-rw-rw-  2.0 fat     9433 b- defN 23-Jun-25 16:04 qlib/utils/paral.py
+-rw-rw-rw-  2.0 fat     9626 b- defN 23-Jun-25 16:04 qlib/utils/resam.py
+-rw-rw-rw-  2.0 fat     6263 b- defN 23-Jun-25 16:04 qlib/utils/serial.py
+-rw-rw-rw-  2.0 fat    12155 b- defN 23-Jun-25 16:04 qlib/utils/time.py
+-rw-rw-rw-  2.0 fat    25250 b- defN 23-Jun-25 16:04 qlib/workflow/__init__.py
+-rw-rw-rw-  2.0 fat     3809 b- defN 23-Jun-25 16:04 qlib/workflow/cli.py
+-rw-rw-rw-  2.0 fat    15192 b- defN 23-Jun-25 16:04 qlib/workflow/exp.py
+-rw-rw-rw-  2.0 fat    18008 b- defN 23-Jun-25 16:04 qlib/workflow/expm.py
+-rw-rw-rw-  2.0 fat    22416 b- defN 23-Jun-25 16:04 qlib/workflow/record_temp.py
+-rw-rw-rw-  2.0 fat    18257 b- defN 23-Jun-25 16:04 qlib/workflow/recorder.py
+-rw-rw-rw-  2.0 fat     1663 b- defN 23-Jun-25 16:04 qlib/workflow/utils.py
+-rw-rw-rw-  2.0 fat        0 b- defN 23-Jun-25 16:04 qlib/workflow/online/__init__.py
+-rw-rw-rw-  2.0 fat    17748 b- defN 23-Jun-25 16:04 qlib/workflow/online/manager.py
+-rw-rw-rw-  2.0 fat     8620 b- defN 23-Jun-25 16:04 qlib/workflow/online/strategy.py
+-rw-rw-rw-  2.0 fat    10884 b- defN 23-Jun-25 16:04 qlib/workflow/online/update.py
+-rw-rw-rw-  2.0 fat     6662 b- defN 23-Jun-25 16:04 qlib/workflow/online/utils.py
+-rw-rw-rw-  2.0 fat      548 b- defN 23-Jun-25 16:04 qlib/workflow/task/__init__.py
+-rw-rw-rw-  2.0 fat    10315 b- defN 23-Jun-25 16:04 qlib/workflow/task/collect.py
+-rw-rw-rw-  2.0 fat    12119 b- defN 23-Jun-25 16:04 qlib/workflow/task/gen.py
+-rw-rw-rw-  2.0 fat    18968 b- defN 23-Jun-25 16:04 qlib/workflow/task/manage.py
+-rw-rw-rw-  2.0 fat     8890 b- defN 23-Jun-25 16:04 qlib/workflow/task/utils.py
+-rw-rw-rw-  2.0 fat     1162 b- defN 23-Jun-25 16:06 pyqlib-0.9.2.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat    39966 b- defN 23-Jun-25 16:06 pyqlib-0.9.2.dist-info/METADATA
+-rw-rw-rw-  2.0 fat      100 b- defN 23-Jun-25 16:06 pyqlib-0.9.2.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat       48 b- defN 23-Jun-25 16:05 pyqlib-0.9.2.dist-info/entry_points.txt
+-rw-rw-rw-  2.0 fat        5 b- defN 23-Jun-25 16:05 pyqlib-0.9.2.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat    20169 b- defN 23-Jun-25 16:06 pyqlib-0.9.2.dist-info/RECORD
+233 files, 2276493 bytes uncompressed, 621669 bytes compressed:  72.7%
```

## zipnote {}

```diff
@@ -159,14 +159,17 @@
 
 Filename: qlib/contrib/model/pytorch_hist.py
 Comment: 
 
 Filename: qlib/contrib/model/pytorch_igmtf.py
 Comment: 
 
+Filename: qlib/contrib/model/pytorch_krnn.py
+Comment: 
+
 Filename: qlib/contrib/model/pytorch_localformer.py
 Comment: 
 
 Filename: qlib/contrib/model/pytorch_localformer_ts.py
 Comment: 
 
 Filename: qlib/contrib/model/pytorch_lstm.py
@@ -174,14 +177,17 @@
 
 Filename: qlib/contrib/model/pytorch_lstm_ts.py
 Comment: 
 
 Filename: qlib/contrib/model/pytorch_nn.py
 Comment: 
 
+Filename: qlib/contrib/model/pytorch_sandwich.py
+Comment: 
+
 Filename: qlib/contrib/model/pytorch_sfm.py
 Comment: 
 
 Filename: qlib/contrib/model/pytorch_tabnet.py
 Comment: 
 
 Filename: qlib/contrib/model/pytorch_tcn.py
@@ -669,26 +675,26 @@
 
 Filename: qlib/workflow/task/manage.py
 Comment: 
 
 Filename: qlib/workflow/task/utils.py
 Comment: 
 
-Filename: pyqlib-0.9.1.dist-info/LICENSE
+Filename: pyqlib-0.9.2.dist-info/LICENSE
 Comment: 
 
-Filename: pyqlib-0.9.1.dist-info/METADATA
+Filename: pyqlib-0.9.2.dist-info/METADATA
 Comment: 
 
-Filename: pyqlib-0.9.1.dist-info/WHEEL
+Filename: pyqlib-0.9.2.dist-info/WHEEL
 Comment: 
 
-Filename: pyqlib-0.9.1.dist-info/entry_points.txt
+Filename: pyqlib-0.9.2.dist-info/entry_points.txt
 Comment: 
 
-Filename: pyqlib-0.9.1.dist-info/top_level.txt
+Filename: pyqlib-0.9.2.dist-info/top_level.txt
 Comment: 
 
-Filename: pyqlib-0.9.1.dist-info/RECORD
+Filename: pyqlib-0.9.2.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## qlib/__init__.py

```diff
@@ -1,12 +1,12 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 from pathlib import Path
 
-__version__ = "0.9.1"
+__version__ = "0.9.2"
 __version__bak = __version__  # This version is backup for QlibConfig.reset_qlib_version
 import os
 from typing import Union
 import yaml
 import logging
 import platform
 import subprocess
```

## qlib/config.py

```diff
@@ -143,14 +143,15 @@
     "dataset_cache_dir_name": "dataset_cache",
     "features_cache_dir_name": "features_cache",
     # redis
     # in order to use cache
     "redis_host": "127.0.0.1",
     "redis_port": 6379,
     "redis_task_db": 1,
+    "redis_password": None,
     # This value can be reset via qlib.init
     "logging_level": logging.INFO,
     # Global configuration of qlib log
     # logging_level can control the logging level more finely
     "logging_config": {
         "version": 1,
         "formatters": {
```

## qlib/backtest/__init__.py

```diff
@@ -36,16 +36,16 @@
     start_time: Union[pd.Timestamp, str] = None,
     end_time: Union[pd.Timestamp, str] = None,
     codes: Union[list, str] = "all",
     subscribe_fields: list = [],
     open_cost: float = 0.0015,
     close_cost: float = 0.0025,
     min_cost: float = 5.0,
-    limit_threshold: Union[Tuple[str, str], float, None] = None,
-    deal_price: Union[str, Tuple[str, str], List[str]] = None,
+    limit_threshold: Union[Tuple[str, str], float, None] | None = None,
+    deal_price: Union[str, Tuple[str, str], List[str]] | None = None,
     **kwargs: Any,
 ) -> Exchange:
     """get_exchange
 
     Parameters
     ----------
 
@@ -280,15 +280,15 @@
     end_time: Union[pd.Timestamp, str],
     strategy: Union[str, dict, object, Path],
     executor: Union[str, dict, object, Path],
     benchmark: str = "SH000300",
     account: Union[float, int, dict] = 1e9,
     exchange_kwargs: dict = {},
     pos_type: str = "Position",
-    return_value: dict = None,
+    return_value: dict | None = None,
 ) -> Generator[object, None, None]:
     """initialize the strategy and executor, then collect the trade decision data for rl training
 
     please refer to the docs of the backtest for the explanation of the parameters
 
     Yields
     -------
```

## qlib/backtest/account.py

```diff
@@ -148,15 +148,17 @@
             # This may result in obscure bugs when data quality is low.
             if isinstance(self.benchmark_config, dict) and "start_time" in self.benchmark_config:
                 self.current_position.fill_stock_value(self.benchmark_config["start_time"], self.freq)
 
         # trading related metrics(e.g. high-frequency trading)
         self.indicator = Indicator()
 
-    def reset(self, freq: str = None, benchmark_config: dict = None, port_metr_enabled: bool = None) -> None:
+    def reset(
+        self, freq: str | None = None, benchmark_config: dict | None = None, port_metr_enabled: bool | None = None
+    ) -> None:
         """reset freq and report of account
 
         Parameters
         ----------
         freq : str, optional
             frequency of account & report, by default None
         benchmark_config : {}, optional
```

## qlib/backtest/backtest.py

```diff
@@ -51,15 +51,15 @@
 
 
 def collect_data_loop(
     start_time: Union[pd.Timestamp, str],
     end_time: Union[pd.Timestamp, str],
     trade_strategy: BaseStrategy,
     trade_executor: BaseExecutor,
-    return_value: dict = None,
+    return_value: dict | None = None,
 ) -> Generator[BaseTradeDecision, Optional[BaseTradeDecision], None]:
     """Generator for collecting the trade decision data for rl training
 
     Parameters
     ----------
     start_time : Union[pd.Timestamp, str]
         closed start time for backtest
```

## qlib/backtest/decision.py

```diff
@@ -250,15 +250,15 @@
 
 
 class IdxTradeRange(TradeRange):
     def __init__(self, start_idx: int, end_idx: int) -> None:
         self._start_idx = start_idx
         self._end_idx = end_idx
 
-    def __call__(self, trade_calendar: TradeCalendarManager = None) -> Tuple[int, int]:
+    def __call__(self, trade_calendar: TradeCalendarManager | None = None) -> Tuple[int, int]:
         return self._start_idx, self._end_idx
 
     def clip_time_range(self, start_time: pd.Timestamp, end_time: pd.Timestamp) -> Tuple[pd.Timestamp, pd.Timestamp]:
         raise NotImplementedError
 
 
 class TradeRangeByTime(TradeRange):
@@ -311,15 +311,15 @@
         2. After a period of time, the decision are updated and become available
         3. The inner strategy try to get the decision and start to execute the decision according to `get_range_limit`
         Case 2:
         1. The outer strategy's decision is available at the start of the interval
         2. Same as `case 1.3`
     """
 
-    def __init__(self, strategy: BaseStrategy, trade_range: Union[Tuple[int, int], TradeRange] = None) -> None:
+    def __init__(self, strategy: BaseStrategy, trade_range: Union[Tuple[int, int], TradeRange, None] = None) -> None:
         """
         Parameters
         ----------
         strategy : BaseStrategy
             The strategy who make the decision
         trade_range: Union[Tuple[int, int], Callable] (optional)
             The index range for underlying strategy.
@@ -550,15 +550,15 @@
     Besides, the time_range is also included.
     """
 
     def __init__(
         self,
         order_list: List[Order],
         strategy: BaseStrategy,
-        trade_range: Union[Tuple[int, int], TradeRange] = None,
+        trade_range: Union[Tuple[int, int], TradeRange, None] = None,
     ) -> None:
         super().__init__(strategy, trade_range=trade_range)
         self.order_list = cast(List[Order], order_list)
         start, end = strategy.trade_calendar.get_step_time()
         for o in order_list:
             assert isinstance(o, Order)
             if o.start_time is None:
```

## qlib/backtest/exchange.py

```diff
@@ -37,18 +37,18 @@
 
     def __init__(
         self,
         freq: str = "day",
         start_time: Union[pd.Timestamp, str] = None,
         end_time: Union[pd.Timestamp, str] = None,
         codes: Union[list, str] = "all",
-        deal_price: Union[str, Tuple[str, str], List[str]] = None,
+        deal_price: Union[str, Tuple[str, str], List[str], None] = None,
         subscribe_fields: list = [],
         limit_threshold: Union[Tuple[str, str], float, None] = None,
-        volume_threshold: Union[tuple, dict] = None,
+        volume_threshold: Union[tuple, dict, None] = None,
         open_cost: float = 0.0015,
         close_cost: float = 0.0025,
         min_cost: float = 5.0,
         impact_cost: float = 0.0,
         extra_quote: pd.DataFrame = None,
         quote_cls: Type[BaseQuote] = NumpyQuote,
         **kwargs: Any,
@@ -336,15 +336,15 @@
         return buy_vol_limit, sell_vol_limit, fields
 
     def check_stock_limit(
         self,
         stock_id: str,
         start_time: pd.Timestamp,
         end_time: pd.Timestamp,
-        direction: int = None,
+        direction: int | None = None,
     ) -> bool:
         """
         Parameters
         ----------
         stock_id : str
         start_time: pd.Timestamp
         end_time: pd.Timestamp
@@ -402,31 +402,31 @@
             return True
 
     def is_stock_tradable(
         self,
         stock_id: str,
         start_time: pd.Timestamp,
         end_time: pd.Timestamp,
-        direction: int = None,
+        direction: int | None = None,
     ) -> bool:
         # check if stock can be traded
         return not (
             self.check_stock_suspended(stock_id, start_time, end_time)
             or self.check_stock_limit(stock_id, start_time, end_time, direction)
         )
 
     def check_order(self, order: Order) -> bool:
         # check limit and suspended
         return self.is_stock_tradable(order.stock_id, order.start_time, order.end_time, order.direction)
 
     def deal_order(
         self,
         order: Order,
-        trade_account: Account = None,
-        position: BasePosition = None,
+        trade_account: Account | None = None,
+        position: BasePosition | None = None,
         dealt_order_amount: Dict[str, float] = defaultdict(float),
     ) -> Tuple[float, float, float]:
         """
         Deal order when the actual transaction
         the results section in `Order` will be changed.
         :param order:  Deal the order.
         :param trade_account: Trade account to be updated after dealing the order.
@@ -582,15 +582,15 @@
                         start_time=start_time,
                         end_time=end_time,
                         direction=direction,
                     )
                 )
         return amount_dict
 
-    def get_real_deal_amount(self, current_amount: float, target_amount: float, factor: float = None) -> float:
+    def get_real_deal_amount(self, current_amount: float, target_amount: float, factor: float | None = None) -> float:
         """
         Calculate the real adjust deal amount when considering the trading unit
         :param current_amount:
         :param target_amount:
         :param factor:
         :return  real_deal_amount;  Positive deal_amount indicates buying more stock.
         """
@@ -708,32 +708,32 @@
                     )
                     * amount_dict[stock_id]
                 )
         return value
 
     def _get_factor_or_raise_error(
         self,
-        factor: float = None,
-        stock_id: str = None,
+        factor: float | None = None,
+        stock_id: str | None = None,
         start_time: pd.Timestamp = None,
         end_time: pd.Timestamp = None,
     ) -> float:
         """Please refer to the docs of get_amount_of_trade_unit"""
         if factor is None:
             if stock_id is not None and start_time is not None and end_time is not None:
                 factor = self.get_factor(stock_id=stock_id, start_time=start_time, end_time=end_time)
             else:
                 raise ValueError(f"`factor` and (`stock_id`, `start_time`, `end_time`) can't both be None")
         assert factor is not None
         return factor
 
     def get_amount_of_trade_unit(
         self,
-        factor: float = None,
-        stock_id: str = None,
+        factor: float | None = None,
+        stock_id: str | None = None,
         start_time: pd.Timestamp = None,
         end_time: pd.Timestamp = None,
     ) -> Optional[float]:
         """
         get the trade unit of amount based on **factor**
         the factor can be given directly or calculated in given time range and stock id.
         `factor` has higher priority than `stock_id`, `start_time` and `end_time`
@@ -758,16 +758,16 @@
             return self.trade_unit / factor
         else:
             return None
 
     def round_amount_by_trade_unit(
         self,
         deal_amount: float,
-        factor: float = None,
-        stock_id: str = None,
+        factor: float | None = None,
+        stock_id: str | None = None,
         start_time: pd.Timestamp = None,
         end_time: pd.Timestamp = None,
     ) -> float:
         """Parameter
         Please refer to the docs of get_amount_of_trade_unit
         deal_amount : float, adjusted amount
         factor : float, adjusted factor
```

## qlib/backtest/executor.py

```diff
@@ -27,16 +27,16 @@
         time_per_step: str,
         start_time: Union[str, pd.Timestamp] = None,
         end_time: Union[str, pd.Timestamp] = None,
         indicator_config: dict = {},
         generate_portfolio_metrics: bool = False,
         verbose: bool = False,
         track_data: bool = False,
-        trade_exchange: Exchange = None,
-        common_infra: CommonInfrastructure = None,
+        trade_exchange: Exchange | None = None,
+        common_infra: CommonInfrastructure | None = None,
         settle_type: str = BasePosition.ST_NO,
         **kwargs: Any,
     ) -> None:
         """
         Parameters
         ----------
         time_per_step : str
@@ -157,15 +157,15 @@
     def trade_calendar(self) -> TradeCalendarManager:
         """
         Though trade calendar can be accessed from multiple sources, but managing in a centralized way will make the
         code easier
         """
         return self.level_infra.get("trade_calendar")
 
-    def reset(self, common_infra: CommonInfrastructure = None, **kwargs: Any) -> None:
+    def reset(self, common_infra: CommonInfrastructure | None = None, **kwargs: Any) -> None:
         """
         - reset `start_time` and `end_time`, used in trade calendar
         - reset `common_infra`, used to reset `trade_account`, `trade_exchange`, .etc
         """
 
         if "start_time" in kwargs or "end_time" in kwargs:
             start_time = kwargs.get("start_time")
@@ -223,15 +223,15 @@
         Tuple[List[object], dict]:
             (<the executed result for trade decision>, <the extra kwargs for `self.trade_account.update_bar_end`>)
         """
 
     def collect_data(
         self,
         trade_decision: BaseTradeDecision,
-        return_value: dict = None,
+        return_value: dict | None = None,
         level: int = 0,
     ) -> Generator[Any, Any, List[object]]:
         """Generator for collecting the trade decision data for rl training
 
         his function will make a step forward
 
         Parameters
@@ -323,15 +323,15 @@
         end_time: Union[str, pd.Timestamp] = None,
         indicator_config: dict = {},
         generate_portfolio_metrics: bool = False,
         verbose: bool = False,
         track_data: bool = False,
         skip_empty_decision: bool = True,
         align_range_limit: bool = True,
-        common_infra: CommonInfrastructure = None,
+        common_infra: CommonInfrastructure | None = None,
         **kwargs: Any,
     ) -> None:
         """
         Parameters
         ----------
         inner_executor : BaseExecutor
             trading env in each trading bar.
@@ -530,15 +530,15 @@
         time_per_step: str,
         start_time: Union[str, pd.Timestamp] = None,
         end_time: Union[str, pd.Timestamp] = None,
         indicator_config: dict = {},
         generate_portfolio_metrics: bool = False,
         verbose: bool = False,
         track_data: bool = False,
-        common_infra: CommonInfrastructure = None,
+        common_infra: CommonInfrastructure | None = None,
         trade_type: str = TT_SERIAL,
         **kwargs: Any,
     ) -> None:
         """
         Parameters
         ----------
         trade_type: str
```

## qlib/backtest/position.py

```diff
@@ -1,10 +1,11 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 
+from __future__ import annotations
 
 from datetime import timedelta
 from typing import Any, Dict, List, Union
 
 import numpy as np
 import pandas as pd
 
@@ -316,15 +317,15 @@
             lack_stock = set(stock_list) - set(price_dict)
             raise ValueError(f"{lack_stock} doesn't have close price in qlib in the latest {last_days} days")
 
         for stock in stock_list:
             self.position[stock]["price"] = price_dict[stock]
         self.position["now_account_value"] = self.calculate_value()
 
-    def _init_stock(self, stock_id: str, amount: float, price: float = None) -> None:
+    def _init_stock(self, stock_id: str, amount: float, price: float | None = None) -> None:
         """
         initialization the stock in current position
 
         Parameters
         ----------
         stock_id :
             the id of the stock
```

## qlib/backtest/report.py

```diff
@@ -1,10 +1,11 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 
+from __future__ import annotations
 
 import pathlib
 from collections import OrderedDict
 from typing import Any, Dict, List, Optional, Text, Tuple, Type, Union, cast
 
 import numpy as np
 import pandas as pd
@@ -82,15 +83,15 @@
         self.total_costs: dict = OrderedDict()  # total trade cost for each trade time
         self.costs: dict = OrderedDict()  # trade cost rate for each trade time
         self.values: dict = OrderedDict()  # value for each trade time
         self.cashes: dict = OrderedDict()
         self.benches: dict = OrderedDict()
         self.latest_pm_time: Optional[pd.TimeStamp] = None
 
-    def init_bench(self, freq: str = None, benchmark_config: dict = None) -> None:
+    def init_bench(self, freq: str | None = None, benchmark_config: dict | None = None) -> None:
         if freq is not None:
             self.freq = freq
         self.benchmark_config = benchmark_config
         self.bench = self._cal_benchmark(self.benchmark_config, self.freq)
 
     @staticmethod
     def _cal_benchmark(benchmark_config: Optional[dict], freq: str) -> Optional[pd.Series]:
@@ -145,23 +146,23 @@
     def get_latest_total_turnover(self) -> Any:
         return self.total_turnovers[self.latest_pm_time]
 
     def update_portfolio_metrics_record(
         self,
         trade_start_time: Union[str, pd.Timestamp] = None,
         trade_end_time: Union[str, pd.Timestamp] = None,
-        account_value: float = None,
-        cash: float = None,
-        return_rate: float = None,
-        total_turnover: float = None,
-        turnover_rate: float = None,
-        total_cost: float = None,
-        cost_rate: float = None,
-        stock_value: float = None,
-        bench_value: float = None,
+        account_value: float | None = None,
+        cash: float | None = None,
+        return_rate: float | None = None,
+        total_turnover: float | None = None,
+        turnover_rate: float | None = None,
+        total_cost: float | None = None,
+        cost_rate: float | None = None,
+        stock_value: float | None = None,
+        bench_value: float | None = None,
     ) -> None:
         # check data
         if None in [
             trade_start_time,
             account_value,
             cash,
             return_rate,
```

## qlib/backtest/utils.py

```diff
@@ -27,15 +27,15 @@
     """
 
     def __init__(
         self,
         freq: str,
         start_time: Union[str, pd.Timestamp] = None,
         end_time: Union[str, pd.Timestamp] = None,
-        level_infra: LevelInfrastructure = None,
+        level_infra: LevelInfrastructure | None = None,
     ) -> None:
         """
         Parameters
         ----------
         freq : str
             frequency of trading calendar, also trade time per trading step
         start_time : Union[str, pd.Timestamp], optional
@@ -95,15 +95,15 @@
     def get_trade_len(self) -> int:
         """get the total step length"""
         return self.trade_len
 
     def get_trade_step(self) -> int:
         return self.trade_step
 
-    def get_step_time(self, trade_step: int = None, shift: int = 0) -> Tuple[pd.Timestamp, pd.Timestamp]:
+    def get_step_time(self, trade_step: int | None = None, shift: int = 0) -> Tuple[pd.Timestamp, pd.Timestamp]:
         """
         Get the left and right endpoints of the trade_step'th trading interval
 
         About the endpoints:
             - Qlib uses the closed interval in time-series data selection, which has the same performance as
             pandas.Series.loc
             # - The returned right endpoints should minus 1 seconds because of the closed interval representation in
```

## qlib/contrib/data/handler.py

```diff
@@ -52,30 +52,30 @@
         end_time=None,
         freq="day",
         infer_processors=_DEFAULT_INFER_PROCESSORS,
         learn_processors=_DEFAULT_LEARN_PROCESSORS,
         fit_start_time=None,
         fit_end_time=None,
         filter_pipe=None,
-        inst_processor=None,
+        inst_processors=None,
         **kwargs
     ):
         infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
         learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
 
         data_loader = {
             "class": "QlibDataLoader",
             "kwargs": {
                 "config": {
                     "feature": self.get_feature_config(),
                     "label": kwargs.pop("label", self.get_label_config()),
                 },
                 "filter_pipe": filter_pipe,
                 "freq": freq,
-                "inst_processor": inst_processor,
+                "inst_processors": inst_processors,
             },
         }
 
         super().__init__(
             instruments=instruments,
             start_time=start_time,
             end_time=end_time,
@@ -148,30 +148,30 @@
         freq="day",
         infer_processors=[],
         learn_processors=_DEFAULT_LEARN_PROCESSORS,
         fit_start_time=None,
         fit_end_time=None,
         process_type=DataHandlerLP.PTYPE_A,
         filter_pipe=None,
-        inst_processor=None,
+        inst_processors=None,
         **kwargs
     ):
         infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
         learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
 
         data_loader = {
             "class": "QlibDataLoader",
             "kwargs": {
                 "config": {
                     "feature": self.get_feature_config(),
                     "label": kwargs.pop("label", self.get_label_config()),
                 },
                 "filter_pipe": filter_pipe,
                 "freq": freq,
-                "inst_processor": inst_processor,
+                "inst_processors": inst_processors,
             },
         }
         super().__init__(
             instruments=instruments,
             start_time=start_time,
             end_time=end_time,
             data_loader=data_loader,
```

## qlib/contrib/data/highfreq_handler.py

```diff
@@ -40,15 +40,15 @@
         )
 
     def get_feature_config(self):
         fields = []
         names = []
 
         template_if = "If(IsNull({1}), {0}, {1})"
-        template_paused = "Select(Gt($hx_paused_num, 1.001), {0})"
+        template_paused = "Select(Gt($paused_num, 1.001), {0})"
 
         def get_normalized_price_feature(price_field, shift=0):
             # norm with the close price of 237th minute of yesterday.
             if shift == 0:
                 template_norm = "{0}/DayLast(Ref({1}, 243))"
             else:
                 template_norm = "Ref({0}, " + str(shift) + ")/DayLast(Ref({1}, 243))"
@@ -111,27 +111,29 @@
         learn_processors=[],
         fit_start_time=None,
         fit_end_time=None,
         drop_raw=True,
         day_length=240,
         freq="1min",
         columns=["$open", "$high", "$low", "$close", "$vwap"],
+        inst_processors=None,
     ):
         self.day_length = day_length
         self.columns = columns
 
         infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
         learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
 
         data_loader = {
             "class": "QlibDataLoader",
             "kwargs": {
                 "config": self.get_feature_config(),
                 "swap_level": False,
                 "freq": freq,
+                "inst_processors": inst_processors,
             },
         }
         super().__init__(
             instruments=instruments,
             start_time=start_time,
             end_time=end_time,
             data_loader=data_loader,
@@ -253,23 +255,25 @@
         self,
         instruments="csi300",
         start_time=None,
         end_time=None,
         day_length=240,
         freq="1min",
         columns=["$close", "$vwap", "$volume"],
+        inst_processors=None,
     ):
         self.day_length = day_length
         self.columns = set(columns)
         data_loader = {
             "class": "QlibDataLoader",
             "kwargs": {
                 "config": self.get_feature_config(),
                 "swap_level": False,
                 "freq": freq,
+                "inst_processors": inst_processors,
             },
         }
         super().__init__(
             instruments=instruments,
             start_time=start_time,
             end_time=end_time,
             data_loader=data_loader,
@@ -307,26 +311,28 @@
         instruments="csi300",
         start_time=None,
         end_time=None,
         infer_processors=[],
         learn_processors=[],
         fit_start_time=None,
         fit_end_time=None,
+        inst_processors=None,
         drop_raw=True,
     ):
 
         infer_processors = check_transform_proc(infer_processors, fit_start_time, fit_end_time)
         learn_processors = check_transform_proc(learn_processors, fit_start_time, fit_end_time)
 
         data_loader = {
             "class": "QlibDataLoader",
             "kwargs": {
                 "config": self.get_feature_config(),
                 "swap_level": False,
                 "freq": "1min",
+                "inst_processors": inst_processors,
             },
         }
         super().__init__(
             instruments=instruments,
             start_time=start_time,
             end_time=end_time,
             data_loader=data_loader,
@@ -478,15 +484,15 @@
         )
 
     def get_feature_config(self):
         fields = []
         names = []
 
         template_if = "If(IsNull({1}), {0}, {1})"
-        template_paused = "Select(Gt($hx_paused_num, 1.001), {0})"
+        template_paused = "Select(Gt($paused_num, 1.001), {0})"
         template_fillnan = "FFillNan({0})"
         fields += [
             template_fillnan.format(template_paused.format("$close")),
         ]
         names += ["$close0"]
 
         fields += [
```

## qlib/contrib/data/highfreq_provider.py

```diff
@@ -124,28 +124,28 @@
     def _gen_dataframe(self, config, datasets=["train", "valid", "test"]):
         try:
             path = config.pop("path")
         except KeyError as e:
             raise ValueError("Must specify the path to save the dataset.") from e
         if os.path.isfile(path):
             start = time.time()
-            self.logger.info("Dataset exists, load from disk.", __name__)
+            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
 
             # res = dataset.prepare(['train', 'valid', 'test'])
             with open(path, "rb") as f:
                 data = pkl.load(f)
             if isinstance(data, dict):
                 res = [data[i] for i in datasets]
             else:
                 res = data.prepare(datasets)
-            self.logger.info(f"Data loaded, time cost: {time.time() - start:.2f}", __name__)
+            self.logger.info(f"[{__name__}]Data loaded, time cost: {time.time() - start:.2f}")
         else:
             if not os.path.exists(os.path.dirname(path)):
                 os.makedirs(os.path.dirname(path))
-            self.logger.info("Generating dataset", __name__)
+            self.logger.info(f"[{__name__}]Generating dataset")
             start_time = time.time()
             self._prepare_calender_cache()
             dataset = init_instance_by_config(config)
             trainset, validset, testset = dataset.prepare(["train", "valid", "test"])
             data = {
                 "train": trainset,
                 "valid": validset,
@@ -156,90 +156,90 @@
             with open(path[:-4] + "train.pkl", "wb") as f:
                 pkl.dump(trainset, f)
             with open(path[:-4] + "valid.pkl", "wb") as f:
                 pkl.dump(validset, f)
             with open(path[:-4] + "test.pkl", "wb") as f:
                 pkl.dump(testset, f)
             res = [data[i] for i in datasets]
-            self.logger.info(f"Data generated, time cost: {(time.time() - start_time):.2f}", __name__)
+            self.logger.info(f"[{__name__}]Data generated, time cost: {(time.time() - start_time):.2f}")
         return res
 
     def _gen_data(self, config, datasets=["train", "valid", "test"]):
         try:
             path = config.pop("path")
         except KeyError as e:
             raise ValueError("Must specify the path to save the dataset.") from e
         if os.path.isfile(path):
             start = time.time()
-            self.logger.info("Dataset exists, load from disk.", __name__)
+            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
 
             # res = dataset.prepare(['train', 'valid', 'test'])
             with open(path, "rb") as f:
                 data = pkl.load(f)
             if isinstance(data, dict):
                 res = [data[i] for i in datasets]
             else:
                 res = data.prepare(datasets)
-            self.logger.info(f"Data loaded, time cost: {time.time() - start:.2f}", __name__)
+            self.logger.info(f"[{__name__}]Data loaded, time cost: {time.time() - start:.2f}")
         else:
             if not os.path.exists(os.path.dirname(path)):
                 os.makedirs(os.path.dirname(path))
-            self.logger.info("Generating dataset", __name__)
+            self.logger.info(f"[{__name__}]Generating dataset")
             start_time = time.time()
             self._prepare_calender_cache()
             dataset = init_instance_by_config(config)
             dataset.config(dump_all=True, recursive=True)
             dataset.to_pickle(path)
             res = dataset.prepare(datasets)
-            self.logger.info(f"Data generated, time cost: {(time.time() - start_time):.2f}", __name__)
+            self.logger.info(f"[{__name__}]Data generated, time cost: {(time.time() - start_time):.2f}")
         return res
 
     def _gen_dataset(self, config):
         try:
             path = config.pop("path")
         except KeyError as e:
             raise ValueError("Must specify the path to save the dataset.") from e
         if os.path.isfile(path):
             start = time.time()
-            self.logger.info("Dataset exists, load from disk.", __name__)
+            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
 
             with open(path, "rb") as f:
                 dataset = pkl.load(f)
-            self.logger.info(f"Data loaded, time cost: {time.time() - start:.2f}", __name__)
+            self.logger.info(f"[{__name__}]Data loaded, time cost: {time.time() - start:.2f}")
         else:
             start = time.time()
             if not os.path.exists(os.path.dirname(path)):
                 os.makedirs(os.path.dirname(path))
-            self.logger.info("Generating dataset", __name__)
+            self.logger.info(f"[{__name__}]Generating dataset")
             self._prepare_calender_cache()
             dataset = init_instance_by_config(config)
-            self.logger.info(f"Dataset init, time cost: {time.time() - start:.2f}", __name__)
+            self.logger.info(f"[{__name__}]Dataset init, time cost: {time.time() - start:.2f}")
             dataset.prepare(["train", "valid", "test"])
-            self.logger.info(f"Dataset prepared, time cost: {time.time() - start:.2f}", __name__)
+            self.logger.info(f"[{__name__}]Dataset prepared, time cost: {time.time() - start:.2f}")
             dataset.config(dump_all=True, recursive=True)
             dataset.to_pickle(path)
         return dataset
 
     def _gen_day_dataset(self, config, conf_type):
         try:
             path = config.pop("path")
         except KeyError as e:
             raise ValueError("Must specify the path to save the dataset.") from e
 
         if os.path.isfile(path + "tmp_dataset.pkl"):
             start = time.time()
-            self.logger.info("Dataset exists, load from disk.", __name__)
+            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
         else:
             start = time.time()
             if not os.path.exists(os.path.dirname(path)):
                 os.makedirs(os.path.dirname(path))
-            self.logger.info("Generating dataset", __name__)
+            self.logger.info(f"[{__name__}]Generating dataset")
             self._prepare_calender_cache()
             dataset = init_instance_by_config(config)
-            self.logger.info(f"Dataset init, time cost: {time.time() - start:.2f}", __name__)
+            self.logger.info(f"[{__name__}]Dataset init, time cost: {time.time() - start:.2f}")
             dataset.config(dump_all=False, recursive=True)
             dataset.to_pickle(path + "tmp_dataset.pkl")
 
         with open(path + "tmp_dataset.pkl", "rb") as f:
             new_dataset = pkl.load(f)
 
         time_list = D.calendar(start_time=self.start_time, end_time=self.end_time, freq=self.freq)[::240]
@@ -264,23 +264,23 @@
         try:
             path = config.pop("path")
         except KeyError as e:
             raise ValueError("Must specify the path to save the dataset.") from e
 
         if os.path.isfile(path + "tmp_dataset.pkl"):
             start = time.time()
-            self.logger.info("Dataset exists, load from disk.", __name__)
+            self.logger.info(f"[{__name__}]Dataset exists, load from disk.")
         else:
             start = time.time()
             if not os.path.exists(os.path.dirname(path)):
                 os.makedirs(os.path.dirname(path))
-            self.logger.info("Generating dataset", __name__)
+            self.logger.info(f"[{__name__}]Generating dataset")
             self._prepare_calender_cache()
             dataset = init_instance_by_config(config)
-            self.logger.info(f"Dataset init, time cost: {time.time() - start:.2f}", __name__)
+            self.logger.info(f"[{__name__}]Dataset init, time cost: {time.time() - start:.2f}")
             dataset.config(dump_all=False, recursive=True)
             dataset.to_pickle(path + "tmp_dataset.pkl")
 
         with open(path + "tmp_dataset.pkl", "rb") as f:
             new_dataset = pkl.load(f)
 
         instruments = D.instruments(market="all")
```

## qlib/contrib/meta/data_selection/dataset.py

```diff
@@ -51,16 +51,18 @@
         perf_task_tpl = deepcopy(self.task_tpl)  # this task is supposed to contains no complicated objects
 
         trainer = auto_filter_kwargs(trainer)(experiment_name=self.exp_name, **trainer_kwargs)
         # NOTE:
         # The handler is initialized for only once.
         if not trainer.has_worker():
             self.dh = init_task_handler(perf_task_tpl)
+            self.dh.config(dump_all=False)  # in some cases, the data handler are saved to disk with `dump_all=True`
         else:
             self.dh = init_instance_by_config(perf_task_tpl["dataset"]["kwargs"]["handler"])
+        assert self.dh.dump_all is False  # otherwise, it will save all the detailed data
 
         seg = perf_task_tpl["dataset"]["kwargs"]["segments"]
 
         # We want to split the training time period into small segments.
         perf_task_tpl["dataset"]["kwargs"]["segments"] = {
             "train": (DatasetH.get_min_time(seg), DatasetH.get_max_time(seg)),
             "test": (None, None),
@@ -73,15 +75,15 @@
         gen_task = task_generator(perf_task_tpl, [rg])
 
         recorders = R.list_recorders(experiment_name=self.exp_name)
         if len(gen_task) == len(recorders):
             get_module_logger("Internal Data").info("the data has been initialized")
         else:
             # train new models
-            assert 0 == len(recorders), "An empty experiment is required for setup `InternalData``"
+            assert 0 == len(recorders), "An empty experiment is required for setup `InternalData`"
             trainer.train(gen_task)
 
         # 2) extract the similarity matrix
         label_df = self.dh.fetch(col_set="label")
         # for
         recorders = R.list_recorders(experiment_name=self.exp_name)
 
@@ -115,27 +117,32 @@
 
 
 class MetaTaskDS(MetaTask):
     """Meta Task for Data Selection"""
 
     def __init__(self, task: dict, meta_info: pd.DataFrame, mode: str = MetaTask.PROC_MODE_FULL, fill_method="max"):
         """
+
         The description of the processed data
 
             time_perf: A array with shape  <hist_step_n * step, data pieces>  ->  data piece performance
 
             time_belong:  A array with shape <sample, data pieces>  -> belong or not (1. or 0.)
             array([[1., 0., 0., ..., 0., 0., 0.],
                    [1., 0., 0., ..., 0., 0., 0.],
                    [1., 0., 0., ..., 0., 0., 0.],
                    ...,
                    [0., 0., 0., ..., 0., 0., 1.],
                    [0., 0., 0., ..., 0., 0., 1.],
                    [0., 0., 0., ..., 0., 0., 1.]])
 
+        Parameters
+        ----------
+        meta_info: pd.DataFrame
+            please refer to the docs of _prepare_meta_ipt for detailed explanation.
         """
         super().__init__(task, meta_info)
         self.fill_method = fill_method
 
         time_perf = self._get_processed_meta_info()
         self.processed_meta_input = {"time_perf": time_perf}
         # FIXME: memory issue in this step
@@ -176,20 +183,49 @@
                 )
             )
 
         # TODO: set device: I think this is not necessary to converting data format.
         self.processed_meta_input = data_to_tensor(self.processed_meta_input)
 
     def _get_processed_meta_info(self):
-        meta_info_norm = self.meta_info.sub(self.meta_info.mean(axis=1), axis=0)  # .fillna(0.)
-        if self.fill_method == "max":
-            meta_info_norm = meta_info_norm.T.fillna(
-                meta_info_norm.max(axis=1)
-            ).T  # fill it with row max to align with previous implementation
+        meta_info_norm = self.meta_info.sub(self.meta_info.mean(axis=1), axis=0)
+        if self.fill_method.startswith("max"):
+            suffix = self.fill_method.lstrip("max")
+            if suffix == "seg":
+                fill_value = {}
+                for col in meta_info_norm.columns:
+                    fill_value[col] = meta_info_norm.loc[meta_info_norm[col].isna(), :].dropna(axis=1).mean().max()
+                fill_value = pd.Series(fill_value).sort_index()
+                # The NaN Values are filled segment-wise. Below is an exampleof fill_value
+                # 2009-01-05  2009-02-06    0.145809
+                # 2009-02-09  2009-03-06    0.148005
+                # 2009-03-09  2009-04-03    0.090385
+                # 2009-04-07  2009-05-05    0.114318
+                # 2009-05-06  2009-06-04    0.119328
+                # ...
+                meta_info_norm = meta_info_norm.fillna(fill_value)
+            else:
+                if len(suffix) > 0:
+                    get_module_logger("MetaTaskDS").warning(
+                        f"fill_method={self.fill_method}; the info after can't be correctly parsed. Please check your parameters."
+                    )
+                fill_value = meta_info_norm.max(axis=1)
+                # fill it with row max to align with previous implementation
+                # This will magnify the data similarity when data is in daily freq
+
+                # the fill value corresponds to data like this
+                # It get a performance value for each day.
+                # The performance value are get from other models on this day
+                # 2009-01-16    0.276320
+                # 2009-01-19    0.280603
+                #                 ...
+                # 2011-06-27    0.203773
+                meta_info_norm = meta_info_norm.T.fillna(fill_value).T
         elif self.fill_method == "zero":
+            # It will fillna(0.0) at the end.
             pass
         else:
             raise NotImplementedError(f"This type of input is not supported")
         meta_info_norm = meta_info_norm.fillna(0.0)  # always fill zero in case of NaN
         return meta_info_norm
 
     def get_meta_input(self):
@@ -282,31 +318,61 @@
                     MetaTaskDS(t, meta_info=self._prepare_meta_ipt(t), mode=task_mode, fill_method=fill_method)
                 )
                 self.task_list.append(t)
             except ValueError as e:
                 logger.warning(f"ValueError: {e}")
         assert len(self.meta_task_l) > 0, "No meta tasks found. Please check the data and setting"
 
-    def _prepare_meta_ipt(self, task):
+    def _prepare_meta_ipt(self, task) -> pd.DataFrame:
+        """
+        Please refer to `self.internal_data.setup` for detailed information about `self.internal_data.data_ic_df`
+
+        Indices with format below can be successfully sliced by  `ic_df.loc[:end, pd.IndexSlice[:, :end]]`
+
+               2021-06-21 2021-06-04 .. 2021-03-22 2021-03-08
+               2021-07-02 2021-06-18 .. 2021-04-02 None
+
+        Returns
+        -------
+            a pd.DataFrame with similar content below.
+            - each column corresponds to a trained model named by the training data range
+            - each row corresponds to a day of data tested by the models of the columns
+            - The rows cells that overlaps with the data used by columns are masked
+
+
+                       2009-01-05 2009-02-09 ... 2011-04-27 2011-05-26
+                       2009-02-06 2009-03-06 ... 2011-05-25 2011-06-23
+            datetime                         ...
+            2009-01-13        NaN   0.310639 ...  -0.169057   0.137792
+            2009-01-14        NaN   0.261086 ...  -0.143567   0.082581
+            ...               ...        ... ...        ...        ...
+            2011-06-30  -0.054907  -0.020219 ...  -0.023226        NaN
+            2011-07-01  -0.075762  -0.026626 ...  -0.003167        NaN
+
+        """
         ic_df = self.internal_data.data_ic_df
 
         segs = task["dataset"]["kwargs"]["segments"]
         end = max(segs[k][1] for k in ("train", "valid") if k in segs)
         ic_df_avail = ic_df.loc[:end, pd.IndexSlice[:, :end]]
 
         # meta data set focus on the **information** instead of preprocess
-        # 1) filter the future info
-        def mask_future(s):
-            """mask future information"""
-            # from qlib.utils import get_date_by_shift
+        # 1) filter the overlap info
+        def mask_overlap(s):
+            """
+            mask overlap information
+            data after self.name[end] with self.trunc_days that contains future info are also considered as overlap info
+
+            Approximately the diagnal + horizon length of data are masked.
+            """
             start, end = s.name
             end = get_date_by_shift(trading_date=end, shift=self.trunc_days - 1, future=True)
             return s.mask((s.index >= start) & (s.index <= end))
 
-        ic_df_avail = ic_df_avail.apply(mask_future)  # apply to each col
+        ic_df_avail = ic_df_avail.apply(mask_overlap)  # apply to each col
 
         # 2) filter the info with too long periods
         total_len = self.step * self.hist_step_n
         if ic_df_avail.shape[0] >= total_len:
             return ic_df_avail.iloc[-total_len:]
         else:
             raise ValueError("the history of distribution data is not long enough.")
```

## qlib/contrib/meta/data_selection/model.py

```diff
@@ -48,23 +48,25 @@
         hist_step_n,
         clip_method="tanh",
         clip_weight=2.0,
         criterion="ic_loss",
         lr=0.0001,
         max_epoch=100,
         seed=43,
+        alpha=0.0,
     ):
         self.step = step
         self.hist_step_n = hist_step_n
         self.clip_method = clip_method
         self.clip_weight = clip_weight
         self.criterion = criterion
         self.lr = lr
         self.max_epoch = max_epoch
         self.fitted = False
+        self.alpha = alpha
         torch.manual_seed(seed)
 
     def run_epoch(self, phase, task_list, epoch, opt, loss_l, ignore_weight=False):
         if phase == "train":
             self.tn.train()
             torch.set_grad_enabled(True)
         else:
@@ -140,15 +142,19 @@
 
         if len(meta_tasks_l[1]):
             R.log_params(
                 **dict(proxy_test_begin=meta_tasks_l[1][0].task["dataset"]["kwargs"]["segments"]["test"])
             )  # debug: record when the test phase starts
 
         self.tn = PredNet(
-            step=self.step, hist_step_n=self.hist_step_n, clip_weight=self.clip_weight, clip_method=self.clip_method
+            step=self.step,
+            hist_step_n=self.hist_step_n,
+            clip_weight=self.clip_weight,
+            clip_method=self.clip_method,
+            alpha=self.alpha,
         )
 
         opt = optim.Adam(self.tn.parameters(), lr=self.lr)
 
         # run weight with no weight
         for phase, task_list in zip(phases, meta_tasks_l):
             self.run_epoch(f"{phase}_noweight", task_list, 0, opt, {}, ignore_weight=True)
```

## qlib/contrib/meta/data_selection/net.py

```diff
@@ -37,31 +37,38 @@
             if time_belong is None:
                 return weights
             else:
                 return time_belong @ weights
 
 
 class PredNet(nn.Module):
-    def __init__(self, step, hist_step_n, clip_weight=None, clip_method="tanh"):
+    def __init__(self, step, hist_step_n, clip_weight=None, clip_method="tanh", alpha: float = 0.0):
+        """
+        Parameters
+        ----------
+        alpha : float
+            the regularization for sub model (useful when align meta model with linear submodel)
+        """
         super().__init__()
         self.step = step
         self.twm = TimeWeightMeta(hist_step_n=hist_step_n, clip_weight=clip_weight, clip_method=clip_method)
         self.init_paramters(hist_step_n)
+        self.alpha = alpha
 
     def get_sample_weights(self, X, time_perf, time_belong, ignore_weight=False):
         weights = torch.from_numpy(np.ones(X.shape[0])).float().to(X.device)
         if not ignore_weight:
             if time_perf is not None:
                 weights_t = self.twm(time_perf, time_belong)
                 weights = weights * weights_t
         return weights
 
     def forward(self, X, y, time_perf, time_belong, X_test, ignore_weight=False):
         """Please refer to the docs of MetaTaskDS for the description of the variables"""
         weights = self.get_sample_weights(X, time_perf, time_belong, ignore_weight=ignore_weight)
         X_w = X.T * weights.view(1, -1)
-        theta = torch.inverse(X_w @ X) @ X_w @ y
+        theta = torch.inverse(X_w @ X + self.alpha * torch.eye(X_w.shape[0])) @ X_w @ y
         return X_test @ theta, weights
 
     def init_paramters(self, hist_step_n):
         self.twm.linear.weight.data = 1.0 / hist_step_n + self.twm.linear.weight.data * 0.01
         self.twm.linear.bias.data.fill_(0.0)
```

## qlib/contrib/meta/data_selection/utils.py

```diff
@@ -1,14 +1,17 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 
 import numpy as np
 import torch
 from torch import nn
 
+from qlib.constant import EPS
+from qlib.log import get_module_logger
+
 
 class ICLoss(nn.Module):
     def forward(self, pred, y, idx, skip_size=50):
         """forward.
         FIXME:
         - Some times it will be a slightly different from the result from `pandas.corr()`
         - It may be caused by the precision problem of model;
@@ -20,31 +23,42 @@
         prev = None
         diff_point = []
         for i, (date, inst) in enumerate(idx):
             if date != prev:
                 diff_point.append(i)
             prev = date
         diff_point.append(None)
+        # The lengths of diff_point will be one more larger then diff_point
 
         ic_all = 0.0
         skip_n = 0
         for start_i, end_i in zip(diff_point, diff_point[1:]):
             pred_focus = pred[start_i:end_i]  # TODO: just for fake
             if pred_focus.shape[0] < skip_size:
                 # skip some days which have very small amount of stock.
                 skip_n += 1
                 continue
             y_focus = y[start_i:end_i]
+            if pred_focus.std() < EPS or y_focus.std() < EPS:
+                # These cases often happend at the end of test data.
+                # Usually caused by fillna(0.)
+                skip_n += 1
+                continue
+
             ic_day = torch.dot(
                 (pred_focus - pred_focus.mean()) / np.sqrt(pred_focus.shape[0]) / pred_focus.std(),
                 (y_focus - y_focus.mean()) / np.sqrt(y_focus.shape[0]) / y_focus.std(),
             )
             ic_all += ic_day
         if len(diff_point) - 1 - skip_n <= 0:
-            raise ValueError("No enough data for calculating iC")
+            raise ValueError("No enough data for calculating IC")
+        if skip_n > 0:
+            get_module_logger("ICLoss").info(
+                f"{skip_n} days are skipped due to zero std or small scale of valid samples."
+            )
         ic_mean = ic_all / (len(diff_point) - 1 - skip_n)
         return -ic_mean  # ic loss
 
 
 def preds_to_weight_with_clamp(preds, clip_weight=None, clip_method="tanh"):
     """
     Clip the weights.
```

## qlib/contrib/model/linear.py

```diff
@@ -1,13 +1,14 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 
 import numpy as np
 import pandas as pd
 from typing import Text, Union
+from qlib.log import get_module_logger
 from qlib.data.dataset.weight import Reweighter
 from scipy.optimize import nnls
 from sklearn.linear_model import LinearRegression, Ridge, Lasso
 
 from ...model.base import Model
 from ...data.dataset import DatasetH
 from ...data.dataset.handler import DataHandlerLP
@@ -25,37 +26,47 @@
     """
 
     OLS = "ols"
     NNLS = "nnls"
     RIDGE = "ridge"
     LASSO = "lasso"
 
-    def __init__(self, estimator="ols", alpha=0.0, fit_intercept=False):
+    def __init__(self, estimator="ols", alpha=0.0, fit_intercept=False, include_valid: bool = False):
         """
         Parameters
         ----------
         estimator : str
             which estimator to use for linear regression
         alpha : float
             l1 or l2 regularization parameter
         fit_intercept : bool
             whether fit intercept
+        include_valid: bool
+            Should the validation data be included for training?
+            The validation data should be included
         """
         assert estimator in [self.OLS, self.NNLS, self.RIDGE, self.LASSO], f"unsupported estimator `{estimator}`"
         self.estimator = estimator
 
         assert alpha == 0 or estimator in [self.RIDGE, self.LASSO], f"alpha is only supported in `ridge`&`lasso`"
         self.alpha = alpha
 
         self.fit_intercept = fit_intercept
 
         self.coef_ = None
+        self.include_valid = include_valid
 
     def fit(self, dataset: DatasetH, reweighter: Reweighter = None):
         df_train = dataset.prepare("train", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+        if self.include_valid:
+            try:
+                df_valid = dataset.prepare("valid", col_set=["feature", "label"], data_key=DataHandlerLP.DK_L)
+                df_train = pd.concat([df_train, df_valid])
+            except KeyError:
+                get_module_logger("LinearModel").info("include_valid=True, but valid does not exist")
         if df_train.empty:
             raise ValueError("Empty data from dataset, please check your dataset config.")
         if reweighter is not None:
             w: pd.Series = reweighter.reweight(df_train)
             w = w.values
         else:
             w = None
```

## qlib/contrib/model/pytorch_nn.py

```diff
@@ -43,33 +43,27 @@
         input dimension
     output_dim : int
         output dimension
     layers : tuple
         layer sizes
     lr : float
         learning rate
-    lr_decay : float
-        learning rate decay
-    lr_decay_steps : int
-        learning rate decay steps
     optimizer : str
         optimizer name
     GPU : int
         the GPU ID used for training
     """
 
     def __init__(
         self,
         lr=0.001,
         max_steps=300,
         batch_size=2000,
         early_stop_rounds=50,
         eval_steps=20,
-        lr_decay=0.96,
-        lr_decay_steps=100,
         optimizer="gd",
         loss="mse",
         GPU=0,
         seed=None,
         weight_decay=0.0,
         data_parall=False,
         scheduler: Optional[Union[Callable]] = "default",  # when it is Callable, it accept one argument named optimizer
@@ -89,16 +83,14 @@
 
         # set hyper-parameters.
         self.lr = lr
         self.max_steps = max_steps
         self.batch_size = batch_size
         self.early_stop_rounds = early_stop_rounds
         self.eval_steps = eval_steps
-        self.lr_decay = lr_decay
-        self.lr_decay_steps = lr_decay_steps
         self.optimizer = optimizer.lower()
         self.loss_type = loss
         if isinstance(GPU, str):
             self.device = torch.device(GPU)
         else:
             self.device = torch.device("cuda:%d" % (GPU) if torch.cuda.is_available() and GPU >= 0 else "cpu")
         self.seed = seed
@@ -112,16 +104,14 @@
         self.logger.info(
             "DNN parameters setting:"
             f"\nlr : {lr}"
             f"\nmax_steps : {max_steps}"
             f"\nbatch_size : {batch_size}"
             f"\nearly_stop_rounds : {early_stop_rounds}"
             f"\neval_steps : {eval_steps}"
-            f"\nlr_decay : {lr_decay}"
-            f"\nlr_decay_steps : {lr_decay_steps}"
             f"\noptimizer : {optimizer}"
             f"\nloss_type : {loss}"
             f"\nseed : {seed}"
             f"\ndevice : {self.device}"
             f"\nuse_GPU : {self.use_gpu}"
             f"\nweight_decay : {weight_decay}"
             f"\nenable data parall : {self.data_parall}"
```

## qlib/contrib/model/pytorch_tcn_ts.py

```diff
@@ -164,15 +164,16 @@
         raise ValueError("unknown metric `%s`" % self.metric)
 
     def train_epoch(self, data_loader):
 
         self.TCN_model.train()
 
         for data in data_loader:
-            feature = data[:, :, 0:-1].to(self.device)
+            data = torch.transpose(data, 1, 2)
+            feature = data[:, 0:-1, :].to(self.device)
             label = data[:, -1, -1].to(self.device)
 
             pred = self.TCN_model(feature.float())
             loss = self.loss_fn(pred, label)
 
             self.train_optimizer.zero_grad()
             loss.backward()
@@ -183,16 +184,16 @@
 
         self.TCN_model.eval()
 
         scores = []
         losses = []
 
         for data in data_loader:
-
-            feature = data[:, :, 0:-1].to(self.device)
+            data = torch.transpose(data, 1, 2)
+            feature = data[:, 0:-1, :].to(self.device)
             # feature[torch.isnan(feature)] = 0
             label = data[:, -1, -1].to(self.device)
 
             with torch.no_grad():
                 pred = self.TCN_model(feature.float())
                 loss = self.loss_fn(pred, label)
                 losses.append(loss.item())
```

## qlib/contrib/ops/high_freq.py

```diff
@@ -66,33 +66,35 @@
     Returns
     ----------
     feature:
         a series of that each value equals the cumsum value during start time and end time.
         Otherwise, the value is zero.
     """
 
-    def __init__(self, feature, start: str = "9:30", end: str = "14:59"):
+    def __init__(self, feature, start: str = "9:30", end: str = "14:59", data_granularity: int = 1):
         self.feature = feature
         self.start = datetime.strptime(start, "%H:%M")
         self.end = datetime.strptime(end, "%H:%M")
 
         self.morning_open = datetime.strptime("9:30", "%H:%M")
         self.morning_close = datetime.strptime("11:30", "%H:%M")
         self.noon_open = datetime.strptime("13:00", "%H:%M")
         self.noon_close = datetime.strptime("15:00", "%H:%M")
 
-        self.start_id = time_to_day_index(self.start)
-        self.end_id = time_to_day_index(self.end)
+        self.data_granularity = data_granularity
+        self.start_id = time_to_day_index(self.start) // self.data_granularity
+        self.end_id = time_to_day_index(self.end) // self.data_granularity
+        assert 240 % self.data_granularity == 0
 
     def period_cusum(self, df):
         df = df.copy()
-        assert len(df) == 240
+        assert len(df) == 240 // self.data_granularity
         df.iloc[0 : self.start_id] = 0
         df = df.cumsum()
-        df.iloc[self.end_id + 1 : 240] = 0
+        df.iloc[self.end_id + 1 : 240 // self.data_granularity] = 0
         return df
 
     def _load_internal(self, instrument, start_index, end_index, freq):
         _calendar = get_calendar_day(freq=freq)
         series = self.feature.load(instrument, start_index, end_index, freq)
         return series.groupby(_calendar[series.index]).transform(self.period_cusum)
```

## qlib/contrib/strategy/rule_strategy.py

```diff
@@ -631,15 +631,15 @@
 
         """
         super().__init__(*args, **kwargs)
         if isinstance(file, pd.DataFrame):
             self.order_df = file
         else:
             with get_io_object(file) as f:
-                self.order_df = pd.read_csv(f, dtype={"datetime": np.str})
+                self.order_df = pd.read_csv(f, dtype={"datetime": str})
 
         self.order_df["datetime"] = self.order_df["datetime"].apply(pd.Timestamp)
         self.order_df = self.order_df.set_index(["datetime", "instrument"])
 
         # make sure the datetime is the first level for fast indexing
         self.order_df = lazy_sort_index(convert_index_format(self.order_df, level="datetime"))
         self.trade_range = trade_range
```

## qlib/data/data.py

```diff
@@ -779,36 +779,36 @@
 
         if not field.endswith("_q") and not field.endswith("_a"):
             raise ValueError("period field must ends with '_q' or '_a'")
         quarterly = field.endswith("_q")
         index_path = C.dpm.get_data_uri() / "financial" / instrument.lower() / f"{field}.index"
         data_path = C.dpm.get_data_uri() / "financial" / instrument.lower() / f"{field}.data"
         if not (index_path.exists() and data_path.exists()):
-            raise FileNotFoundError("No file is found. Raise exception and  ")
+            raise FileNotFoundError("No file is found.")
         # NOTE: The most significant performance loss is here.
         # Does the acceleration that makes the program complicated really matters?
         # - It makes parameters of the interface complicate
         # - It does not performance in the optimal way (places all the pieces together, we may achieve higher performance)
         #    - If we design it carefully, we can go through for only once to get the historical evolution of the data.
         # So I decide to deprecated previous implementation and keep the logic of the program simple
         # Instead, I'll add a cache for the index file.
         data = np.fromfile(data_path, dtype=DATA_RECORDS)
 
         # find all revision periods before `cur_time`
         cur_time_int = int(cur_time.year) * 10000 + int(cur_time.month) * 100 + int(cur_time.day)
         loc = np.searchsorted(data["date"], cur_time_int, side="right")
         if loc <= 0:
-            return pd.Series()
+            return pd.Series(dtype=C.pit_record_type["value"])
         last_period = data["period"][:loc].max()  # return the latest quarter
         first_period = data["period"][:loc].min()
         period_list = get_period_list(first_period, last_period, quarterly)
         if period is not None:
             # NOTE: `period` has higher priority than `start_index` & `end_index`
             if period not in period_list:
-                return pd.Series()
+                return pd.Series(dtype=C.pit_record_type["value"])
             else:
                 period_list = [period]
         else:
             period_list = period_list[max(0, len(period_list) + start_index - 1) : len(period_list) + end_index]
         value = np.full((len(period_list),), np.nan, dtype=VALUE_DTYPE)
         for i, p in enumerate(period_list):
             # last_period_index = self.period_index[field].get(period)  # For acceleration
@@ -864,15 +864,15 @@
                 f"instrument={instrument}, field=({field}), start_time={start_time}, end_time={end_time}, freq={freq}. "
                 f"error info: {str(e)}"
             )
             raise
         # Ensure that each column type is consistent
         # FIXME:
         # 1) The stock data is currently float. If there is other types of data, this part needs to be re-implemented.
-        # 2) The the precision should be configurable
+        # 2) The precision should be configurable
         try:
             series = series.astype(np.float32)
         except ValueError:
             pass
         except TypeError:
             pass
         if not series.empty:
```

## qlib/data/dataset/__init__.py

```diff
@@ -413,15 +413,15 @@
         if flt_data is not None:
             if isinstance(flt_data, pd.DataFrame):
                 assert len(flt_data.columns) == 1
                 flt_data = flt_data.iloc[:, 0]
             # NOTE: bool(np.nan) is True !!!!!!!!
             # make sure reindex comes first. Otherwise extra NaN may appear.
             flt_data = flt_data.swaplevel()
-            flt_data = flt_data.reindex(self.data_index).fillna(False).astype(np.bool)
+            flt_data = flt_data.reindex(self.data_index).fillna(False).astype(bool)
             self.flt_data = flt_data.values
             self.idx_map = self.flt_idx_map(self.flt_data, self.idx_map)
             self.data_index = self.data_index[np.where(self.flt_data)[0]]
         self.idx_map = self.idx_map2arr(self.idx_map)
         self.idx_map, self.data_index = self.slice_idx_map_and_data_index(
             self.idx_map, self.idx_df, self.data_index, start, end
         )
```

## qlib/data/dataset/handler.py

```diff
@@ -3,14 +3,15 @@
 
 # coding=utf-8
 import warnings
 from typing import Callable, Union, Tuple, List, Iterator, Optional
 
 import pandas as pd
 
+from qlib.typehint import Literal
 from ...log import get_module_logger, TimeInspector
 from ...utils import init_instance_by_config
 from ...utils.serial import Serializable
 from .utils import fetch_df_by_index, fetch_df_by_col
 from ...utils import lazy_sort_index
 from .loader import DataLoader
 
@@ -45,14 +46,16 @@
                    SH600005    37.796539  12231662.0       38.258602        37.919757    0.970325  0.0289
 
 
     Tips for improving the performance of datahandler
     - Fetching data with `col_set=CS_RAW` will return the raw data and may avoid pandas from copying the data when calling `loc`
     """
 
+    _data: pd.DataFrame  # underlying data.
+
     def __init__(
         self,
         instruments=None,
         start_time=None,
         end_time=None,
         data_loader: Union[dict, str, DataLoader] = None,
         init_data=True,
@@ -151,14 +154,19 @@
         col_set: Union[str, List[str]] = CS_ALL,
         squeeze: bool = False,
         proc_func: Callable = None,
     ) -> pd.DataFrame:
         """
         fetch data from underlying data source
 
+        Design motivation:
+        - providing a unified interface for underlying data.
+        - Potential to make the interface more friendly.
+        - User can improve performance when fetching data in this extra layer
+
         Parameters
         ----------
         selector : Union[pd.Timestamp, slice, str]
             describe how to select data by index
             It can be categories as following
 
             - fetch single index
@@ -324,14 +332,17 @@
         if min_periods is None:
             min_periods = periods
         for cur_date in trading_dates[min_periods:]:
             selector = self.get_range_selector(cur_date, periods)
             yield cur_date, self.fetch(selector, **kwargs)
 
 
+DATA_KEY_TYPE = Literal["raw", "infer", "learn"]
+
+
 class DataHandlerLP(DataHandler):
     """
     DataHandler with **(L)earnable (P)rocessor**
 
     This handler will produce three pieces of data in pd.DataFrame format.
 
     - DK_R / self._data: the raw data loaded from the loader
@@ -342,25 +353,36 @@
     Here are some examples.
 
     - The instrument universe for learning and inference may be different.
     - The processing of some samples may rely on label (for example, some samples hit the limit may need extra processing or be dropped).
 
         - These processors only apply to the learning phase.
 
-    Tips to improve the performance of data handler
+    Tips for data handler
 
     - To reduce the memory cost
 
         - `drop_raw=True`: this will modify the data inplace on raw data;
+
+    - Please note processed data like `self._infer` or `self._learn` are concepts different from `segments` in Qlib's `Dataset` like "train" and "test"
+
+        - Processed data like `self._infer` or `self._learn` are underlying data processed with different processors
+        - `segments` in Qlib's `Dataset` like "train" and "test" are simply the time segmentations when querying data("train" are often before "test" in time-series).
+        - For example, you can query `data._infer` processed by `infer_processors` in the "train" time segmentation.
     """
 
+    # based on `self._data`, _infer and _learn are genrated after processors
+    _infer: pd.DataFrame  # data for inference
+    _learn: pd.DataFrame  # data for learning models
+
     # data key
-    DK_R = "raw"
-    DK_I = "infer"
-    DK_L = "learn"
+    DK_R: DATA_KEY_TYPE = "raw"
+    DK_I: DATA_KEY_TYPE = "infer"
+    DK_L: DATA_KEY_TYPE = "learn"
+    # map data_key to attribute name
     ATTR_MAP = {DK_R: "_data", DK_I: "_infer", DK_L: "_learn"}
 
     # process type
     PTYPE_I = "independent"
     # - self._infer will be processed by shared_processors + infer_processors
     # - self._learn will be processed by shared_processors + learn_processors
 
@@ -596,28 +618,28 @@
             elif init_type == DataHandlerLP.IT_FIT_SEQ:
                 self.fit_process_data()
             else:
                 raise NotImplementedError(f"This type of input is not supported")
 
         # TODO: Be able to cache handler data. Save the memory for data processing
 
-    def _get_df_by_key(self, data_key: str = DK_I) -> pd.DataFrame:
+    def _get_df_by_key(self, data_key: DATA_KEY_TYPE = DK_I) -> pd.DataFrame:
         if data_key == self.DK_R and self.drop_raw:
             raise AttributeError(
                 "DataHandlerLP has not attribute _data, please set drop_raw = False if you want to use raw data"
             )
         df = getattr(self, self.ATTR_MAP[data_key])
         return df
 
     def fetch(
         self,
         selector: Union[pd.Timestamp, slice, str] = slice(None, None),
         level: Union[str, int] = "datetime",
         col_set=DataHandler.CS_ALL,
-        data_key: str = DK_I,
+        data_key: DATA_KEY_TYPE = DK_I,
         squeeze: bool = False,
         proc_func: Callable = None,
     ) -> pd.DataFrame:
         """
         fetch data from underlying data source
 
         Parameters
@@ -643,23 +665,23 @@
             selector=selector,
             level=level,
             col_set=col_set,
             squeeze=squeeze,
             proc_func=proc_func,
         )
 
-    def get_cols(self, col_set=DataHandler.CS_ALL, data_key: str = DK_I) -> list:
+    def get_cols(self, col_set=DataHandler.CS_ALL, data_key: DATA_KEY_TYPE = DK_I) -> list:
         """
         get the column names
 
         Parameters
         ----------
         col_set : str
             select a set of meaningful columns.(e.g. features, columns).
-        data_key : str
+        data_key : DATA_KEY_TYPE
             the data to fetch:  DK_*.
 
         Returns
         -------
         list:
             list of column names
         """
@@ -694,7 +716,30 @@
             "start_time",
             "end_time",
             "fetch_orig",
             "drop_raw",
         ]:
             setattr(new_hd, key, getattr(handler, key, None))
         return new_hd
+
+    @classmethod
+    def from_df(cls, df: pd.DataFrame) -> "DataHandlerLP":
+        """
+        Motivation:
+        - When user want to get a quick data handler.
+
+        The created data handler will have only one shared Dataframe without processors.
+        After creating the handler, user may often want to dump the handler for reuse
+        Here is a typical use case
+
+        .. code-block:: python
+
+            from qlib.data.dataset import DataHandlerLP
+            dh = DataHandlerLP.from_df(df)
+            dh.to_pickle(fname, dump_all=True)
+
+        TODO:
+        - The StaticDataLoader is quite slow. It don't have to copy the data again...
+
+        """
+        loader = data_loader_module.StaticDataLoader(df)
+        return cls(data_loader=loader)
```

## qlib/data/dataset/loader.py

```diff
@@ -149,50 +149,53 @@
 
     def __init__(
         self,
         config: Tuple[list, tuple, dict],
         filter_pipe: List = None,
         swap_level: bool = True,
         freq: Union[str, dict] = "day",
-        inst_processor: dict = None,
+        inst_processors: Union[dict, list] = None,
     ):
         """
         Parameters
         ----------
         config : Tuple[list, tuple, dict]
             Please refer to the doc of DLWParser
         filter_pipe :
             Filter pipe for the instruments
         swap_level :
             Whether to swap level of MultiIndex
         freq:  dict or str
             If type(config) == dict and type(freq) == str, load config data using freq.
             If type(config) == dict and type(freq) == dict, load config[<group_name>] data using freq[<group_name>]
-        inst_processor: dict
-            If inst_processor is not None and type(config) == dict; load config[<group_name>] data using inst_processor[<group_name>]
+        inst_processors: dict | list
+            If inst_processors is not None and type(config) == dict; load config[<group_name>] data using inst_processors[<group_name>]
+            If inst_processors is a list, then it will be applied to all groups.
         """
         self.filter_pipe = filter_pipe
         self.swap_level = swap_level
         self.freq = freq
 
         # sample
-        self.inst_processor = inst_processor if inst_processor is not None else {}
-        assert isinstance(self.inst_processor, dict), f"inst_processor(={self.inst_processor}) must be dict"
+        self.inst_processors = inst_processors if inst_processors is not None else {}
+        assert isinstance(
+            self.inst_processors, (dict, list)
+        ), f"inst_processors(={self.inst_processors}) must be dict or list"
 
         super().__init__(config)
 
         if self.is_group:
             # check sample config
             if isinstance(freq, dict):
                 for _gp in config.keys():
                     if _gp not in freq:
                         raise ValueError(f"freq(={freq}) missing group(={_gp})")
                 assert (
-                    self.inst_processor
-                ), f"freq(={self.freq}), inst_processor(={self.inst_processor}) cannot be None/empty"
+                    self.inst_processors
+                ), f"freq(={self.freq}), inst_processors(={self.inst_processors}) cannot be None/empty"
 
     def load_group_df(
         self,
         instruments,
         exprs: list,
         names: list,
         start_time: Union[str, pd.Timestamp] = None,
@@ -204,17 +207,18 @@
             instruments = "all"
         if isinstance(instruments, str):
             instruments = D.instruments(instruments, filter_pipe=self.filter_pipe)
         elif self.filter_pipe is not None:
             warnings.warn("`filter_pipe` is not None, but it will not be used with `instruments` as list")
 
         freq = self.freq[gp_name] if isinstance(self.freq, dict) else self.freq
-        df = D.features(
-            instruments, exprs, start_time, end_time, freq=freq, inst_processors=self.inst_processor.get(gp_name, [])
+        inst_processors = (
+            self.inst_processors if isinstance(self.inst_processors, list) else self.inst_processors.get(gp_name, [])
         )
+        df = D.features(instruments, exprs, start_time, end_time, freq=freq, inst_processors=inst_processors)
         df.columns = names
         if self.swap_level:
             df = df.swaplevel().sort_index()  # NOTE: if swaplevel, return <datetime, instrument>
         return df
 
 
 class StaticDataLoader(DataLoader, Serializable):
```

## qlib/data/dataset/processor.py

```diff
@@ -1,20 +1,22 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 
 import abc
-from typing import Union, Text
+from typing import Union, Text, Optional
 import numpy as np
 import pandas as pd
 
 from qlib.utils.data import robust_zscore, zscore
 from ...constant import EPS
 from .utils import fetch_df_by_index
 from ...utils.serial import Serializable
 from ...utils.paral import datetime_groupby_apply
+from qlib.data.inst_processor import InstProcessor
+from qlib.data import D
 
 
 def get_group_columns(df: pd.DataFrame, group: Union[Text, None]):
     """
     get a group of columns from multi-index columns DataFrame
 
     Parameters
@@ -374,7 +376,46 @@
 class HashStockFormat(Processor):
     """Process the storage of from df into hasing stock format"""
 
     def __call__(self, df: pd.DataFrame):
         from .storage import HashingStockStorage  # pylint: disable=C0415
 
         return HashingStockStorage.from_df(df)
+
+
+class TimeRangeFlt(InstProcessor):
+    """
+    This is a filter to filter stock.
+    Only keep the data that exist from start_time to end_time (the existence in the middle is not checked.)
+    WARNING:  It may induce leakage!!!
+    """
+
+    def __init__(
+        self,
+        start_time: Optional[Union[pd.Timestamp, str]] = None,
+        end_time: Optional[Union[pd.Timestamp, str]] = None,
+        freq: str = "day",
+    ):
+        """
+        Parameters
+        ----------
+        start_time : Optional[Union[pd.Timestamp, str]]
+            The data must start earlier (or equal) than `start_time`
+            None indicates data will not be filtered based on `start_time`
+        end_time : Optional[Union[pd.Timestamp, str]]
+            similar to start_time
+        freq : str
+            The frequency of the calendar
+        """
+        # Align to calendar before filtering
+        cal = D.calendar(start_time=start_time, end_time=end_time, freq=freq)
+        self.start_time = None if start_time is None else cal[0]
+        self.end_time = None if end_time is None else cal[-1]
+
+    def __call__(self, df: pd.DataFrame, instrument, *args, **kwargs):
+        if (
+            df.empty
+            or (self.start_time is None or df.index.min() <= self.start_time)
+            and (self.end_time is None or df.index.max() >= self.end_time)
+        ):
+            return df
+        return df.head(0)
```

## qlib/data/dataset/utils.py

```diff
@@ -1,14 +1,13 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 from __future__ import annotations
 import pandas as pd
-from typing import Union, List
+from typing import Union, List, TYPE_CHECKING
 from qlib.utils import init_instance_by_config
-from typing import TYPE_CHECKING
 
 if TYPE_CHECKING:
     from qlib.data.dataset import DataHandler
 
 
 def get_level_index(df: pd.DataFrame, level=Union[str, int]) -> int:
     """
@@ -117,15 +116,15 @@
     """
 
     if get_level_index(df, level=level) == 1:
         df = df.swaplevel().sort_index()
     return df
 
 
-def init_task_handler(task: dict) -> Union[DataHandler, None]:
+def init_task_handler(task: dict) -> DataHandler:
     """
     initialize the handler part of the task **inplace**
 
     Parameters
     ----------
     task : dict
         the task to be handled
@@ -138,9 +137,10 @@
     # avoid recursive import
     from .handler import DataHandler  # pylint: disable=C0415
 
     h_conf = task["dataset"]["kwargs"].get("handler")
     if h_conf is not None:
         handler = init_instance_by_config(h_conf, accept_types=DataHandler)
         task["dataset"]["kwargs"]["handler"] = handler
-
         return handler
+    else:
+        raise ValueError("The task does not contains a handler part.")
```

## qlib/rl/contrib/backtest.py

```diff
@@ -24,22 +24,23 @@
 from qlib.rl.data.integration import init_qlib
 from qlib.rl.order_execution.simulator_qlib import SingleAssetOrderExecution
 from qlib.typehint import Literal
 
 
 def _get_multi_level_executor_config(
     strategy_config: dict,
-    cash_limit: float = None,
+    cash_limit: float | None = None,
     generate_report: bool = False,
+    data_granularity: str = "1min",
 ) -> dict:
     executor_config = {
         "class": "SimulatorExecutor",
         "module_path": "qlib.backtest.executor",
         "kwargs": {
-            "time_per_step": "1min",
+            "time_per_step": data_granularity,
             "verbose": False,
             "trade_type": SimulatorExecutor.TT_PARAL if cash_limit is not None else SimulatorExecutor.TT_SERIAL,
             "generate_report": generate_report,
             "track_data": True,
         },
     }
 
@@ -123,15 +124,15 @@
     return report
 
 
 def single_with_simulator(
     backtest_config: dict,
     orders: pd.DataFrame,
     split: Literal["stock", "day"] = "stock",
-    cash_limit: float = None,
+    cash_limit: float | None = None,
     generate_report: bool = False,
 ) -> Union[Tuple[pd.DataFrame, dict], pd.DataFrame]:
     """Run backtest in a single thread with SingleAssetOrderExecution simulator. The orders will be executed day by day.
     A new simulator will be created and used for every single-day order.
 
     Parameters
     ----------
@@ -150,20 +151,15 @@
     generate_report
         Whether to generate reports.
 
     Returns
     -------
         If generate_report is True, return execution records and the generated report. Otherwise, return only records.
     """
-    if split == "stock":
-        stock_id = orders.iloc[0].instrument
-        init_qlib(backtest_config["qlib"], part=stock_id)
-    else:
-        day = orders.iloc[0].datetime
-        init_qlib(backtest_config["qlib"], part=day)
+    init_qlib(backtest_config["qlib"])
 
     stocks = orders.instrument.unique().tolist()
 
     reports = []
     decisions = []
     for _, row in orders.iterrows():
         date = pd.Timestamp(row["datetime"])
@@ -177,36 +173,37 @@
             end_time=end_time,
         )
 
         executor_config = _get_multi_level_executor_config(
             strategy_config=backtest_config["strategies"],
             cash_limit=cash_limit,
             generate_report=generate_report,
+            data_granularity=backtest_config["data_granularity"],
         )
 
         exchange_config = copy.deepcopy(backtest_config["exchange"])
         exchange_config.update(
             {
                 "codes": stocks,
-                "freq": "1min",
+                "freq": backtest_config["data_granularity"],
             }
         )
 
         simulator = SingleAssetOrderExecution(
             order=order,
             executor_config=executor_config,
             exchange_config=exchange_config,
             qlib_config=None,
             cash_limit=None,
         )
 
         reports.append(simulator.report_dict)
         decisions += simulator.decisions
 
-    indicator_1day_objs = [report["indicator"]["1day"][1] for report in reports]
+    indicator_1day_objs = [report["indicator_dict"]["1day"][1] for report in reports]
     indicator_info = {k: v for obj in indicator_1day_objs for k, v in obj.order_indicator_his.items()}
     records = _convert_indicator_to_dataframe(indicator_info)
     assert records is None or not np.isnan(records["ffr"]).any()
 
     if generate_report:
         _report = _generate_report(decisions, [report["indicator"] for report in reports])
 
@@ -222,15 +219,15 @@
         return records
 
 
 def single_with_collect_data_loop(
     backtest_config: dict,
     orders: pd.DataFrame,
     split: Literal["stock", "day"] = "stock",
-    cash_limit: float = None,
+    cash_limit: float | None = None,
     generate_report: bool = False,
 ) -> Union[Tuple[pd.DataFrame, dict], pd.DataFrame]:
     """Run backtest in a single thread with collect_data_loop.
 
     Parameters
     ----------
     backtest_config:
@@ -249,20 +246,15 @@
         Whether to generate reports.
 
     Returns
     -------
         If generate_report is True, return execution records and the generated report. Otherwise, return only records.
     """
 
-    if split == "stock":
-        stock_id = orders.iloc[0].instrument
-        init_qlib(backtest_config["qlib"], part=stock_id)
-    else:
-        day = orders.iloc[0].datetime
-        init_qlib(backtest_config["qlib"], part=day)
+    init_qlib(backtest_config["qlib"])
 
     trade_start_time = orders["datetime"].min()
     trade_end_time = orders["datetime"].max()
     stocks = orders.instrument.unique().tolist()
 
     strategy_config = {
         "class": "FileOrderStrategy",
@@ -276,21 +268,22 @@
         },
     }
 
     executor_config = _get_multi_level_executor_config(
         strategy_config=backtest_config["strategies"],
         cash_limit=cash_limit,
         generate_report=generate_report,
+        data_granularity=backtest_config["data_granularity"],
     )
 
     exchange_config = copy.deepcopy(backtest_config["exchange"])
     exchange_config.update(
         {
             "codes": stocks,
-            "freq": "1min",
+            "freq": backtest_config["data_granularity"],
         }
     )
 
     strategy, executor = get_strategy_executor(
         start_time=pd.Timestamp(trade_start_time),
         end_time=pd.Timestamp(trade_end_time) + pd.DateOffset(1),
         strategy=strategy_config,
@@ -353,15 +346,18 @@
             pickle.dump(report, f)
         res = pd.concat([r[0] for r in res], 0)
     else:
         res = pd.concat(res)
 
     if not output_path.exists():
         os.makedirs(output_path)
-    res.to_csv(output_path / "summary.csv")
+
+    if "pa" in res.columns:
+        res["pa"] = res["pa"] * 10000.0  # align with training metrics
+    res.to_csv(output_path / "backtest_result.csv")
     return res
 
 
 if __name__ == "__main__":
     import warnings
 
     warnings.filterwarnings("ignore", category=DeprecationWarning)
```

## qlib/rl/contrib/naive_config_parser.py

```diff
@@ -94,13 +94,14 @@
     backtest_config["exchange"] = _convert_all_list_to_tuple(backtest_config["exchange"])
 
     backtest_config_default = {
         "debug_single_stock": None,
         "debug_single_day": None,
         "concurrency": -1,
         "multiplier": 1.0,
-        "output_dir": "outputs/",
+        "output_dir": "outputs_backtest/",
         "generate_report": False,
+        "data_granularity": "1min",
     }
     backtest_config = merge_a_into_b(a=backtest_config, b=backtest_config_default)
 
     return backtest_config
```

## qlib/rl/contrib/train_onpolicy.py

```diff
@@ -1,33 +1,35 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
+from __future__ import annotations
+
 import argparse
 import os
 import random
+import sys
+import warnings
 from pathlib import Path
 from typing import cast, List, Optional
 
 import numpy as np
 import pandas as pd
-import qlib
 import torch
 import yaml
 from qlib.backtest import Order
 from qlib.backtest.decision import OrderDir
 from qlib.constant import ONE_MIN
-from qlib.rl.data.pickle_styled import load_simple_intraday_backtest_data
+from qlib.rl.data.native import load_handler_intraday_processed_data
 from qlib.rl.interpreter import ActionInterpreter, StateInterpreter
 from qlib.rl.order_execution import SingleAssetOrderExecutionSimple
 from qlib.rl.reward import Reward
 from qlib.rl.trainer import Checkpoint, backtest, train
 from qlib.rl.trainer.callbacks import Callback, EarlyStopping, MetricsWriter
 from qlib.rl.utils.log import CsvWriter
 from qlib.utils import init_instance_by_config
 from tianshou.policy import BasePolicy
-from torch import nn
 from torch.utils.data import Dataset
 
 
 def seed_everything(seed: int) -> None:
     torch.manual_seed(seed)
     torch.cuda.manual_seed_all(seed)
     np.random.seed(seed)
@@ -45,45 +47,48 @@
             orders.append(order_data)
         return pd.concat(orders)
 
 
 class LazyLoadDataset(Dataset):
     def __init__(
         self,
+        data_dir: str,
         order_file_path: Path,
-        data_dir: Path,
         default_start_time_index: int,
         default_end_time_index: int,
     ) -> None:
         self._default_start_time_index = default_start_time_index
         self._default_end_time_index = default_end_time_index
 
-        self._order_file_path = order_file_path
         self._order_df = _read_orders(order_file_path).reset_index()
-
-        self._data_dir = data_dir
         self._ticks_index: Optional[pd.DatetimeIndex] = None
+        self._data_dir = Path(data_dir)
 
     def __len__(self) -> int:
         return len(self._order_df)
 
     def __getitem__(self, index: int) -> Order:
         row = self._order_df.iloc[index]
         date = pd.Timestamp(str(row["date"]))
 
         if self._ticks_index is None:
             # TODO: We only load ticks index once based on the assumption that ticks index of different dates
             # TODO: in one experiment are all the same. If that assumption is not hold, we need to load ticks index
             # TODO: of all dates.
-            backtest_data = load_simple_intraday_backtest_data(
+
+            data = load_handler_intraday_processed_data(
                 data_dir=self._data_dir,
                 stock_id=row["instrument"],
                 date=date,
+                feature_columns_today=[],
+                feature_columns_yesterday=[],
+                backtest=True,
+                index_only=True,
             )
-            self._ticks_index = [t - date for t in backtest_data.get_time_index()]
+            self._ticks_index = [t - date for t in data.today.index]
 
         order = Order(
             stock_id=row["instrument"],
             amount=row["amount"],
             direction=OrderDir(int(row["order_type"])),
             start_time=date + self._ticks_index[self._default_start_time_index],
             end_time=date + self._ticks_index[self._default_end_time_index - 1] + ONE_MIN,
@@ -97,128 +102,140 @@
     simulator_config: dict,
     trainer_config: dict,
     data_config: dict,
     state_interpreter: StateInterpreter,
     action_interpreter: ActionInterpreter,
     policy: BasePolicy,
     reward: Reward,
+    run_training: bool,
     run_backtest: bool,
 ) -> None:
-    qlib.init()
-
     order_root_path = Path(data_config["source"]["order_dir"])
 
     data_granularity = simulator_config.get("data_granularity", 1)
 
     def _simulator_factory_simple(order: Order) -> SingleAssetOrderExecutionSimple:
         return SingleAssetOrderExecutionSimple(
             order=order,
-            data_dir=Path(data_config["source"]["data_dir"]),
-            ticks_per_step=simulator_config["time_per_step"],
+            data_dir=data_config["source"]["feature_root_dir"],
+            feature_columns_today=data_config["source"]["feature_columns_today"],
+            feature_columns_yesterday=data_config["source"]["feature_columns_yesterday"],
             data_granularity=data_granularity,
-            deal_price_type=data_config["source"].get("deal_price_column", "close"),
+            ticks_per_step=simulator_config["time_per_step"],
             vol_threshold=simulator_config["vol_limit"],
         )
 
     assert data_config["source"]["default_start_time_index"] % data_granularity == 0
     assert data_config["source"]["default_end_time_index"] % data_granularity == 0
 
-    train_dataset, valid_dataset, test_dataset = [
-        LazyLoadDataset(
-            order_file_path=order_root_path / tag,
-            data_dir=Path(data_config["source"]["data_dir"]),
-            default_start_time_index=data_config["source"]["default_start_time_index"] // data_granularity,
-            default_end_time_index=data_config["source"]["default_end_time_index"] // data_granularity,
-        )
-        for tag in ("train", "valid", "test")
-    ]
+    if run_training:
+        train_dataset, valid_dataset = [
+            LazyLoadDataset(
+                data_dir=data_config["source"]["feature_root_dir"],
+                order_file_path=order_root_path / tag,
+                default_start_time_index=data_config["source"]["default_start_time_index"] // data_granularity,
+                default_end_time_index=data_config["source"]["default_end_time_index"] // data_granularity,
+            )
+            for tag in ("train", "valid")
+        ]
 
-    if "checkpoint_path" in trainer_config:
         callbacks: List[Callback] = []
-        callbacks.append(MetricsWriter(dirpath=Path(trainer_config["checkpoint_path"])))
-        callbacks.append(
-            Checkpoint(
-                dirpath=Path(trainer_config["checkpoint_path"]) / "checkpoints",
-                every_n_iters=trainer_config.get("checkpoint_every_n_iters", 1),
-                save_latest="copy",
-            ),
-        )
-    if "earlystop_patience" in trainer_config:
-        callbacks.append(
-            EarlyStopping(
-                patience=trainer_config["earlystop_patience"],
-                monitor="val/pa",
+        if "checkpoint_path" in trainer_config:
+            callbacks.append(MetricsWriter(dirpath=Path(trainer_config["checkpoint_path"])))
+            callbacks.append(
+                Checkpoint(
+                    dirpath=Path(trainer_config["checkpoint_path"]) / "checkpoints",
+                    every_n_iters=trainer_config.get("checkpoint_every_n_iters", 1),
+                    save_latest="copy",
+                ),
+            )
+        if "earlystop_patience" in trainer_config:
+            callbacks.append(
+                EarlyStopping(
+                    patience=trainer_config["earlystop_patience"],
+                    monitor="val/pa",
+                )
             )
-        )
-
-    trainer_kwargs = {
-        "max_iters": trainer_config["max_epoch"],
-        "finite_env_type": env_config["parallel_mode"],
-        "concurrency": env_config["concurrency"],
-        "val_every_n_iters": trainer_config.get("val_every_n_epoch", None),
-        "callbacks": callbacks,
-    }
-    vessel_kwargs = {
-        "episode_per_iter": trainer_config["episode_per_collect"],
-        "update_kwargs": {
-            "batch_size": trainer_config["batch_size"],
-            "repeat": trainer_config["repeat_per_collect"],
-        },
-        "val_initial_states": valid_dataset,
-    }
 
-    train(
-        simulator_fn=_simulator_factory_simple,
-        state_interpreter=state_interpreter,
-        action_interpreter=action_interpreter,
-        policy=policy,
-        reward=reward,
-        initial_states=cast(List[Order], train_dataset),
-        trainer_kwargs=trainer_kwargs,
-        vessel_kwargs=vessel_kwargs,
-    )
+        train(
+            simulator_fn=_simulator_factory_simple,
+            state_interpreter=state_interpreter,
+            action_interpreter=action_interpreter,
+            policy=policy,
+            reward=reward,
+            initial_states=cast(List[Order], train_dataset),
+            trainer_kwargs={
+                "max_iters": trainer_config["max_epoch"],
+                "finite_env_type": env_config["parallel_mode"],
+                "concurrency": env_config["concurrency"],
+                "val_every_n_iters": trainer_config.get("val_every_n_epoch", None),
+                "callbacks": callbacks,
+            },
+            vessel_kwargs={
+                "episode_per_iter": trainer_config["episode_per_collect"],
+                "update_kwargs": {
+                    "batch_size": trainer_config["batch_size"],
+                    "repeat": trainer_config["repeat_per_collect"],
+                },
+                "val_initial_states": valid_dataset,
+            },
+        )
 
     if run_backtest:
+        test_dataset = LazyLoadDataset(
+            data_dir=data_config["source"]["feature_root_dir"],
+            order_file_path=order_root_path / "test",
+            default_start_time_index=data_config["source"]["default_start_time_index"] // data_granularity,
+            default_end_time_index=data_config["source"]["default_end_time_index"] // data_granularity,
+        )
+
         backtest(
             simulator_fn=_simulator_factory_simple,
             state_interpreter=state_interpreter,
             action_interpreter=action_interpreter,
             initial_states=test_dataset,
             policy=policy,
             logger=CsvWriter(Path(trainer_config["checkpoint_path"])),
             reward=reward,
-            finite_env_type=trainer_kwargs["finite_env_type"],
-            concurrency=trainer_kwargs["concurrency"],
+            finite_env_type=env_config["parallel_mode"],
+            concurrency=env_config["concurrency"],
         )
 
 
-def main(config: dict, run_backtest: bool) -> None:
+def main(config: dict, run_training: bool, run_backtest: bool) -> None:
+    if not run_training and not run_backtest:
+        warnings.warn("Skip the entire job since training and backtest are both skipped.")
+        return
+
     if "seed" in config["runtime"]:
         seed_everything(config["runtime"]["seed"])
 
-    state_config = config["state_interpreter"]
-    state_interpreter: StateInterpreter = init_instance_by_config(state_config)
+    for extra_module_path in config["env"].get("extra_module_paths", []):
+        sys.path.append(extra_module_path)
 
+    state_interpreter: StateInterpreter = init_instance_by_config(config["state_interpreter"])
     action_interpreter: ActionInterpreter = init_instance_by_config(config["action_interpreter"])
     reward: Reward = init_instance_by_config(config["reward"])
 
+    additional_policy_kwargs = {
+        "obs_space": state_interpreter.observation_space,
+        "action_space": action_interpreter.action_space,
+    }
+
     # Create torch network
-    if "kwargs" not in config["network"]:
-        config["network"]["kwargs"] = {}
-    config["network"]["kwargs"].update({"obs_space": state_interpreter.observation_space})
-    network: nn.Module = init_instance_by_config(config["network"])
+    if "network" in config:
+        if "kwargs" not in config["network"]:
+            config["network"]["kwargs"] = {}
+        config["network"]["kwargs"].update({"obs_space": state_interpreter.observation_space})
+        additional_policy_kwargs["network"] = init_instance_by_config(config["network"])
 
     # Create policy
-    config["policy"]["kwargs"].update(
-        {
-            "network": network,
-            "obs_space": state_interpreter.observation_space,
-            "action_space": action_interpreter.action_space,
-        }
-    )
+    if "kwargs" not in config["policy"]:
+        config["policy"]["kwargs"] = {}
+    config["policy"]["kwargs"].update(additional_policy_kwargs)
     policy: BasePolicy = init_instance_by_config(config["policy"])
 
     use_cuda = config["runtime"].get("use_cuda", False)
     if use_cuda:
         policy.cuda()
 
     train_and_test(
@@ -226,26 +243,26 @@
         simulator_config=config["simulator"],
         data_config=config["data"],
         trainer_config=config["trainer"],
         action_interpreter=action_interpreter,
         state_interpreter=state_interpreter,
         policy=policy,
         reward=reward,
+        run_training=run_training,
         run_backtest=run_backtest,
     )
 
 
 if __name__ == "__main__":
-    import warnings
-
     warnings.filterwarnings("ignore", category=DeprecationWarning)
     warnings.filterwarnings("ignore", category=RuntimeWarning)
 
     parser = argparse.ArgumentParser()
     parser.add_argument("--config_path", type=str, required=True, help="Path to the config file")
-    parser.add_argument("--run_backtest", action="store_true", help="Run backtest workflow after training is finished")
+    parser.add_argument("--no_training", action="store_true", help="Skip training workflow.")
+    parser.add_argument("--run_backtest", action="store_true", help="Run backtest workflow.")
     args = parser.parse_args()
 
     with open(args.config_path, "r") as input_stream:
         config = yaml.safe_load(input_stream)
 
-    main(config, run_backtest=args.run_backtest)
+    main(config, run_training=not args.no_training, run_backtest=args.run_backtest)
```

## qlib/rl/data/integration.py

```diff
@@ -4,56 +4,22 @@
 """
 TODO: This file is used to integrate NeuTrader with Qlib to run the existing projects.
 TODO: The implementation here is kind of adhoc. It is better to design a more uniformed & general implementation.
 """
 
 from __future__ import annotations
 
-import pickle
 from pathlib import Path
-from typing import List
 
-import cachetools
-import numpy as np
-import pandas as pd
 import qlib
 from qlib.constant import REG_CN
 from qlib.contrib.ops.high_freq import BFillNan, Cut, Date, DayCumsum, DayLast, FFillNan, IsInf, IsNull, Select
-from qlib.data.dataset import DatasetH
 
-dataset = None
 
-
-class DataWrapper:
-    def __init__(
-        self,
-        feature_dataset: DatasetH,
-        backtest_dataset: DatasetH,
-        columns_today: List[str],
-        columns_yesterday: List[str],
-        _internal: bool = False,
-    ):
-        assert _internal, "Init function of data wrapper is for internal use only."
-
-        self.feature_dataset = feature_dataset
-        self.backtest_dataset = backtest_dataset
-        self.columns_today = columns_today
-        self.columns_yesterday = columns_yesterday
-
-    @cachetools.cached(  # type: ignore
-        cache=cachetools.LRUCache(100),
-        key=lambda _, stock_id, date, backtest: (stock_id, date.replace(hour=0, minute=0, second=0), backtest),
-    )
-    def get(self, stock_id: str, date: pd.Timestamp, backtest: bool = False) -> pd.DataFrame:
-        start_time, end_time = date.replace(hour=0, minute=0, second=0), date.replace(hour=23, minute=59, second=59)
-        dataset = self.backtest_dataset if backtest else self.feature_dataset
-        return dataset.handler.fetch(pd.IndexSlice[stock_id, start_time:end_time], level=None)
-
-
-def init_qlib(qlib_config: dict, part: str = None) -> None:
+def init_qlib(qlib_config: dict) -> None:
     """Initialize necessary resource to launch the workflow, including data direction, feature columns, etc..
 
     Parameters
     ----------
     qlib_config:
         Qlib configuration.
 
@@ -68,28 +34,23 @@
                     "$bidV", "$bidV1", "$bidV3", "$bidV5", "$askV", "$askV1", "$askV3", "$askV5",
                 ],
                 "feature_columns_yesterday": [
                     "$open_1", "$high_1", "$low_1", "$close_1", "$vwap_1", "$bid_1", "$ask_1", "$volume_1",
                     "$bidV_1", "$bidV1_1", "$bidV3_1", "$bidV5_1", "$askV_1", "$askV1_1", "$askV3_1", "$askV5_1",
                 ],
             }
-    part
-        Identifying which part (stock / date) to load.
     """
 
-    global dataset  # pylint: disable=W0603
-
     def _convert_to_path(path: str | Path) -> Path:
         return path if isinstance(path, Path) else Path(path)
 
     provider_uri_map = {}
-    if "provider_uri_day" in qlib_config:
-        provider_uri_map["day"] = _convert_to_path(qlib_config["provider_uri_day"]).as_posix()
-    if "provider_uri_1min" in qlib_config:
-        provider_uri_map["1min"] = _convert_to_path(qlib_config["provider_uri_1min"]).as_posix()
+    for granularity in ["1min", "5min", "day"]:
+        if f"provider_uri_{granularity}" in qlib_config:
+            provider_uri_map[f"{granularity}"] = _convert_to_path(qlib_config[f"provider_uri_{granularity}"]).as_posix()
 
     qlib.init(
         region=REG_CN,
         auto_mount=False,
         custom_ops=[DayLast, FFillNan, BFillNan, Date, Select, IsNull, IsInf, Cut, DayCumsum],
         expression_cache=None,
         calendar_provider={
@@ -115,51 +76,7 @@
             },
         },
         provider_uri=provider_uri_map,
         kernels=1,
         redis_port=-1,
         clear_mem_cache=False,  # init_qlib will be called for multiple times. Keep the cache for improving performance
     )
-
-    if part == "skip":
-        return
-
-    # this won't work if it's put outside in case of multiprocessing
-    from qlib.data import D  # noqa pylint: disable=C0415,W0611
-
-    if part is None:
-        feature_path = Path(qlib_config["feature_root_dir"]) / "feature.pkl"
-        backtest_path = Path(qlib_config["feature_root_dir"]) / "backtest.pkl"
-    else:
-        feature_path = Path(qlib_config["feature_root_dir"]) / "feature" / (part + ".pkl")
-        backtest_path = Path(qlib_config["feature_root_dir"]) / "backtest" / (part + ".pkl")
-
-    with feature_path.open("rb") as f:
-        feature_dataset = pickle.load(f)
-    with backtest_path.open("rb") as f:
-        backtest_dataset = pickle.load(f)
-
-    dataset = DataWrapper(
-        feature_dataset,
-        backtest_dataset,
-        qlib_config["feature_columns_today"],
-        qlib_config["feature_columns_yesterday"],
-        _internal=True,
-    )
-
-
-def fetch_features(stock_id: str, date: pd.Timestamp, yesterday: bool = False, backtest: bool = False) -> pd.DataFrame:
-    assert dataset is not None, "You must call init_qlib() before doing this."
-
-    if backtest:
-        fields = ["$close", "$volume"]
-    else:
-        fields = dataset.columns_yesterday if yesterday else dataset.columns_today
-
-    data = dataset.get(stock_id, date, backtest)
-    if data is None or len(data) == 0:
-        # create a fake index, but RL doesn't care about index
-        data = pd.DataFrame(0.0, index=np.arange(240), columns=fields, dtype=np.float32)  # FIXME: hardcode here
-    else:
-        data = data.rename(columns={c: c.rstrip("0") for c in data.columns})
-        data = data[fields]
-    return data
```

## qlib/rl/data/native.py

```diff
@@ -1,22 +1,34 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 from __future__ import annotations
 
-from typing import cast
+from pathlib import Path
+from typing import cast, List
 
 import cachetools
 import pandas as pd
+import pickle
+import os
 
 from qlib.backtest import Exchange, Order
 from qlib.backtest.decision import TradeRange, TradeRangeByTime
-from qlib.rl.order_execution.utils import get_ticks_slice
-
+from qlib.constant import EPS_T
 from .base import BaseIntradayBacktestData, BaseIntradayProcessedData, ProcessedDataProvider
-from .integration import fetch_features
+
+
+def get_ticks_slice(
+    ticks_index: pd.DatetimeIndex,
+    start: pd.Timestamp,
+    end: pd.Timestamp,
+    include_end: bool = False,
+) -> pd.DatetimeIndex:
+    if not include_end:
+        end = end - EPS_T
+    return ticks_index[ticks_index.slice_indexer(start, end)]
 
 
 class IntradayBacktestData(BaseIntradayBacktestData):
     """Backtest data for Qlib simulator"""
 
     def __init__(
         self,
@@ -67,14 +79,39 @@
     def get_volume(self) -> pd.Series:
         return self._volume
 
     def get_time_index(self) -> pd.DatetimeIndex:
         return pd.DatetimeIndex([e[1] for e in list(self._exchange.quote_df.index)])
 
 
+class DataframeIntradayBacktestData(BaseIntradayBacktestData):
+    """Backtest data from dataframe"""
+
+    def __init__(self, df: pd.DataFrame, price_column: str = "$close0", volume_column: str = "$volume0") -> None:
+        self.df = df
+        self.price_column = price_column
+        self.volume_column = volume_column
+
+    def __repr__(self) -> str:
+        with pd.option_context("memory_usage", False, "display.max_info_columns", 1, "display.large_repr", "info"):
+            return f"{self.__class__.__name__}({self.df})"
+
+    def __len__(self) -> int:
+        return len(self.df)
+
+    def get_deal_price(self) -> pd.Series:
+        return self.df[self.price_column]
+
+    def get_volume(self) -> pd.Series:
+        return self.df[self.volume_column]
+
+    def get_time_index(self) -> pd.DatetimeIndex:
+        return cast(pd.DatetimeIndex, self.df.index)
+
+
 @cachetools.cached(  # type: ignore
     cache=cachetools.LRUCache(100),
     key=lambda order, _, __: order.key_by_day,
 )
 def load_backtest_data(
     order: Order,
     trade_exchange: Exchange,
@@ -99,45 +136,98 @@
         exchange=trade_exchange,
         ticks_index=ticks_index,
         ticks_for_order=ticks_for_order,
     )
     return backtest_data
 
 
-class NTIntradayProcessedData(BaseIntradayProcessedData):
-    """Subclass of IntradayProcessedData. Used to handle NT style data."""
+class HandlerIntradayProcessedData(BaseIntradayProcessedData):
+    """Subclass of IntradayProcessedData. Used to handle handler (bin format) style data."""
 
     def __init__(
         self,
+        data_dir: Path,
         stock_id: str,
         date: pd.Timestamp,
+        feature_columns_today: List[str],
+        feature_columns_yesterday: List[str],
+        backtest: bool = False,
+        index_only: bool = False,
     ) -> None:
         def _drop_stock_id(df: pd.DataFrame) -> pd.DataFrame:
             df = df.reset_index()
             if "instrument" in df.columns:
                 df = df.drop(columns=["instrument"])
             return df.set_index(["datetime"])
 
-        self.today = _drop_stock_id(fetch_features(stock_id, date))
-        self.yesterday = _drop_stock_id(fetch_features(stock_id, date, yesterday=True))
+        path = os.path.join(data_dir, "backtest" if backtest else "feature", f"{stock_id}.pkl")
+        start_time, end_time = date.replace(hour=0, minute=0, second=0), date.replace(hour=23, minute=59, second=59)
+        with open(path, "rb") as fstream:
+            dataset = pickle.load(fstream)
+        data = dataset.handler.fetch(pd.IndexSlice[stock_id, start_time:end_time], level=None)
+
+        if index_only:
+            self.today = _drop_stock_id(data[[]])
+            self.yesterday = _drop_stock_id(data[[]])
+        else:
+            self.today = _drop_stock_id(data[feature_columns_today])
+            self.yesterday = _drop_stock_id(data[feature_columns_yesterday])
 
     def __repr__(self) -> str:
         with pd.option_context("memory_usage", False, "display.max_info_columns", 1, "display.large_repr", "info"):
             return f"{self.__class__.__name__}({self.today}, {self.yesterday})"
 
 
 @cachetools.cached(  # type: ignore
     cache=cachetools.LRUCache(100),  # 100 * 50K = 5MB
+    key=lambda data_dir, stock_id, date, feature_columns_today, feature_columns_yesterday, backtest, index_only: (
+        stock_id,
+        date,
+        backtest,
+        index_only,
+    ),
 )
-def load_nt_intraday_processed_data(stock_id: str, date: pd.Timestamp) -> NTIntradayProcessedData:
-    return NTIntradayProcessedData(stock_id, date)
+def load_handler_intraday_processed_data(
+    data_dir: Path,
+    stock_id: str,
+    date: pd.Timestamp,
+    feature_columns_today: List[str],
+    feature_columns_yesterday: List[str],
+    backtest: bool = False,
+    index_only: bool = False,
+) -> HandlerIntradayProcessedData:
+    return HandlerIntradayProcessedData(
+        data_dir, stock_id, date, feature_columns_today, feature_columns_yesterday, backtest, index_only
+    )
 
 
-class NTProcessedDataProvider(ProcessedDataProvider):
+class HandlerProcessedDataProvider(ProcessedDataProvider):
+    def __init__(
+        self,
+        data_dir: str,
+        feature_columns_today: List[str],
+        feature_columns_yesterday: List[str],
+        backtest: bool = False,
+    ) -> None:
+        super().__init__()
+
+        self.data_dir = Path(data_dir)
+        self.feature_columns_today = feature_columns_today
+        self.feature_columns_yesterday = feature_columns_yesterday
+        self.backtest = backtest
+
     def get_data(
         self,
         stock_id: str,
         date: pd.Timestamp,
         feature_dim: int,
         time_index: pd.Index,
     ) -> BaseIntradayProcessedData:
-        return load_nt_intraday_processed_data(stock_id, date)
+        return load_handler_intraday_processed_data(
+            self.data_dir,
+            stock_id,
+            date,
+            self.feature_columns_today,
+            self.feature_columns_yesterday,
+            backtest=self.backtest,
+            index_only=False,
+        )
```

## qlib/rl/data/pickle_styled.py

```diff
@@ -100,15 +100,15 @@
 
     def __init__(
         self,
         data_dir: Path | str,
         stock_id: str,
         date: pd.Timestamp,
         deal_price: DealPriceType = "close",
-        order_dir: int = None,
+        order_dir: int | None = None,
     ) -> None:
         super(SimpleIntradayBacktestData, self).__init__()
 
         backtest = _read_pickle((data_dir if isinstance(data_dir, Path) else Path(data_dir)) / stock_id)
         backtest = backtest.loc[pd.IndexSlice[stock_id, :, date]]
 
         # No longer need for pandas >= 1.4
@@ -154,16 +154,16 @@
         """Return a volume series that can be indexed with time."""
         return self.data["$volume0"]
 
     def get_time_index(self) -> pd.DatetimeIndex:
         return cast(pd.DatetimeIndex, self.data.index)
 
 
-class IntradayProcessedData(BaseIntradayProcessedData):
-    """Subclass of IntradayProcessedData. Used to handle Dataset Handler style data."""
+class PickleIntradayProcessedData(BaseIntradayProcessedData):
+    """Subclass of IntradayProcessedData. Used to handle pickle-styled data."""
 
     def __init__(
         self,
         data_dir: Path | str,
         stock_id: str,
         date: pd.Timestamp,
         feature_dim: int,
@@ -204,31 +204,31 @@
 
 @lru_cache(maxsize=100)  # 100 * 50K = 5MB
 def load_simple_intraday_backtest_data(
     data_dir: Path,
     stock_id: str,
     date: pd.Timestamp,
     deal_price: DealPriceType = "close",
-    order_dir: int = None,
+    order_dir: int | None = None,
 ) -> SimpleIntradayBacktestData:
     return SimpleIntradayBacktestData(data_dir, stock_id, date, deal_price, order_dir)
 
 
 @cachetools.cached(  # type: ignore
     cache=cachetools.LRUCache(100),  # 100 * 50K = 5MB
     key=lambda data_dir, stock_id, date, feature_dim, time_index: hashkey(data_dir, stock_id, date),
 )
-def load_pickled_intraday_processed_data(
+def load_pickle_intraday_processed_data(
     data_dir: Path,
     stock_id: str,
     date: pd.Timestamp,
     feature_dim: int,
     time_index: pd.Index,
 ) -> BaseIntradayProcessedData:
-    return IntradayProcessedData(data_dir, stock_id, date, feature_dim, time_index)
+    return PickleIntradayProcessedData(data_dir, stock_id, date, feature_dim, time_index)
 
 
 class PickleProcessedDataProvider(ProcessedDataProvider):
     def __init__(self, data_dir: Path) -> None:
         super().__init__()
 
         self._data_dir = data_dir
@@ -236,15 +236,15 @@
     def get_data(
         self,
         stock_id: str,
         date: pd.Timestamp,
         feature_dim: int,
         time_index: pd.Index,
     ) -> BaseIntradayProcessedData:
-        return load_pickled_intraday_processed_data(
+        return load_pickle_intraday_processed_data(
             data_dir=self._data_dir,
             stock_id=stock_id,
             date=date,
             feature_dim=feature_dim,
             time_index=time_index,
         )
```

## qlib/rl/order_execution/interpreter.py

```diff
@@ -49,14 +49,26 @@
     cur_step: Any
     num_step: Any
     target: Any
     position: Any
     position_history: Any
 
 
+class DummyStateInterpreter(StateInterpreter[SAOEState, dict]):
+    """Dummy interpreter for policies that do not need inputs (for example, AllOne)."""
+
+    def interpret(self, state: SAOEState) -> dict:
+        # TODO: A fake state, used to pass `check_nan_observation`. Find a better way in the future.
+        return {"DUMMY": _to_int32(1)}
+
+    @property
+    def observation_space(self) -> spaces.Dict:
+        return spaces.Dict({"DUMMY": spaces.Box(-np.inf, np.inf, shape=(), dtype=np.int32)})
+
+
 class FullHistoryStateInterpreter(StateInterpreter[SAOEState, FullHistoryObs]):
     """The observation of all the history, including today (until this moment), and yesterday.
 
     Parameters
     ----------
     max_step
         Total number of steps (an upper-bound estimation). For example, 390min / 30min-per-step = 13 steps.
```

## qlib/rl/order_execution/policy.py

```diff
@@ -8,19 +8,19 @@
 
 import gym
 import numpy as np
 import torch
 import torch.nn as nn
 from gym.spaces import Discrete
 from tianshou.data import Batch, ReplayBuffer, to_torch
-from tianshou.policy import BasePolicy, PPOPolicy
+from tianshou.policy import BasePolicy, PPOPolicy, DQNPolicy
 
 from qlib.rl.trainer.trainer import Trainer
 
-__all__ = ["AllOne", "PPO"]
+__all__ = ["AllOne", "PPO", "DQN"]
 
 
 # baselines #
 
 
 class NonLearnablePolicy(BasePolicy):
     """Tianshou's BasePolicy with empty ``learn`` and ``process_fn``.
@@ -28,38 +28,43 @@
     This could be moved outside in future.
     """
 
     def __init__(self, obs_space: gym.Space, action_space: gym.Space) -> None:
         super().__init__()
 
     def learn(self, batch: Batch, **kwargs: Any) -> Dict[str, Any]:
-        pass
+        return {}
 
     def process_fn(
         self,
         batch: Batch,
         buffer: ReplayBuffer,
         indices: np.ndarray,
     ) -> Batch:
-        pass
+        return Batch({})
 
 
 class AllOne(NonLearnablePolicy):
     """Forward returns a batch full of 1.
 
     Useful when implementing some baselines (e.g., TWAP).
     """
 
+    def __init__(self, obs_space: gym.Space, action_space: gym.Space, fill_value: float | int = 1.0) -> None:
+        super().__init__(obs_space, action_space)
+
+        self.fill_value = fill_value
+
     def forward(
         self,
         batch: Batch,
         state: dict | Batch | np.ndarray = None,
         **kwargs: Any,
     ) -> Batch:
-        return Batch(act=np.full(len(batch), 1.0), state=state)
+        return Batch(act=np.full(len(batch), self.fill_value), state=state)
 
 
 # ppo #
 
 
 class PPOActor(nn.Module):
     def __init__(self, extractor: nn.Module, action_dim: int) -> None:
@@ -149,14 +154,64 @@
             observation_space=obs_space,
             action_space=action_space,
         )
         if weight_file is not None:
             set_weight(self, Trainer.get_policy_state_dict(weight_file))
 
 
+DQNModel = PPOActor  # Reuse PPOActor.
+
+
+class DQN(DQNPolicy):
+    """A wrapper of tianshou DQNPolicy.
+
+    Differences:
+
+    - Auto-create model network. Supports discrete action space only.
+    - Support a ``weight_file`` that supports loading checkpoint.
+    """
+
+    def __init__(
+        self,
+        network: nn.Module,
+        obs_space: gym.Space,
+        action_space: gym.Space,
+        lr: float,
+        weight_decay: float = 0.0,
+        discount_factor: float = 0.99,
+        estimation_step: int = 1,
+        target_update_freq: int = 0,
+        reward_normalization: bool = False,
+        is_double: bool = True,
+        clip_loss_grad: bool = False,
+        weight_file: Optional[Path] = None,
+    ) -> None:
+        assert isinstance(action_space, Discrete)
+
+        model = DQNModel(network, action_space.n)
+        optimizer = torch.optim.Adam(
+            model.parameters(),
+            lr=lr,
+            weight_decay=weight_decay,
+        )
+
+        super().__init__(
+            model,
+            optimizer,
+            discount_factor=discount_factor,
+            estimation_step=estimation_step,
+            target_update_freq=target_update_freq,
+            reward_normalization=reward_normalization,
+            is_double=is_double,
+            clip_loss_grad=clip_loss_grad,
+        )
+        if weight_file is not None:
+            set_weight(self, Trainer.get_policy_state_dict(weight_file))
+
+
 # utilities: these should be put in a separate (common) file. #
 
 
 def auto_device(module: nn.Module) -> torch.device:
     for param in module.parameters():
         return param.device
     return torch.device("cpu")  # fallback to cpu
```

## qlib/rl/order_execution/reward.py

```diff
@@ -3,14 +3,15 @@
 
 from __future__ import annotations
 
 from typing import cast
 
 import numpy as np
 
+from qlib.backtest.decision import OrderDir
 from qlib.rl.order_execution.state import SAOEMetrics, SAOEState
 from qlib.rl.reward import Reward
 
 __all__ = ["PAPenaltyReward"]
 
 
 class PAPenaltyReward(Reward[SAOEState]):
@@ -43,7 +44,56 @@
 
         # Throw error in case of NaN
         assert not (np.isnan(reward) or np.isinf(reward)), f"Invalid reward for simulator state: {simulator_state}"
 
         self.log("reward/pa", pa)
         self.log("reward/penalty", penalty)
         return reward * self.scale
+
+
+class PPOReward(Reward[SAOEState]):
+    """Reward proposed by paper "An End-to-End Optimal Trade Execution Framework based on Proximal Policy Optimization".
+
+    Parameters
+    ----------
+    max_step
+        Maximum number of steps.
+    start_time_index
+        First time index that allowed to trade.
+    end_time_index
+        Last time index that allowed to trade.
+    """
+
+    def __init__(self, max_step: int, start_time_index: int = 0, end_time_index: int = 239) -> None:
+        self.max_step = max_step
+        self.start_time_index = start_time_index
+        self.end_time_index = end_time_index
+
+    def reward(self, simulator_state: SAOEState) -> float:
+        if simulator_state.cur_step == self.max_step - 1 or simulator_state.position < 1e-6:
+            if simulator_state.history_exec["deal_amount"].sum() == 0.0:
+                vwap_price = cast(
+                    float,
+                    np.average(simulator_state.history_exec["market_price"]),
+                )
+            else:
+                vwap_price = cast(
+                    float,
+                    np.average(
+                        simulator_state.history_exec["market_price"],
+                        weights=simulator_state.history_exec["deal_amount"],
+                    ),
+                )
+            twap_price = simulator_state.backtest_data.get_deal_price().mean()
+
+            if simulator_state.order.direction == OrderDir.SELL:
+                ratio = vwap_price / twap_price if twap_price != 0 else 1.0
+            else:
+                ratio = twap_price / vwap_price if vwap_price != 0 else 1.0
+            if ratio < 1.0:
+                return -1.0
+            elif ratio < 1.1:
+                return 0.0
+            else:
+                return 1.0
+        else:
+            return 0.0
```

## qlib/rl/order_execution/simulator_qlib.py

```diff
@@ -34,16 +34,16 @@
     """
 
     def __init__(
         self,
         order: Order,
         executor_config: dict,
         exchange_config: dict,
-        qlib_config: dict = None,
-        cash_limit: Optional[float] = None,
+        qlib_config: dict | None = None,
+        cash_limit: float | None = None,
     ) -> None:
         super().__init__(initial=order)
 
         assert order.start_time.date() == order.end_time.date(), "Start date and end date must be the same."
 
         strategy_config = {
             "class": "SingleOrderStrategy",
@@ -59,19 +59,19 @@
 
     def reset(
         self,
         order: Order,
         strategy_config: dict,
         executor_config: dict,
         exchange_config: dict,
-        qlib_config: dict = None,
+        qlib_config: dict | None = None,
         cash_limit: Optional[float] = None,
     ) -> None:
         if qlib_config is not None:
-            init_qlib(qlib_config, part="skip")
+            init_qlib(qlib_config)
 
         strategy, self._executor = get_strategy_executor(
             start_time=order.date,
             end_time=order.date + pd.DateOffset(1),
             strategy=strategy_config,
             executor=executor_config,
             benchmark=order.stock_id,
```

## qlib/rl/order_execution/simulator_simple.py

```diff
@@ -1,23 +1,25 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 
 from __future__ import annotations
 
-from pathlib import Path
-from typing import Any, cast, Optional
+from typing import Any, cast, List, Optional
 
 import numpy as np
 import pandas as pd
+
+from pathlib import Path
 from qlib.backtest.decision import Order, OrderDir
 from qlib.constant import EPS, EPS_T, float_or_ndarray
-from qlib.rl.data.pickle_styled import DealPriceType, load_simple_intraday_backtest_data
+from qlib.rl.data.base import BaseIntradayBacktestData
+from qlib.rl.data.native import DataframeIntradayBacktestData, load_handler_intraday_processed_data
+from qlib.rl.data.pickle_styled import load_simple_intraday_backtest_data
 from qlib.rl.simulator import Simulator
 from qlib.rl.utils import LogLevel
-
 from .state import SAOEMetrics, SAOEState
 
 __all__ = ["SingleAssetOrderExecutionSimple"]
 
 
 class SingleAssetOrderExecutionSimple(Simulator[Order, SAOEState, float]):
     """Single-asset order execution (SAOE) simulator.
@@ -32,20 +34,24 @@
     then bounded by volume maximum execution volume (i.e., ``vol_threshold``),
     and if it's the last step, try to ensure all the amount to be executed.
 
     Parameters
     ----------
     order
         The seed to start an SAOE simulator is an order.
+    data_dir
+        Path to load backtest data.
+    feature_columns_today
+        Columns of today's feature.
+    feature_columns_yesterday
+        Columns of yesterday's feature.
     data_granularity
         Number of ticks between consecutive data entries.
     ticks_per_step
         How many ticks per step.
-    data_dir
-        Path to load backtest data
     vol_threshold
         Maximum execution volume (divided by market execution volume).
     """
 
     history_exec: pd.DataFrame
     """All execution history at every possible time ticks. See :class:`SAOEMetrics` for available columns.
     Index is ``datetime``.
@@ -69,36 +75,32 @@
     ticks_for_order: pd.DatetimeIndex
     """Ticks that is available for trading (sliced by order)."""
 
     def __init__(
         self,
         order: Order,
         data_dir: Path,
+        feature_columns_today: List[str] = [],
+        feature_columns_yesterday: List[str] = [],
         data_granularity: int = 1,
         ticks_per_step: int = 30,
-        deal_price_type: DealPriceType = "close",
         vol_threshold: Optional[float] = None,
     ) -> None:
         super().__init__(initial=order)
 
         assert ticks_per_step % data_granularity == 0
 
         self.order = order
+        self.data_dir = data_dir
+        self.feature_columns_today = feature_columns_today
+        self.feature_columns_yesterday = feature_columns_yesterday
         self.ticks_per_step: int = ticks_per_step // data_granularity
-        self.deal_price_type = deal_price_type
         self.vol_threshold = vol_threshold
-        self.data_dir = data_dir
-        self.backtest_data = load_simple_intraday_backtest_data(
-            self.data_dir,
-            order.stock_id,
-            pd.Timestamp(order.start_time.date()),
-            self.deal_price_type,
-            order.direction,
-        )
 
+        self.backtest_data = self.get_backtest_data()
         self.ticks_index = self.backtest_data.get_time_index()
 
         # Get time index available for trading
         self.ticks_for_order = self._get_ticks_slice(self.order.start_time, self.order.end_time)
 
         self.cur_time = self.ticks_for_order[0]
         self.cur_step = 0
@@ -114,14 +116,38 @@
         self.history_steps = pd.DataFrame(columns=metric_keys).set_index("datetime")
         self.metrics = None
 
         self.market_price: Optional[np.ndarray] = None
         self.market_vol: Optional[np.ndarray] = None
         self.market_vol_limit: Optional[np.ndarray] = None
 
+    def get_backtest_data(self) -> BaseIntradayBacktestData:
+        try:
+            data = load_handler_intraday_processed_data(
+                data_dir=self.data_dir,
+                stock_id=self.order.stock_id,
+                date=pd.Timestamp(self.order.start_time.date()),
+                feature_columns_today=self.feature_columns_today,
+                feature_columns_yesterday=self.feature_columns_yesterday,
+                backtest=True,
+                index_only=False,
+            )
+            return DataframeIntradayBacktestData(data.today)
+        except (AttributeError, FileNotFoundError):
+            # TODO: For compatibility with older versions of test scripts (tests/rl/test_saoe_simple.py)
+            # TODO: In the future, we should modify the data format used by the test script,
+            # TODO: and then delete this branch.
+            return load_simple_intraday_backtest_data(
+                self.data_dir / "backtest",
+                self.order.stock_id,
+                pd.Timestamp(self.order.start_time.date()),
+                "close",
+                self.order.direction,
+            )
+
     def step(self, amount: float) -> None:
         """Execute one step or SAOE.
 
         Parameters
         ----------
         amount
             The amount you wish to deal. The simulator doesn't guarantee all the amount to be successfully dealt.
```

## qlib/rl/order_execution/strategy.py

```diff
@@ -3,14 +3,15 @@
 
 from __future__ import annotations
 
 import collections
 from types import GeneratorType
 from typing import Any, Callable, cast, Dict, Generator, List, Optional, Tuple, Union
 
+import warnings
 import numpy as np
 import pandas as pd
 import torch
 from tianshou.data import Batch
 from tianshou.policy import BasePolicy
 
 from qlib.backtest import CommonInfrastructure, Order
@@ -85,14 +86,15 @@
         self,
         order: Order,
         trade_decision: BaseTradeDecision,
         executor: BaseExecutor,
         exchange: Exchange,
         ticks_per_step: int,
         backtest_data: IntradayBacktestData,
+        data_granularity: int = 1,
     ) -> None:
         self.position = order.amount
         self.order = order
         self.executor = executor
         self.exchange = exchange
         self.backtest_data = backtest_data
         self.start_idx, _ = get_start_end_idx(self.executor.trade_calendar, trade_decision)
@@ -102,19 +104,21 @@
         metric_keys = list(SAOEMetrics.__annotations__.keys())  # pylint: disable=no-member
         self.history_exec = pd.DataFrame(columns=metric_keys).set_index("datetime")
         self.history_steps = pd.DataFrame(columns=metric_keys).set_index("datetime")
         self.metrics: Optional[SAOEMetrics] = None
 
         self.cur_time = max(backtest_data.ticks_for_order[0], order.start_time)
         self.ticks_per_step = ticks_per_step
+        self.data_granularity = data_granularity
+        assert self.ticks_per_step % self.data_granularity == 0
 
     def _next_time(self) -> pd.Timestamp:
         current_loc = self.backtest_data.ticks_index.get_loc(self.cur_time)
-        next_loc = current_loc + self.ticks_per_step
-        next_loc = next_loc - next_loc % self.ticks_per_step
+        next_loc = current_loc + (self.ticks_per_step // self.data_granularity)
+        next_loc = next_loc - next_loc % (self.ticks_per_step // self.data_granularity)
         if (
             next_loc < len(self.backtest_data.ticks_index)
             and self.backtest_data.ticks_index[next_loc] < self.order.end_time
         ):
             return self.backtest_data.ticks_index[next_loc]
         else:
             return self.order.end_time
@@ -126,19 +130,24 @@
     ) -> None:
         last_step_size = last_step_range[1] - last_step_range[0] + 1
         start_time = self.backtest_data.ticks_index[last_step_range[0]]
         end_time = self.backtest_data.ticks_index[last_step_range[1]]
 
         exec_vol = np.zeros(last_step_size)
         for order, _, __, ___ in execute_result:
-            idx, _ = get_day_min_idx_range(order.start_time, order.end_time, "1min", REG_CN)
+            idx, _ = get_day_min_idx_range(order.start_time, order.end_time, f"{self.data_granularity}min", REG_CN)
             exec_vol[idx - last_step_range[0]] = order.deal_amount
 
         if exec_vol.sum() > self.position and exec_vol.sum() > 0.0:
-            assert exec_vol.sum() < self.position + 1, f"{exec_vol} too large"
+            if exec_vol.sum() > self.position + 1.0:
+                warnings.warn(
+                    f"Sum of execution volume is {exec_vol.sum()} which is larger than "
+                    f"position + 1.0 = {self.position} + 1.0 = {self.position + 1.0}. "
+                    f"All execution volume is scaled down linearly to ensure that their sum does not position."
+                )
             exec_vol *= self.position / (exec_vol.sum())
 
         market_volume = cast(
             IndexData,
             self.exchange.get_volume(
                 self.order.stock_id,
                 pd.Timestamp(start_time),
@@ -164,15 +173,17 @@
         # Get data from the current level executor's indicator
         current_trade_account = self.executor.trade_account
         current_df = current_trade_account.get_trade_indicator().generate_trade_indicators_dataframe()
         self.history_exec = dataframe_append(
             self.history_exec,
             self._collect_multi_order_metric(
                 order=self.order,
-                datetime=_get_all_timestamps(start_time, end_time, include_end=True),
+                datetime=_get_all_timestamps(
+                    start_time, end_time, include_end=True, granularity=ONE_MIN * self.data_granularity
+                ),
                 market_vol=market_volume,
                 market_price=market_price,
                 exec_vol=exec_vol,
                 pa=current_df.iloc[-1]["pa"],
             ),
         )
 
@@ -289,27 +300,29 @@
 
 class SAOEStrategy(RLStrategy):
     """RL-based strategies that use SAOEState as state."""
 
     def __init__(
         self,
         policy: BasePolicy,
-        outer_trade_decision: BaseTradeDecision = None,
-        level_infra: LevelInfrastructure = None,
-        common_infra: CommonInfrastructure = None,
+        outer_trade_decision: BaseTradeDecision | None = None,
+        level_infra: LevelInfrastructure | None = None,
+        common_infra: CommonInfrastructure | None = None,
+        data_granularity: int = 1,
         **kwargs: Any,
     ) -> None:
         super(SAOEStrategy, self).__init__(
             policy=policy,
             outer_trade_decision=outer_trade_decision,
             level_infra=level_infra,
             common_infra=common_infra,
             **kwargs,
         )
 
+        self._data_granularity = data_granularity
         self.adapter_dict: Dict[tuple, SAOEStateAdapter] = {}
         self._last_step_range = (0, 0)
 
     def _create_qlib_backtest_adapter(
         self,
         order: Order,
         trade_decision: BaseTradeDecision,
@@ -320,17 +333,18 @@
         return SAOEStateAdapter(
             order=order,
             trade_decision=trade_decision,
             executor=self.executor,
             exchange=self.trade_exchange,
             ticks_per_step=int(pd.Timedelta(self.trade_calendar.get_freq()) / ONE_MIN),
             backtest_data=backtest_data,
+            data_granularity=self._data_granularity,
         )
 
-    def reset(self, outer_trade_decision: BaseTradeDecision = None, **kwargs: Any) -> None:
+    def reset(self, outer_trade_decision: BaseTradeDecision | None = None, **kwargs: Any) -> None:
         super(SAOEStrategy, self).reset(outer_trade_decision=outer_trade_decision, **kwargs)
 
         self.adapter_dict = {}
         self._last_step_range = (0, 0)
 
         if outer_trade_decision is not None and not outer_trade_decision.empty():
             trade_range = outer_trade_decision.trade_range
@@ -362,15 +376,15 @@
                 results[e[0].key_by_day].append(e)
 
         for key, adapter in self.adapter_dict.items():
             adapter.update(results[key], self._last_step_range)
 
     def generate_trade_decision(
         self,
-        execute_result: list = None,
+        execute_result: list | None = None,
     ) -> Union[BaseTradeDecision, Generator[Any, Any, BaseTradeDecision]]:
         """
         For SAOEStrategy, we need to update the `self._last_step_range` every time a decision is generated.
         This operation should be invisible to developers, so we implement it in `generate_trade_decision()`
         The concrete logic to generate decisions should be implemented in `_generate_trade_decision()`.
         In other words, all subclass of `SAOEStrategy` should overwrite `_generate_trade_decision()` instead of
         `generate_trade_decision()`.
@@ -381,48 +395,48 @@
         if isinstance(decision, GeneratorType):
             decision = yield from decision
 
         return decision
 
     def _generate_trade_decision(
         self,
-        execute_result: list = None,
+        execute_result: list | None = None,
     ) -> Union[BaseTradeDecision, Generator[Any, Any, BaseTradeDecision]]:
         raise NotImplementedError
 
 
 class ProxySAOEStrategy(SAOEStrategy):
     """Proxy strategy that uses SAOEState. It is called a 'proxy' strategy because it does not make any decisions
     by itself. Instead, when the strategy is required to generate a decision, it will yield the environment's
     information and let the outside agents to make the decision. Please refer to `_generate_trade_decision` for
     more details.
     """
 
     def __init__(
         self,
-        outer_trade_decision: BaseTradeDecision = None,
-        level_infra: LevelInfrastructure = None,
-        common_infra: CommonInfrastructure = None,
+        outer_trade_decision: BaseTradeDecision | None = None,
+        level_infra: LevelInfrastructure | None = None,
+        common_infra: CommonInfrastructure | None = None,
         **kwargs: Any,
     ) -> None:
         super().__init__(None, outer_trade_decision, level_infra, common_infra, **kwargs)
 
-    def _generate_trade_decision(self, execute_result: list = None) -> Generator[Any, Any, BaseTradeDecision]:
+    def _generate_trade_decision(self, execute_result: list | None = None) -> Generator[Any, Any, BaseTradeDecision]:
         # Once the following line is executed, this ProxySAOEStrategy (self) will be yielded to the outside
         # of the entire executor, and the execution will be suspended. When the execution is resumed by `send()`,
         # the item will be captured by `exec_vol`. The outside policy could communicate with the inner
         # level strategy through this way.
         exec_vol = yield self
 
         oh = self.trade_exchange.get_order_helper()
         order = oh.create(self._order.stock_id, exec_vol, self._order.direction)
 
         return TradeDecisionWO([order], self)
 
-    def reset(self, outer_trade_decision: BaseTradeDecision = None, **kwargs: Any) -> None:
+    def reset(self, outer_trade_decision: BaseTradeDecision | None = None, **kwargs: Any) -> None:
         super().reset(outer_trade_decision=outer_trade_decision, **kwargs)
 
         assert isinstance(outer_trade_decision, TradeDecisionWO)
         if outer_trade_decision is not None:
             order_list = outer_trade_decision.order_list
             assert len(order_list) == 1
             self._order = order_list[0]
@@ -433,17 +447,17 @@
 
     def __init__(
         self,
         policy: dict | BasePolicy,
         state_interpreter: dict | StateInterpreter,
         action_interpreter: dict | ActionInterpreter,
         network: dict | torch.nn.Module | None = None,
-        outer_trade_decision: BaseTradeDecision = None,
-        level_infra: LevelInfrastructure = None,
-        common_infra: CommonInfrastructure = None,
+        outer_trade_decision: BaseTradeDecision | None = None,
+        level_infra: LevelInfrastructure | None = None,
+        common_infra: CommonInfrastructure | None = None,
         **kwargs: Any,
     ) -> None:
         super(SAOEIntStrategy, self).__init__(
             policy=policy,
             outer_trade_decision=outer_trade_decision,
             level_infra=level_infra,
             common_infra=common_infra,
@@ -484,15 +498,15 @@
             self._policy = policy
         else:
             raise ValueError(f"Unsupported policy type: {type(policy)}.")
 
         if self._policy is not None:
             self._policy.eval()
 
-    def reset(self, outer_trade_decision: BaseTradeDecision = None, **kwargs: Any) -> None:
+    def reset(self, outer_trade_decision: BaseTradeDecision | None = None, **kwargs: Any) -> None:
         super().reset(outer_trade_decision=outer_trade_decision, **kwargs)
 
     def _generate_trade_details(self, act: np.ndarray, exec_vols: List[float]) -> pd.DataFrame:
         assert hasattr(self.outer_trade_decision, "order_list")
 
         trade_details = []
         for a, v, o in zip(act, exec_vols, getattr(self.outer_trade_decision, "order_list")):
@@ -504,15 +518,15 @@
                     "rl_exec_vol": v,
                 }
             )
             if a is not None:
                 trade_details[-1]["rl_action"] = a
         return pd.DataFrame.from_records(trade_details)
 
-    def _generate_trade_decision(self, execute_result: list = None) -> BaseTradeDecision:
+    def _generate_trade_decision(self, execute_result: list | None = None) -> BaseTradeDecision:
         states = []
         obs_batch = []
         for decision in self.outer_trade_decision.get_decision():
             order = cast(Order, decision)
             state = self.get_saoe_state_by_order(order)
 
             states.append(state)
```

## qlib/rl/order_execution/utils.py

```diff
@@ -6,26 +6,15 @@
 from typing import Any, cast
 
 import numpy as np
 import pandas as pd
 
 from qlib.backtest.decision import OrderDir
 from qlib.backtest.executor import BaseExecutor, NestedExecutor, SimulatorExecutor
-from qlib.constant import EPS_T, float_or_ndarray
-
-
-def get_ticks_slice(
-    ticks_index: pd.DatetimeIndex,
-    start: pd.Timestamp,
-    end: pd.Timestamp,
-    include_end: bool = False,
-) -> pd.DatetimeIndex:
-    if not include_end:
-        end = end - EPS_T
-    return ticks_index[ticks_index.slice_indexer(start, end)]
+from qlib.constant import float_or_ndarray
 
 
 def dataframe_append(df: pd.DataFrame, other: Any) -> pd.DataFrame:
     # dataframe.append is deprecated
     other_df = pd.DataFrame(other).set_index("datetime")
     other_df.index.name = "datetime"
```

## qlib/rl/strategy/single_order.py

```diff
@@ -1,29 +1,31 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 
+from __future__ import annotations
+
 from qlib.backtest import Order
 from qlib.backtest.decision import OrderHelper, TradeDecisionWO, TradeRange
 from qlib.strategy.base import BaseStrategy
 
 
 class SingleOrderStrategy(BaseStrategy):
     """Strategy used to generate a trade decision with exactly one order."""
 
     def __init__(
         self,
         order: Order,
-        trade_range: TradeRange = None,
+        trade_range: TradeRange | None = None,
     ) -> None:
         super().__init__()
 
         self._order = order
         self._trade_range = trade_range
 
-    def generate_trade_decision(self, execute_result: list = None) -> TradeDecisionWO:
+    def generate_trade_decision(self, execute_result: list | None = None) -> TradeDecisionWO:
         oh: OrderHelper = self.common_infra.get("trade_exchange").get_order_helper()
         order_list = [
             oh.create(
                 code=self._order.stock_id,
                 amount=self._order.amount,
                 direction=self._order.direction,
             ),
```

## qlib/rl/utils/data_queue.py

```diff
@@ -1,13 +1,14 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 
 from __future__ import annotations
 
 import multiprocessing
+from multiprocessing.sharedctypes import Synchronized
 import os
 import threading
 import time
 import warnings
 from queue import Empty
 from typing import Any, Generator, Generic, Sequence, TypeVar, cast
 
@@ -74,15 +75,17 @@
         self.dataset: Sequence[T] = dataset
         self.repeat: int = repeat
         self.shuffle: bool = shuffle
         self.producer_num_workers: int = producer_num_workers
 
         self._activated: bool = False
         self._queue: multiprocessing.Queue = multiprocessing.Queue(maxsize=queue_maxsize)
-        self._done = multiprocessing.Value("i", 0)
+        # Mypy 0.981 brought '"SynchronizedBase[Any]" has no attribute "value"  [attr-defined]' bug.
+        # Therefore, add this type casting to pass Mypy checking.
+        self._done = cast(Synchronized, multiprocessing.Value("i", 0))
 
     def __enter__(self) -> DataQueue:
         self.activate()
         return self
 
     def __exit__(self, exc_type, exc_val, exc_tb):
         self.cleanup()
@@ -118,15 +121,15 @@
         while True:
             try:
                 return self._queue.get(block=block, timeout=timeout)
             except Empty:
                 if self._done.value:
                     raise StopIteration  # pylint: disable=raise-missing-from
 
-    def put(self, obj: Any, block: bool = True, timeout: int = None) -> None:
+    def put(self, obj: Any, block: bool = True, timeout: int | None = None) -> None:
         self._queue.put(obj, block=block, timeout=timeout)
 
     def mark_as_done(self) -> None:
         with self._done.get_lock():
             self._done.value = 1
 
     def done(self) -> int:
```

## qlib/rl/utils/env_wrapper.py

```diff
@@ -95,17 +95,17 @@
 
     def __init__(
         self,
         simulator_fn: Callable[..., Simulator[InitialStateType, StateType, ActType]],
         state_interpreter: StateInterpreter[StateType, ObsType],
         action_interpreter: ActionInterpreter[StateType, PolicyActType, ActType],
         seed_iterator: Optional[Iterable[InitialStateType]],
-        reward_fn: Reward = None,
-        aux_info_collector: AuxiliaryInfoCollector[StateType, Any] = None,
-        logger: LogCollector = None,
+        reward_fn: Reward | None = None,
+        aux_info_collector: AuxiliaryInfoCollector[StateType, Any] | None = None,
+        logger: LogCollector | None = None,
     ) -> None:
         # Assign weak reference to wrapper.
         #
         # Use weak reference here, because:
         # 1. Logically, the other components should be able to live without an env_wrapper.
         #    For example, they might live in a strategy_wrapper in future.
         #    Therefore injecting a "hard" attribute called "env" is not appropripate.
```

## qlib/rl/utils/log.py

```diff
@@ -393,15 +393,15 @@
 
     prefix: str
     """Prefix can be set via ``writer.prefix``."""
 
     def __init__(
         self,
         log_every_n_episode: int = 20,
-        total_episodes: int = None,
+        total_episodes: int | None = None,
         float_format: str = ":.4f",
         counter_format: str = ":4d",
         loglevel: int | LogLevel = LogLevel.PERIODIC,
     ) -> None:
         super().__init__(loglevel)
         # TODO: support log_every_n_step
         self.log_every_n_episode = log_every_n_episode
```

## qlib/tests/data.py

```diff
@@ -1,78 +1,101 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 
+import os
 import re
 import sys
 import qlib
 import shutil
 import zipfile
 import requests
 import datetime
 from tqdm import tqdm
 from pathlib import Path
 from loguru import logger
+from cryptography.fernet import Fernet
 from qlib.utils import exists_qlib_data
 
 
 class GetData:
-    DATASET_VERSION = "v2"
     REMOTE_URL = "https://qlibpublic.blob.core.windows.net/data/default/stock_data"
-    QLIB_DATA_NAME = "{dataset_name}_{region}_{interval}_{qlib_version}.zip"
+    # "?" is not included in the token.
+    TOKEN = "gAAAAABkmDhojHc0VSCDdNK1MqmRzNLeDFXe5hy8obHpa6SDQh4de6nW5gtzuD-fa6O_WZb0yyqYOL7ndOfJX_751W3xN5YB4-n-P22jK-t6ucoZqhT70KPD0Lf0_P328QPJVZ1gDnjIdjhi2YLOcP4BFTHLNYO0mvzszR8TKm9iT5AKRvuysWnpi8bbYwGU9zAcJK3x9EPL43hOGtxliFHcPNGMBoJW4g_ercdhi0-Qgv5_JLsV-29_MV-_AhuaYvJuN2dEywBy"
+    KEY = "EYcA8cgorA8X9OhyMwVfuFxn_1W3jGk6jCbs3L2oPoA="
 
     def __init__(self, delete_zip_file=False):
         """
 
         Parameters
         ----------
         delete_zip_file : bool, optional
             Whether to delete the zip file, value from True or False, by default False
         """
         self.delete_zip_file = delete_zip_file
 
-    def normalize_dataset_version(self, dataset_version: str = None):
-        if dataset_version is None:
-            dataset_version = self.DATASET_VERSION
-        return dataset_version
+    def merge_remote_url(self, file_name: str):
+        fernet = Fernet(self.KEY)
+        token = fernet.decrypt(self.TOKEN).decode()
+        return f"{self.REMOTE_URL}/{file_name}?{token}"
 
-    def merge_remote_url(self, file_name: str, dataset_version: str = None):
-        return f"{self.REMOTE_URL}/{self.normalize_dataset_version(dataset_version)}/{file_name}"
+    def download_data(self, file_name: str, target_dir: [Path, str], delete_old: bool = True):
+        """
+        Download the specified file to the target folder.
 
-    def _download_data(
-        self, file_name: str, target_dir: [Path, str], delete_old: bool = True, dataset_version: str = None
-    ):
+        Parameters
+        ----------
+        target_dir: str
+            data save directory
+        file_name: str
+            dataset name, needs to endwith .zip, value from [rl_data.zip, csv_data_cn.zip, ...]
+            may contain folder names, for example: v2/qlib_data_simple_cn_1d_latest.zip
+        delete_old: bool
+            delete an existing directory, by default True
+
+        Examples
+        ---------
+        # get rl data
+        python get_data.py download_data --file_name rl_data.zip --target_dir ~/.qlib/qlib_data/rl_data
+        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/rl_data.zip?{token}
+
+        # get cn csv data
+        python get_data.py download_data --file_name csv_data_cn.zip --target_dir ~/.qlib/csv_data/cn_data
+        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/csv_data_cn.zip?{token}
+        -------
+
+        """
         target_dir = Path(target_dir).expanduser()
         target_dir.mkdir(exist_ok=True, parents=True)
         # saved file name
-        _target_file_name = datetime.datetime.now().strftime("%Y%m%d%H%M%S") + "_" + file_name
+        _target_file_name = datetime.datetime.now().strftime("%Y%m%d%H%M%S") + "_" + os.path.basename(file_name)
         target_path = target_dir.joinpath(_target_file_name)
 
-        url = self.merge_remote_url(file_name, dataset_version)
+        url = self.merge_remote_url(file_name)
         resp = requests.get(url, stream=True, timeout=60)
         resp.raise_for_status()
         if resp.status_code != 200:
             raise requests.exceptions.HTTPError()
 
         chunk_size = 1024
         logger.warning(
             f"The data for the example is collected from Yahoo Finance. Please be aware that the quality of the data might not be perfect. (You can refer to the original data source: https://finance.yahoo.com/lookup.)"
         )
-        logger.info(f"{file_name} downloading......")
+        logger.info(f"{os.path.basename(file_name)} downloading......")
         with tqdm(total=int(resp.headers.get("Content-Length", 0))) as p_bar:
             with target_path.open("wb") as fp:
                 for chunk in resp.iter_content(chunk_size=chunk_size):
                     fp.write(chunk)
                     p_bar.update(chunk_size)
 
         self._unzip(target_path, target_dir, delete_old)
         if self.delete_zip_file:
             target_path.unlink()
 
-    def check_dataset(self, file_name: str, dataset_version: str = None):
-        url = self.merge_remote_url(file_name, dataset_version)
+    def check_dataset(self, file_name: str):
+        url = self.merge_remote_url(file_name)
         resp = requests.get(url, stream=True, timeout=60)
         status = True
         if resp.status_code == 404:
             status = False
         return status
 
     @staticmethod
@@ -136,48 +159,33 @@
         exists_skip: bool
             exists skip, by default False
 
         Examples
         ---------
         # get 1d data
         python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data --interval 1d --region cn
+        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1d_latest.zip?{token}
 
         # get 1min data
         python get_data.py qlib_data --name qlib_data --target_dir ~/.qlib/qlib_data/cn_data_1min --interval 1min --region cn
+        When this command is run, the data will be downloaded from this link: https://qlibpublic.blob.core.windows.net/data/default/stock_data/v2/qlib_data_cn_1min_latest.zip?{token}
         -------
 
         """
         if exists_skip and exists_qlib_data(target_dir):
             logger.warning(
                 f"Data already exists: {target_dir}, the data download will be skipped\n"
                 f"\tIf downloading is required: `exists_skip=False` or `change target_dir`"
             )
             return
 
         qlib_version = ".".join(re.findall(r"(\d+)\.+", qlib.__version__))
 
-        def _get_file_name(v):
-            return self.QLIB_DATA_NAME.format(
-                dataset_name=name, region=region.lower(), interval=interval.lower(), qlib_version=v
-            )
-
-        file_name = _get_file_name(qlib_version)
-        if not self.check_dataset(file_name, version):
-            file_name = _get_file_name("latest")
-        self._download_data(file_name.lower(), target_dir, delete_old, dataset_version=version)
-
-    def csv_data_cn(self, target_dir="~/.qlib/csv_data/cn_data"):
-        """download cn csv data from remote
-
-        Parameters
-        ----------
-        target_dir: str
-            data save directory
-
-        Examples
-        ---------
-        python get_data.py csv_data_cn --target_dir ~/.qlib/csv_data/cn_data
-        -------
-
-        """
-        file_name = "csv_data_cn.zip"
-        self._download_data(file_name, target_dir)
+        def _get_file_name_with_version(qlib_version, dataset_version):
+            dataset_version = "v2" if dataset_version is None else dataset_version
+            file_name_with_version = f"{dataset_version}/{name}_{region.lower()}_{interval.lower()}_{qlib_version}.zip"
+            return file_name_with_version
+
+        file_name = _get_file_name_with_version(qlib_version, dataset_version=version)
+        if not self.check_dataset(file_name):
+            file_name = _get_file_name_with_version("latest", dataset_version=version)
+        self.download_data(file_name.lower(), target_dir, delete_old)
```

## qlib/utils/__init__.py

```diff
@@ -1,10 +1,11 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
 
+# TODO: this utils covers too much utilities, please seperat it into sub modules
 
 from __future__ import division
 from __future__ import print_function
 
 import os
 import pickle
 import re
@@ -39,15 +40,15 @@
 # MultiIndex.is_lexsorted() is a deprecated method in Pandas 1.3.0.
 is_deprecated_lexsorted_pandas = version.parse(pd.__version__) > version.parse("1.3.0")
 
 
 #################### Server ####################
 def get_redis_connection():
     """get redis connection instance."""
-    return redis.StrictRedis(host=C.redis_host, port=C.redis_port, db=C.redis_task_db)
+    return redis.StrictRedis(host=C.redis_host, port=C.redis_port, db=C.redis_task_db, password=C.redis_password)
 
 
 #################### Data ####################
 def read_bin(file_path: Union[str, Path], start_index, end_index):
     file_path = Path(file_path.expanduser().resolve())
     with file_path.open("rb") as f:
         # read start_index
@@ -220,15 +221,15 @@
             assert res.status_code in {200, 206}
             return res
         except AssertionError:
             continue
         except Exception as e:
             log.warning("exception encountered {}".format(e))
             continue
-    raise Exception("ERROR: requests failed!")
+    raise TimeoutError("ERROR: requests failed!")
 
 
 #################### Parse ####################
 def parse_config(config):
     # Check whether need parse, all object except str do not need to be parsed
     if not isinstance(config, str):
         return config
@@ -422,15 +423,16 @@
         return config
 
     if isinstance(config, (str, Path)):
         if isinstance(config, str):
             # path like 'file:///<path to pickle file>/obj.pkl'
             pr = urlparse(config)
             if pr.scheme == "file":
-                with open(os.path.join(pr.netloc, pr.path), "rb") as f:
+                pr_path = os.path.join(pr.netloc, pr.path) if bool(pr.path) else pr.netloc
+                with open(os.path.normpath(pr_path), "rb") as f:
                     return pickle.load(f)
         else:
             with config.open("rb") as f:
                 return pickle.load(f)
 
     klass, cls_kwargs = get_callable_kwargs(config, default_module=default_module)
```

## qlib/utils/data.py

```diff
@@ -1,10 +1,14 @@
 # Copyright (c) Microsoft Corporation.
 # Licensed under the MIT License.
-from typing import Union
+"""
+This module covers some utility functions that operate on data or basic object
+"""
+from copy import deepcopy
+from typing import List, Union
 import pandas as pd
 import numpy as np
 
 
 def robust_zscore(x: pd.Series, zscore=False):
     """Robust ZScore Normalization
 
@@ -50,7 +54,52 @@
         return tuple(deepcopy_basic_type(i) for i in obj)
     elif isinstance(obj, list):
         return list(deepcopy_basic_type(i) for i in obj)
     elif isinstance(obj, dict):
         return {k: deepcopy_basic_type(v) for k, v in obj.items()}
     else:
         return obj
+
+
+S_DROP = "__DROP__"  # this is a symbol which indicates drop the value
+
+
+def update_config(base_config: dict, ext_config: Union[dict, List[dict]]):
+    """
+    supporting adding base config based on the ext_config
+
+    >>> bc = {"a": "xixi"}
+    >>> ec = {"b": "haha"}
+    >>> new_bc = update_config(bc, ec)
+    >>> print(new_bc)
+    {'a': 'xixi', 'b': 'haha'}
+    >>> print(bc)  # base config should not be changed
+    {'a': 'xixi'}
+    >>> print(update_config(bc, {"b": S_DROP}))
+    {'a': 'xixi'}
+    >>> print(update_config(new_bc, {"b": S_DROP}))
+    {'a': 'xixi'}
+    """
+
+    base_config = deepcopy(base_config)  # in case of modifying base config
+
+    for ec in ext_config if isinstance(ext_config, (list, tuple)) else [ext_config]:
+        for key in ec:
+            if key not in base_config:
+                # if it is not in the default key, then replace it.
+                # ADD if not drop
+                if ec[key] != S_DROP:
+                    base_config[key] = ec[key]
+
+            else:
+                if isinstance(base_config[key], dict) and isinstance(ec[key], dict):
+                    # Recursive
+                    # Both of them are dict, then update it nested
+                    base_config[key] = update_config(base_config[key], ec[key])
+                elif ec[key] == S_DROP:
+                    # DROP
+                    del base_config[key]
+                else:
+                    # REPLACE
+                    # one of then are not dict. Then replace
+                    base_config[key] = ec[key]
+    return base_config
```

## qlib/workflow/cli.py

```diff
@@ -1,19 +1,25 @@
 #  Copyright (c) Microsoft Corporation.
 #  Licensed under the MIT License.
-
+import logging
 import sys
 import os
 from pathlib import Path
 
 import qlib
 import fire
 import ruamel.yaml as yaml
 from qlib.config import C
 from qlib.model.trainer import task_train
+from qlib.utils.data import update_config
+from qlib.log import get_module_logger
+from qlib.utils import set_log_with_config
+
+set_log_with_config(C.logging_config)
+logger = get_module_logger("qrun", logging.INFO)
 
 
 def get_path_list(path):
     if isinstance(path, str):
         return [path]
     else:
         return list(path)
@@ -43,18 +49,55 @@
 
 # workflow handler function
 def workflow(config_path, experiment_name="workflow", uri_folder="mlruns"):
     """
     This is a Qlib CLI entrance.
     User can run the whole Quant research workflow defined by a configure file
     - the code is located here ``qlib/workflow/cli.py`
+
+    User can specify a base_config file in your workflow.yml file by adding "BASE_CONFIG_PATH".
+    Qlib will load the configuration in BASE_CONFIG_PATH first, and the user only needs to update the custom fields
+    in their own workflow.yml file.
+
+    For examples:
+
+        qlib_init:
+            provider_uri: "~/.qlib/qlib_data/cn_data"
+            region: cn
+        BASE_CONFIG_PATH: "workflow_config_lightgbm_Alpha158_csi500.yaml"
+        market: csi300
+
     """
     with open(config_path) as fp:
         config = yaml.safe_load(fp)
 
+    base_config_path = config.get("BASE_CONFIG_PATH", None)
+    if base_config_path:
+        logger.info(f"Use BASE_CONFIG_PATH: {base_config_path}")
+        base_config_path = Path(base_config_path)
+
+        # it will find config file in absolute path and relative path
+        if base_config_path.exists():
+            path = base_config_path
+        else:
+            logger.info(
+                f"Can't find BASE_CONFIG_PATH base on: {Path.cwd()}, "
+                f"try using relative path to config path: {Path(config_path).absolute()}"
+            )
+            relative_path = Path(config_path).absolute().parent.joinpath(base_config_path)
+            if relative_path.exists():
+                path = relative_path
+            else:
+                raise FileNotFoundError(f"Can't find the BASE_CONFIG file: {base_config_path}")
+
+        with open(path) as fp:
+            base_config = yaml.safe_load(fp)
+        logger.info(f"Load BASE_CONFIG_PATH succeed: {path.resolve()}")
+        config = update_config(base_config, config)
+
     # config the `sys` section
     sys_config(config, config_path)
 
     if "exp_manager" in config.get("qlib_init"):
         qlib.init(**config.get("qlib_init"))
     else:
         exp_manager = C["exp_manager"]
```

## qlib/workflow/exp.py

```diff
@@ -329,15 +329,15 @@
         try:
             if recorder_id is not None:
                 self._client.delete_run(recorder_id)
             else:
                 recorder = self._get_recorder(recorder_name=recorder_name)
                 self._client.delete_run(recorder.id)
         except MlflowException as e:
-            raise Exception(
+            raise ValueError(
                 f"Error: {e}. Something went wrong when deleting recorder. Please check if the name/id of the recorder is correct."
             ) from e
 
     UNLIMITED = 50000  # FIXME: Mlflow can only list 50000 records at most!!!!!!!
 
     def list_recorders(
         self,
```

## qlib/workflow/expm.py

```diff
@@ -411,15 +411,15 @@
                 self.client.delete_experiment(experiment_id)
             else:
                 experiment = self.client.get_experiment_by_name(experiment_name)
                 if experiment is None:
                     raise MlflowException("No valid experiment has been found.")
                 self.client.delete_experiment(experiment.experiment_id)
         except MlflowException as e:
-            raise Exception(
+            raise ValueError(
                 f"Error: {e}. Something went wrong when deleting experiment. Please check if the name/id of the experiment is correct."
             ) from e
 
     def list_experiments(self):
         # retrieve all the existing experiments
         exps = self.client.list_experiments(view_type=ViewType.ACTIVE_ONLY)
         experiments = dict()
```

## qlib/workflow/recorder.py

```diff
@@ -320,15 +320,15 @@
             local_dir_path = str(local_dir_path.resolve())
             if os.path.isdir(local_dir_path):
                 return local_dir_path
             else:
                 raise RuntimeError("This recorder is not saved in the local file system.")
 
         else:
-            raise Exception(
+            raise ValueError(
                 "Please make sure the recorder has been created and started properly before getting artifact uri."
             )
 
     def start_run(self):
         # set the tracking uri
         mlflow.set_tracking_uri(self.uri)
         # start the run
@@ -460,15 +460,15 @@
         for key in keys:
             self.client.delete_tag(self.id, key)
 
     def get_artifact_uri(self):
         if self.artifact_uri is not None:
             return self.artifact_uri
         else:
-            raise Exception(
+            raise ValueError(
                 "Please make sure the recorder has been created and started properly before getting artifact uri."
             )
 
     def list_artifacts(self, artifact_path=None):
         assert self.uri is not None, "Please start the experiment and recorder first before using recorder directly."
         artifacts = self.client.list_artifacts(self.id, artifact_path)
         return [art.path for art in artifacts]
```

## Comparing `pyqlib-0.9.1.dist-info/LICENSE` & `pyqlib-0.9.2.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `pyqlib-0.9.1.dist-info/METADATA` & `pyqlib-0.9.2.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pyqlib
-Version: 0.9.1
+Version: 0.9.2
 Summary: A Quantitative-research Platform
 Home-page: https://github.com/microsoft/qlib
 License: MIT Licence
 Platform: UNKNOWN
 Classifier: Operating System :: POSIX :: Linux
 Classifier: Operating System :: Microsoft :: Windows
 Classifier: Operating System :: MacOS
@@ -44,39 +44,43 @@
 Requires-Dist: ruamel.yaml (>=0.16.12)
 Requires-Dist: pymongo (==3.7.2)
 Requires-Dist: scikit-learn (>=0.22)
 Requires-Dist: dill
 Requires-Dist: filelock
 Requires-Dist: jinja2 (<3.1.0)
 Requires-Dist: gym
+Requires-Dist: cryptography
 Requires-Dist: dataclasses ; python_version < "3.7"
 Requires-Dist: protobuf (<=3.20.1) ; python_version <= "3.8"
 Provides-Extra: dev
 Requires-Dist: coverage ; extra == 'dev'
 Requires-Dist: pytest (>=3) ; extra == 'dev'
 Requires-Dist: sphinx ; extra == 'dev'
 Requires-Dist: sphinx-rtd-theme ; extra == 'dev'
 Requires-Dist: pre-commit ; extra == 'dev'
 Requires-Dist: wheel ; extra == 'dev'
 Requires-Dist: setuptools ; extra == 'dev'
 Requires-Dist: black ; extra == 'dev'
 Requires-Dist: pylint ; extra == 'dev'
 Requires-Dist: mypy (<0.981) ; extra == 'dev'
 Requires-Dist: flake8 ; extra == 'dev'
+Requires-Dist: nbqa ; extra == 'dev'
+Requires-Dist: jupyter ; extra == 'dev'
+Requires-Dist: nbconvert ; extra == 'dev'
 Requires-Dist: importlib-metadata (<5.0.0) ; extra == 'dev'
 Requires-Dist: readthedocs-sphinx-ext ; extra == 'dev'
 Requires-Dist: cmake ; extra == 'dev'
 Requires-Dist: lxml ; extra == 'dev'
 Requires-Dist: baostock ; extra == 'dev'
 Requires-Dist: yahooquery ; extra == 'dev'
 Requires-Dist: beautifulsoup4 ; extra == 'dev'
 Requires-Dist: tianshou (<=0.4.10) ; extra == 'dev'
 Requires-Dist: gym (>=0.24) ; extra == 'dev'
 Provides-Extra: rl
-Requires-Dist: tianshou ; extra == 'rl'
+Requires-Dist: tianshou (<=0.4.10) ; extra == 'rl'
 Requires-Dist: torch ; extra == 'rl'
 
 [![Python Versions](https://img.shields.io/pypi/pyversions/pyqlib.svg?logo=python&logoColor=white)](https://pypi.org/project/pyqlib/#files)
 [![Platform](https://img.shields.io/badge/platform-linux%20%7C%20windows%20%7C%20macos-lightgrey)](https://pypi.org/project/pyqlib/#files)
 [![PypI Versions](https://img.shields.io/pypi/v/pyqlib)](https://pypi.org/project/pyqlib/#history)
 [![Upload Python Package](https://github.com/microsoft/qlib/workflows/Upload%20Python%20Package/badge.svg)](https://pypi.org/project/pyqlib/)
 [![Github Actions Test Status](https://github.com/microsoft/qlib/workflows/Test/badge.svg?branch=main)](https://github.com/microsoft/qlib/actions)
@@ -84,14 +88,15 @@
 [![License](https://img.shields.io/pypi/l/pyqlib)](LICENSE)
 [![Join the chat at https://gitter.im/Microsoft/qlib](https://badges.gitter.im/Microsoft/qlib.svg)](https://gitter.im/Microsoft/qlib?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)
 
 ## :newspaper: **What's NEW!** &nbsp;   :sparkling_heart: 
 Recent released features
 | Feature | Status |
 | --                      | ------    |
+| KRNN and Sandwich models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1414/) on May 26, 2023 |
 | Release Qlib v0.9.0 | :octocat: [Released](https://github.com/microsoft/qlib/releases/tag/v0.9.0) on Dec 9, 2022 |
 | RL Learning Framework | :hammer: :chart_with_upwards_trend: Released on Nov 10, 2022. [#1332](https://github.com/microsoft/qlib/pull/1332), [#1322](https://github.com/microsoft/qlib/pull/1322), [#1316](https://github.com/microsoft/qlib/pull/1316),[#1299](https://github.com/microsoft/qlib/pull/1299),[#1263](https://github.com/microsoft/qlib/pull/1263), [#1244](https://github.com/microsoft/qlib/pull/1244), [#1169](https://github.com/microsoft/qlib/pull/1169), [#1125](https://github.com/microsoft/qlib/pull/1125), [#1076](https://github.com/microsoft/qlib/pull/1076)|
 | HIST and IGMTF models | :chart_with_upwards_trend: [Released](https://github.com/microsoft/qlib/pull/1040) on Apr 10, 2022 |
 | Qlib [notebook tutorial](https://github.com/microsoft/qlib/tree/main/examples/tutorial) | 📖 [Released](https://github.com/microsoft/qlib/pull/1037) on Apr 7, 2022 | 
 | Ibovespa index data | :rice: [Released](https://github.com/microsoft/qlib/pull/990) on Apr 6, 2022 |
 | Point-in-Time database | :hammer: [Released](https://github.com/microsoft/qlib/pull/343) on Mar 10, 2022 |
 | Arctic Provider Backend & Orderbook data example | :hammer: [Released](https://github.com/microsoft/qlib/pull/744) on Jan 17, 2022 |
@@ -115,21 +120,19 @@
 
 Features released before 2021 are not listed here.
 
 <p align="center">
   <img src="http://fintech.msra.cn/images_v070/logo/1.png" />
 </p>
 
+Qlib is an open-source, AI-oriented quantitative investment platform that aims to realize the potential, empower research, and create value using AI technologies in quantitative investment, from exploring ideas to implementing productions. Qlib supports diverse machine learning modeling paradigms, including supervised learning, market dynamics modeling, and reinforcement learning.
 
-Qlib is an AI-oriented quantitative investment platform, which aims to realize the potential, empower the research, and create the value of AI technologies in quantitative investment.
+An increasing number of SOTA Quant research works/papers in diverse paradigms are being released in Qlib to collaboratively solve key challenges in quantitative investment. For example, 1) using supervised learning to mine the market's complex non-linear patterns from rich and heterogeneous financial data, 2) modeling the dynamic nature of the financial market using adaptive concept drift technology, and 3) using reinforcement learning to model continuous investment decisions and assist investors in optimizing their trading strategies.
 
 It contains the full ML pipeline of data processing, model training, back-testing; and covers the entire chain of quantitative investment: alpha seeking, risk modeling, portfolio optimization, and order execution. 
-
-With Qlib, users can easily try ideas to create better Quant investment strategies.
-
 For more details, please refer to our paper ["Qlib: An AI-oriented Quantitative Investment Platform"](https://arxiv.org/abs/2009.11189).
 
 
 <table>
   <tbody>
     <tr>
       <th>Frameworks, Tutorial, Data & DevOps</th>
@@ -428,14 +431,16 @@
 - [Localformer based on pytorch (Juyong Jiang, et al.)](examples/benchmarks/Localformer/)
 - [TRA based on pytorch (Hengxu, Dong, et al. KDD 2021)](examples/benchmarks/TRA/)
 - [TCN based on pytorch (Shaojie Bai, et al. 2018)](examples/benchmarks/TCN/)
 - [ADARNN based on pytorch (YunTao Du, et al. 2021)](examples/benchmarks/ADARNN/)
 - [ADD based on pytorch (Hongshun Tang, et al.2020)](examples/benchmarks/ADD/)
 - [IGMTF based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/IGMTF/)
 - [HIST based on pytorch (Wentao Xu, et al.2021)](examples/benchmarks/HIST/)
+- [KRNN based on pytorch](examples/benchmarks/KRNN/)
+- [Sandwich based on pytorch](examples/benchmarks/Sandwich/)
 
 Your PR of new Quant models is highly welcomed.
 
 The performance of each model on the `Alpha158` and `Alpha360` dataset can be found [here](examples/benchmarks/README.md).
 
 ### Run a single model
 All the models listed above are runnable with ``Qlib``. Users can find the config files we provide and some details about the model through the [benchmarks](examples/benchmarks) folder. More information can be retrieved at the model files listed above.
```

## Comparing `pyqlib-0.9.1.dist-info/RECORD` & `pyqlib-0.9.2.dist-info/RECORD`

 * *Files 8% similar despite different names*

```diff
@@ -1,86 +1,88 @@
-qlib/__init__.py,sha256=HdT67bLw6f0xYAWFSQPUGdC6GhTLc-3gCNzQoBwtHXA,11959
-qlib/config.py,sha256=FOK3DYxPoNBiLnpsrOok6iJCDGeXoQyfGV6bxRsP4dU,18102
+qlib/__init__.py,sha256=aI6TfVKx_q29tYDoh3-jCI8OdviL2U6lgSbArI90YHw,11959
+qlib/config.py,sha256=XllNieM0Gm0UZ2yMAsvp-85YTy2MGPthUaVOLr3W6ME,18131
 qlib/constant.py,sha256=UtvceKWAt1x72ptlMvZdeR0eiC2RGBoea4yhB-bDnag,521
 qlib/log.py,sha256=mhF0YanZqKARtkk76AWoPx484ykCm4yp3vt85Z04B-w,7864
 qlib/typehint.py,sha256=rEL9jK_E9a7zRd_ybaO3ehEN1CP0PPaALUFA36ZhJD0,2058
-qlib/backtest/__init__.py,sha256=W1jfxBzlobW5M_5-J0I73zp-Jf65MIX-RXWHNQg_Xrw,12144
-qlib/backtest/account.py,sha256=OHFUiwbL14H9C92CaYVmwksWLsXhUOScMXBDj8PqqaA,18379
-qlib/backtest/backtest.py,sha256=X-qv0g04z_Rq8h-UJBNDZfzsmSuC0W0TH1DfScvztso,4142
-qlib/backtest/decision.py,sha256=15NoI8jAQDU1PkdSvO6SVpIcY_3AB2de5kFQi-ViufM,22447
-qlib/backtest/exchange.py,sha256=QbVvSLP4th3ZgnC9_-td7-gENHu72yttkxHa7L5teHE,45054
-qlib/backtest/executor.py,sha256=ZDNJ1aZylfg7BfGwO09ddxTAG-K0zPy1maU0bwOnArw,27198
+qlib/backtest/__init__.py,sha256=RkaZr4hWEz8jWB3xDkEhArKeCQcgveB_L_RBVMpFnQ0,12165
+qlib/backtest/account.py,sha256=cBiG8yo2vs2cXXiyjktadR7eAb7MEhRqnM-ZQAI1U4s,18416
+qlib/backtest/backtest.py,sha256=nJvipPMNJ0W7eLU68zoa7SVnuMYPaIqih6sqw0StZGE,4149
+qlib/backtest/decision.py,sha256=poZtFbyPOtHHW_w4VszeUkXUPwot4Hsa1WXW5-INglE,22466
+qlib/backtest/exchange.py,sha256=vi3ER7H-rEanYH3uk4Zuuwm6oFrUfUWyxwLoOCVJnEs,45143
+qlib/backtest/executor.py,sha256=dquUzndpFIfabNQbz-P-FLUUnpF4Nbyqug7RdK_Q-og,27240
 qlib/backtest/high_performance_ds.py,sha256=Rl3jP-6XyLj8DFHzZCMOQYcKGPD-d6ers18g8ZoSyfc,23892
-qlib/backtest/position.py,sha256=9bPSuaHeB-dqCKGHtHw-8WOSqIwEkPUyiwP3pM7Q1SY,20569
+qlib/backtest/position.py,sha256=rs2y2khG_JEZNWDsuk6RFId1vnirGNvpEcuRasg_CK4,20612
 qlib/backtest/profit_attribution.py,sha256=CsjdEO58UkUr9JZ_IG3UgD6s7TrdhA6M41kqD661S2Q,15326
-qlib/backtest/report.py,sha256=lEcL405Wu_CRKr1X2seUWJeG7nW89ZJ_G8WMHsQWpJk,28201
+qlib/backtest/report.py,sha256=L9bgt_h0llCPuiQWdAuDG9S_3RvKh3KXtb04EOKnQxM,28314
 qlib/backtest/signal.py,sha256=XG8GDzig9f7QUDUnHyYKWBTyuXyD2Vb33Gefjmy_HPw,4108
-qlib/backtest/utils.py,sha256=p3RXpXtUSHQRJjp9B6LmG_F-syHfvSZBTv3rENpNccs,10812
+qlib/backtest/utils.py,sha256=nV2t_EVAIoV2yirceiJhXJWOKybO4YrugSWC4n3qFio,10826
 qlib/contrib/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 qlib/contrib/evaluate.py,sha256=dZQqm0CXuOdR9Spnr4QBUuQ_Q9qdo2HdglukZDgT05A,14471
 qlib/contrib/evaluate_portfolio.py,sha256=QeD63NM6L6Fe6FuRggumwE7ZBOpS1TqUbkIxm6D0rwk,6639
 qlib/contrib/torch.py,sha256=fxnEquHKFlQJ9qLF3QkFkkNdpK1_0qoltYlFJljek2Y,1105
 qlib/contrib/data/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 qlib/contrib/data/data.py,sha256=AN61Z4vCcFnAwnFkBUrhVo60izc8VUjMoApII1q_wng,2185
 qlib/contrib/data/dataset.py,sha256=Nzfc4AcFGdNGu6VStTeqAq6vG42rs7Bt2UPxxtySLRk,13957
-qlib/contrib/data/handler.py,sha256=jHssmQXbe6_b_IYb65sXt2SuqlaVuXq26bOAlmRiuW4,20635
-qlib/contrib/data/highfreq_handler.py,sha256=xp5jqyDmDDMrI_-2IC9v5oh6hNd5g4CzgPu7U5IJclA,19031
+qlib/contrib/data/handler.py,sha256=gqa6GIn5bWtfaPs9yjXmU_dStkxSGCPQdtM6rkMM6jc,20641
+qlib/contrib/data/highfreq_handler.py,sha256=JsT4JSxXzbH4_D7lwYShaW4pYG2F_CRGOJulJy6XPFk,19277
 qlib/contrib/data/highfreq_processor.py,sha256=sxDD8aSWhTxbTUkOkITJc3burGGINC_M2Pk_cZZPuQs,3151
-qlib/contrib/data/highfreq_provider.py,sha256=WC5SE6ThAUUk3zYDAeOjczQQgs0IUiyOgmSpunxMH0A,12540
+qlib/contrib/data/highfreq_provider.py,sha256=nuySm_1RGRncadso_7TcLUzxlABVo53o4vWVpxHwT_g,12588
 qlib/contrib/data/processor.py,sha256=5sHlEibP65uum-0J9X9RP1yh0FBO4dT70GS5OgwK-gQ,4571
 qlib/contrib/data/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 qlib/contrib/data/utils/sepdf.py,sha256=e__2Ipkx8f55YnMx-OPkzFsW-7prESnHo38ZPoRZzFs,7163
 qlib/contrib/eva/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 qlib/contrib/eva/alpha.py,sha256=jC3RCExBlLn7fvz31FI4KhQE7qh3zIPucwBEStt117g,6847
 qlib/contrib/meta/__init__.py,sha256=bhS50Fc9PCZP7DBdnG8mkA2_k6MERSUBPdeV-YAtIaY,207
 qlib/contrib/meta/data_selection/__init__.py,sha256=NHHRX2tUJXq6bBC4hukoF9ffH9u-oSCrEMGSTRHGxfM,219
-qlib/contrib/meta/data_selection/dataset.py,sha256=dlkfPGeTLuEvBnwxCNAlYckS3VPVvmxfXk8CvKoqBmM,14695
-qlib/contrib/meta/data_selection/model.py,sha256=gWNyO55C_XAECoIzcwp5hfS6mytK-B79VaEePMk4vtE,6496
-qlib/contrib/meta/data_selection/net.py,sha256=iXilr9p3FDgF4us3IwLfAda-tBRLi-2VoEXJg73f7zY,2824
-qlib/contrib/meta/data_selection/utils.py,sha256=hVfs5qT1cNcKmdI6WRf93w1hMwFyc2g56coIv_XWxxo,3506
+qlib/contrib/meta/data_selection/dataset.py,sha256=w904zNk0HckzcyrR1no-k9yMT3RLrVpI8HqzYfYoA2g,18148
+qlib/contrib/meta/data_selection/model.py,sha256=bCmnUwpwYElAlRsUEzBNkaMtMTNC7OTzm1pD1NB_VoE,6615
+qlib/contrib/meta/data_selection/net.py,sha256=aLERCdcTPweZhoFBQO19AMG1X64oNrvdYCZhHSKR0S8,3098
+qlib/contrib/meta/data_selection/utils.py,sha256=kehCNJkEzPoR1zAnNpWF6MRGTx03GU64BMEL48RYQR4,4075
 qlib/contrib/model/__init__.py,sha256=_kfQ7WmL5yT4xsf_MLqip2znd_1PG0BPS7s6Ehyj_us,1754
 qlib/contrib/model/catboost_model.py,sha256=w9ivJqmvc_PUw52WiwvImCrjWc9x6U11whU1puXC4J0,3878
 qlib/contrib/model/double_ensemble.py,sha256=h3QvMQCxQc1rhTyGpx0a1b5t9FRclsFtu5b9ZPISH3U,12459
 qlib/contrib/model/gbdt.py,sha256=Bih0uEfr1kJrX1SiY9XbzkmxjVKOo12ulrajHb9_PGY,5061
 qlib/contrib/model/highfreq_gdbt_model.py,sha256=zqefgzTBMMZTLtHuIVdK5VBYMnuJecW--_zL8sOFkE0,6810
-qlib/contrib/model/linear.py,sha256=-OTgAp8LUfscxiG-5wBFN-mDRcjp2TrfI_aCLtRU3wk,3700
+qlib/contrib/model/linear.py,sha256=y0_PWTSUusKtcvJM0KH33bJ7G-oEH9tu3Es-ngtrrC8,4315
 qlib/contrib/model/pytorch_adarnn.py,sha256=GpBlkleKG5Uyu6uxYbFRrZWhTHIMhovIkO9k0rlgXG8,28733
 qlib/contrib/model/pytorch_add.py,sha256=T7mEfYiAezSCV1Ngl8kHuy16kZq9-_AtA9t81XkbXGc,22112
 qlib/contrib/model/pytorch_alstm.py,sha256=pZbV3ui2ry8qaaVYB8Wo8SmwXGiNK08VPjp-7TurIDg,11690
 qlib/contrib/model/pytorch_alstm_ts.py,sha256=gJXmc-Ns9blibxXHDqQkY4c0J2h8zIvWuoVWWZ49W1Y,11925
 qlib/contrib/model/pytorch_gats.py,sha256=teNlCFslg0kLYucvEI-Z4ecvP6HueDqvVOwXGWSkOPY,13097
 qlib/contrib/model/pytorch_gats_ts.py,sha256=T6JkP0H6BKbusqgrxiZSNH_wTGMF5eV0cVmnifd2BBg,13548
 qlib/contrib/model/pytorch_gru.py,sha256=mKj9x_mj0XnPy6CTg7cSVl9rZM6zSGo9yIxCflxRTMA,9973
 qlib/contrib/model/pytorch_gru_ts.py,sha256=5C7oXbfXSAvUozBh9QvMs32KuiqyzpfXa_N0WDnZ4Hg,10187
 qlib/contrib/model/pytorch_hist.py,sha256=qLRYTN8IT8psPyOAchgByPAAmmok0A4TfH-3JRjrJq0,19174
 qlib/contrib/model/pytorch_igmtf.py,sha256=AKUUgTo_fx-Rg9eyYJmUIey_c84UMtJ4TUbhiMFaFcI,16293
+qlib/contrib/model/pytorch_krnn.py,sha256=kHgK-jv5lFe2vQNA0i_eistz-swBC7nj3CrklWROWj8,16228
 qlib/contrib/model/pytorch_localformer.py,sha256=zpt4-kxhvNEG2ke32qppm-f1sQ-G8XCiEKcs-3y0puQ,11054
 qlib/contrib/model/pytorch_localformer_ts.py,sha256=2LrQXhTiHGCp704lsvjjMAbBWQIEa3jqwU8GWTJV4ro,10377
 qlib/contrib/model/pytorch_lstm.py,sha256=HWEE_lIz8BQbQMxaKzAZwE3TtLRg3OBM9oZJqqGiUjE,9728
 qlib/contrib/model/pytorch_lstm_ts.py,sha256=N880Pc0gZRwhMrCnANy1sdvoSrG6IA7Q0T0Dzbx1W60,9974
-qlib/contrib/model/pytorch_nn.py,sha256=cALX57wyDK8ppssFQ6fsQetZhnc-Ajqon8-HBkwVtLQ,17924
+qlib/contrib/model/pytorch_nn.py,sha256=JX58XDI52Jz_O2JzY-1hN2hP68EUClvIIoZ0sZkLY0o,17587
+qlib/contrib/model/pytorch_sandwich.py,sha256=u51_bpuQfrvF0YuwoCp7XxbMHIhSQkbD8gfaWZxNU5I,12042
 qlib/contrib/model/pytorch_sfm.py,sha256=ichSvLYU8X0V8IOtJpGAW8In_TS4U2K5cnupErAuccA,16377
 qlib/contrib/model/pytorch_tabnet.py,sha256=mut5YfP_hfZ3CAG6syVqhqDiG8lTKHAciXdHcsMmkkk,23506
 qlib/contrib/model/pytorch_tcn.py,sha256=-JVgP5ao8Vm_LO9iX1K3fm0cddhl_pj72nrtksy9qp0,9903
-qlib/contrib/model/pytorch_tcn_ts.py,sha256=Y-zBy_Af96D8PDUoDWWGax6r3J5OPTpuTGVFkwvRgus,9374
+qlib/contrib/model/pytorch_tcn_ts.py,sha256=MX-X--9Igi8g8oo0N0GZ_xDpIBnkdiKhsn6OBv9gE_E,9468
 qlib/contrib/model/pytorch_tcts.py,sha256=hKWISfrkx2j80ZutcJ_0RxOsGB2gpwLHwGLKPJlzg7s,14727
 qlib/contrib/model/pytorch_tra.py,sha256=I7rIZaDh0duJ7DHqH29erX7Sx3z7S1xulUUpfhZF8DY,35174
 qlib/contrib/model/pytorch_transformer.py,sha256=kx4BxNj2CjXpPXbS4Qn_JMh31QNqaUsmfGbXh5bnYjw,9881
 qlib/contrib/model/pytorch_transformer_ts.py,sha256=brJXZPXx8wyp-uDNdc_9PCbxfWQsdrgg9D8Jwgrvnew,9179
 qlib/contrib/model/pytorch_utils.py,sha256=SSysDG8nhbvCBK7gjCcI1NIZEf5KbcPZPa99psig7XQ,1234
 qlib/contrib/model/tcn.py,sha256=cV4JdS77oeCOpbRRJ65u2Ebj9X61dn1VdptNT42r19M,2682
 qlib/contrib/model/xgboost.py,sha256=Lkb410LvfDIFzAUgxtyLjXXSGxkO48ECfdp2QPDQdVE,3170
 qlib/contrib/online/__init__.py,sha256=sR8osPGs_a8HsW2IZ9QRQuNUbeCgXNPjsbRIEArmxcU,607
 qlib/contrib/online/manager.py,sha256=PQJA75Rb7Ym5zsZ4dnneM-vwKxIjjVDvrPEs8D-w-Sg,5636
 qlib/contrib/online/online_model.py,sha256=ot93XYONg4egpK09bnr3mlXh77Z5gBDrPrlBwGOyYZE,1149
 qlib/contrib/online/operator.py,sha256=mn2W7-plBAWSBKT8KlzPeE-CYOyYkzM1lt3kWG-R698,13138
 qlib/contrib/online/user.py,sha256=rBysCTPLnmpUXYbwGGUyKSchOUi_NYR9v7-9xgS76EM,3057
 qlib/contrib/online/utils.py,sha256=EJ-4fagemHmNu6NAqkNer9dPfRpdhRYR439fqqm9DN8,3177
 qlib/contrib/ops/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/contrib/ops/high_freq.py,sha256=IADE7jKCDiRs-Q7HrAqT2C3Wt7ZkTKb3nTyYvJJ6Pi8,8202
+qlib/contrib/ops/high_freq.py,sha256=jn1Ml0FcKptf8Y2JOsPjjnH1tmpU8tF_wrmBQSN0Z_4,8428
 qlib/contrib/report/__init__.py,sha256=6VBXkJS3olq0729kDPOvM7MYX4r_RC3T95HhIUdp1-M,367
 qlib/contrib/report/graph.py,sha256=pLN8If9qVOx_upgke5GF_dVZvnLnulkqjLH0aG7surU,11574
 qlib/contrib/report/utils.py,sha256=oAVJmFEq8OT6RBDp-wK4Y1OX3LQlid3dlJJInztDerg,2541
 qlib/contrib/report/analysis_model/__init__.py,sha256=CZyrTgVMoJCH_H9i8nSwy3SEvoPuz9TdkT8GsnJ_44s,185
 qlib/contrib/report/analysis_model/analysis_model_performance.py,sha256=VXEZcMaIegqUwWeZGUWk2Kx2qJUoSIGuSSsUF-LDzlE,11719
 qlib/contrib/report/analysis_position/__init__.py,sha256=DF6zU5wUrZtaSnm64s5QrutuxpkvpAEl9JGsg6ZXo3Q,415
 qlib/contrib/report/analysis_position/cumulative_return.py,sha256=yiEXG7MVYUm4QIBlFERy-YnklirQBqMlhei8eNtkvTU,9725
@@ -91,15 +93,15 @@
 qlib/contrib/report/analysis_position/score_ic.py,sha256=OavmzPZL2qeMAUuHvhTxc-0zVlZ4qTuoXmEmXWyCo8w,2641
 qlib/contrib/report/data/__init__.py,sha256=dtBK1Y9yoq-fppZV8R59BmF53UGTFK9gQrQDc1mEz1I,131
 qlib/contrib/report/data/ana.py,sha256=nWUou48Jk112CbocVol_Saq4DC4aHnX1CTuYNUXXheo,6810
 qlib/contrib/report/data/base.py,sha256=nDIP2K-lHQb9ws7kihwuVB5M1uD7yrO2NzKcUy6C2bY,967
 qlib/contrib/strategy/__init__.py,sha256=HU42uZOVXcNvHOYumuYBt9KXxD-KYbxfLClI0JURzeA,540
 qlib/contrib/strategy/cost_control.py,sha256=3uY6HhccfqHcza2mr6xOmzDYMDBLXwOkCNWu0kBIaM0,3928
 qlib/contrib/strategy/order_generator.py,sha256=Q3N6_Cvb8Dkrfiu-Gahc8rcGXtAQ1CLzeV60tr2fylo,8478
-qlib/contrib/strategy/rule_strategy.py,sha256=OZmHPc-IZ4Ke0m54ptrwdEUDfnGX_BnbCBLmO_E_LD8,30048
+qlib/contrib/strategy/rule_strategy.py,sha256=RCdfvNEapwwNHMUfY_9uXCq3BRNzN8EXPRy6i867dUc,30045
 qlib/contrib/strategy/signal_strategy.py,sha256=WMLLNuTu6ga8Gg24L1lCPP9g_CvQhKqWParOfltHC9I,23023
 qlib/contrib/strategy/optimizer/__init__.py,sha256=F0-1v-SqvAIikXBMBQLj6i-O2ZDhw3pwKI5ENuDYg1s,295
 qlib/contrib/strategy/optimizer/base.py,sha256=42FYnLSCIKbd_-8oOzjum71-RC809l7x0W-NsWHQPWI,326
 qlib/contrib/strategy/optimizer/enhanced_indexing.py,sha256=e7ZdbR-Jtf7XP6PQO-bsQX4Q0pTSVZ_tL2aWk92DPmg,6717
 qlib/contrib/strategy/optimizer/optimizer.py,sha256=fzz-K8WulvBWEIm5qaAE5GfYgqqFRswGgFfViiv05xs,8906
 qlib/contrib/tuner/__init__.py,sha256=mzpYtSSiJ3FA4-1qo7EgF2bIPiU33Tiz0Fyk0vK93OM,37
 qlib/contrib/tuner/config.py,sha256=C8bUdRZM85gkaEt5LiBdn91Wlp0MSOiz86tlzV8rOzs,3720
@@ -109,30 +111,30 @@
 qlib/contrib/tuner/tuner.py,sha256=1uxdJ1DHe6P32mOb_VNVTAJw8S8z-3BOV1mDp2urBNk,8231
 qlib/contrib/workflow/__init__.py,sha256=sDKpsibnrzwU26YBp0H5LQG5nyKqYvG-NficMUZ5Oao,213
 qlib/contrib/workflow/record_temp.py,sha256=Ru1QGd3iYFBZ02Ie8vs6hC6938DJxfA3aqcEi4guAxw,3454
 qlib/data/__init__.py,sha256=oDv-vXV31G47m0ZYAUh57DPf1PJggtr4SWf7AuSVqbk,1476
 qlib/data/base.py,sha256=zCkQO0_LWnyH8pGU_D8P0KErYhJQpv3q285RctsjBHk,8668
 qlib/data/cache.py,sha256=IJgGdFTblph3naQ7MyT6cQ41R4mSPyd5gL2d4Z0Ih-4,48454
 qlib/data/client.py,sha256=Y0ELILOtNQScrwMbxyOBo9_mtVNHQVM3kix2j8KTwf8,3852
-qlib/data/data.py,sha256=aozAMWIMEbcWERzK4RAbN0WqoOmUXOYRz0OcLqwZ2AI,50784
+qlib/data/data.py,sha256=OgKJDHxiBGzkBtqClf2Qnhx-WjlYW8JIrrmFyOFCytY,50822
 qlib/data/filter.py,sha256=VUGNrTvYZx7zloterDLZ--GPbQNPyA_3B8BrKngsFvg,14298
 qlib/data/inst_processor.py,sha256=4sxpGYaj60TxCsmihI-pR7rhVZRbQ4nSEBF1-HUC9lM,620
 qlib/data/ops.py,sha256=b3hWJEiq7KMzdSdOJAY2imGXRNTKJXM4N-dGM2mFhpE,47127
 qlib/data/pit.py,sha256=1Dhas3gUd1PXz7mAWjPbZCj9Fu1M-4wU9g3M4mjup6A,3304
 qlib/data/_libs/__init__.py,sha256=aCOr6sEsQpv9z4cJgWFA4qOs4xJqclqYYnxOVcxiK2Q,75
-qlib/data/_libs/expanding.cp38-win_amd64.pyd,sha256=qQyaW-zXdAqngFIsJ_3kht2Oj40TBBkXeni_9YU0-Eo,122880
+qlib/data/_libs/expanding.cp38-win_amd64.pyd,sha256=zaIRv3w-RYpLWTXliIDcMRYkDcaxr397TODCyaP4zyw,122880
 qlib/data/_libs/expanding.pyx,sha256=KsoPvgUfdS4sciHql8v4XNTxUAbaQenCMwKDMm23Eh0,4303
-qlib/data/_libs/rolling.cp38-win_amd64.pyd,sha256=bn-UaJHB6NeTTPnzz9TfXT-YEMbWqI3mx4MYpkcVjos,86016
+qlib/data/_libs/rolling.cp38-win_amd64.pyd,sha256=zlEFXNrZ9bFitlABe39dVn5uscpTiTNmlHG6-R2w8GQ,86528
 qlib/data/_libs/rolling.pyx,sha256=VIJKjdYk_yjaVnZGWzymHaqjskejV3DjQDA0IsScrsA,6318
-qlib/data/dataset/__init__.py,sha256=Ne-USEOuvg2G-x7giNb41CTrruQn91xhPGn4YSX0Mc8,27922
-qlib/data/dataset/handler.py,sha256=MR_qsD17URXoW1h5QbHyRPVFsBtIbWLlnGTTHsR_U10,25489
-qlib/data/dataset/loader.py,sha256=uuNv4RaSVYSOCGq4tkC8Kv4YjhEwAz1v66EMdHweDf8,12421
-qlib/data/dataset/processor.py,sha256=C3s-vVmj25BZ5jWNHc_061PKqbOc1r2cWCBnZYhzq9Q,13267
+qlib/data/dataset/__init__.py,sha256=h8VcuMzJaIjJhxuGZ0NUbX3ViSB0nEyv0pVGJcbAehA,27919
+qlib/data/dataset/handler.py,sha256=DU9YZUTFEZxM8cu667fonEnanIrRlupYj7Y2ptaInT0,27480
+qlib/data/dataset/loader.py,sha256=nW2NtHGtrW9CtS7PNaimmMtmH7ZyHUIbmCe2mVi_XZw,12690
+qlib/data/dataset/processor.py,sha256=RJNskCtK2yHRnyazsrfrUkfT6DCg11XuwdWUK-gMfBE,14801
 qlib/data/dataset/storage.py,sha256=WmqGTnypXbS_h6UkVrPxZotbl-Tfz4FZWYkILiIdRWA,6507
-qlib/data/dataset/utils.py,sha256=DIO0TKStZcL6p5mhjeriljXnnVM8uoIlKALvwFXzD-c,4281
+qlib/data/dataset/utils.py,sha256=SGoWnokmWIw45guXqva1CBMikS7CTny4ZuvUrtsl-dc,4330
 qlib/data/dataset/weight.py,sha256=6smHSNpcnB5VeQk88ceFx06pTJrA-Rvlyl4bRAflEwg,781
 qlib/data/storage/__init__.py,sha256=EoHku26sqCZypwS5xwubeUZg2BR2wlA8A854xUWy-pA,276
 qlib/data/storage/file_storage.py,sha256=cBElcWw-YGufN9MoKXNlQeBfgmobrPNYnxwTn9XGE1w,14768
 qlib/data/storage/storage.py,sha256=vjiruoreGF96amrTx70Vp4JqgSCvAMVvYemUabKvwAU,15156
 qlib/model/__init__.py,sha256=XBWjB3NKBzYTxkjsmtA1gWWL19jF43fiCPk-uxPuO5Q,158
 qlib/model/base.py,sha256=huwwfvNWlgGrQAnQ-vvbCa24b701ko5yId0sdyNxKPc,3881
 qlib/model/trainer.py,sha256=OJs5q8kjEb1WSMWwElDtRs0hiFWa-qR8WTErQ1oW6cI,23384
@@ -154,78 +156,78 @@
 qlib/rl/__init__.py,sha256=IEE54Pl3SKG9AqVKeiRAQ8KoDsrylIrHT4ZaANvgYJI,347
 qlib/rl/aux_info.py,sha256=7xAOr5BAj4Mr1eJ4TPvXYhI0hjlSI4gy0w8shwssrmQ,1165
 qlib/rl/interpreter.py,sha256=z_3n8_Zf3Toqf11-SzkZ2Vx5UfjOwv54Bb0sM4kYueg,5376
 qlib/rl/reward.py,sha256=7qsPJSyKHALRn5faCbo1Lbb95c_AL_ZvsLEXh9igVm0,2788
 qlib/rl/seed.py,sha256=vIbUeVlf2yjMH1Rqh-l8RcBgp4RsBm5Yg0sROK-q_7k,351
 qlib/rl/simulator.py,sha256=Jk1XWXYRbrq5yahE-VH7Gda24PLFz7IPx3pqBb2npMI,3106
 qlib/rl/contrib/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-qlib/rl/contrib/backtest.py,sha256=Vq---oKMmY3ZeXesPCXnSe8ihb458Uer3k_jO7y3NHU,13632
-qlib/rl/contrib/naive_config_parser.py,sha256=xcAkEh5tdS9gVjpfokUS2zB9txl1_8LC2qI2II-thQQ,3448
-qlib/rl/contrib/train_onpolicy.py,sha256=yxYnUDzZx60vJ3zxnK6zq7NcT1nFcuPtfufiYXfCOTI,9086
+qlib/rl/contrib/backtest.py,sha256=zMyPv46Y9lvqcmxc7JW8SS63dW9hSLXZLxA5J1kjf8o,13614
+qlib/rl/contrib/naive_config_parser.py,sha256=bQ4-i9Rrdww2E2hR21mLHvjKpzhohiNXFyJQJPjajYQ,3494
+qlib/rl/contrib/train_onpolicy.py,sha256=DCNIRQ8eMCXW5L1yxd6vnlOm8fpvomLER5w35wuQYQc,10373
 qlib/rl/contrib/utils.py,sha256=GsGl7_5IRTYSwFHGoaoE4276mXVnS9EnVKiFGBSeSms,865
 qlib/rl/data/__init__.py,sha256=1_nFqflPuHiX76gWQG6JL5Uxlox1vAxg0z4aDQKr1GA,253
 qlib/rl/data/base.py,sha256=sQms-RLBkOdkNH9X5Rfo9rzOJhqQrLRv6Riv_yMq_ck,1819
-qlib/rl/data/integration.py,sha256=ai0z5_dzSF6x_KxQ2K4un_bzX5wp27EWkwWVChrgZDw,6358
-qlib/rl/data/native.py,sha256=wSxRxP_ArHL18fTCt1gbirbp3ocgjRq525xNnDM5kak,4595
-qlib/rl/data/pickle_styled.py,sha256=ESicoSmigpvwrkNr8aerhw2P9GaPBKBcLlWO73FAgRo,10824
+qlib/rl/data/integration.py,sha256=BcIaXtjKWxviHcBxpDxITcVax8JOviW-92PizqgxSfk,3193
+qlib/rl/data/native.py,sha256=fO5ipPPvQiWaLJz12U7aBLbA0J5iNLOVu7e4XqE9_Yg,7645
+qlib/rl/data/pickle_styled.py,sha256=7PWyb4DewP4Mz8ckbLd74c28dZj6ovFuKvPL55BsrwQ,10840
 qlib/rl/order_execution/__init__.py,sha256=lJ6yNdMqqlpk3zuDmd9AiTWJ-ABgr2t_qP62DL_ABY4,1046
-qlib/rl/order_execution/interpreter.py,sha256=9ueGXhtLAwyweRMYF0h2-WPVGtWFUtklAHw51l8kHds,9412
+qlib/rl/order_execution/interpreter.py,sha256=XVNpMmRAiCApCoGJRYroQGVWBNb8D0RChk__lRFGzxA,9926
 qlib/rl/order_execution/network.py,sha256=vuDld-Bz779xC1wd2KnrWVPatE07cBn3xop6okTIOCE,4970
-qlib/rl/order_execution/policy.py,sha256=tbU3h6ovgvbb5BK1-sj2c_igrDfHeoeYpc1IkYY3Szk,5516
-qlib/rl/order_execution/reward.py,sha256=IT_rOglx54lBBo43icn1ag7mCvAfU8lqpbJb2OhFIio,1752
-qlib/rl/order_execution/simulator_qlib.py,sha256=Pwk_BNd1pnrr8e5J1IFPFFry4j7jMlpmWrP2_tGRql0,4908
-qlib/rl/order_execution/simulator_simple.py,sha256=x_Ac0QB0c1oNoQTWxdeNP2FoJfpIwcl0TZJ-CFVmJPA,13727
+qlib/rl/order_execution/policy.py,sha256=j8TKU2CkNshmOa8ZX02jgb8S4XEZr2uQhAgeSSV_Jjo,7245
+qlib/rl/order_execution/reward.py,sha256=b67Fi5p6ghzZSmFG2xWjZuz--xfVXx-no0bPsq7h_Qk,3648
+qlib/rl/order_execution/simulator_qlib.py,sha256=wIcL6Pr_Sphj6NgPHQTmCjGcaOzzG_C9GDsGT_B6Fv0,4906
+qlib/rl/order_execution/simulator_simple.py,sha256=pcsrnD_8ubC1DYrWahRSZh1vsEzhLY9prULgHOFI5_A,15104
 qlib/rl/order_execution/state.py,sha256=VFlY5KlpMU0eEs-iKsN1rz7v4TRRJTmfR-QN9TlRNTg,3791
-qlib/rl/order_execution/strategy.py,sha256=r8Z9p1sJS4Bd5BgiU1IG61l-Of-IL-TmAwc2rwjWYpw,20828
-qlib/rl/order_execution/utils.py,sha256=qE0Wt1pFOMGFn6pMFQlMj7jM1TZXG5MJvRIzKPYoLVs,1973
+qlib/rl/order_execution/strategy.py,sha256=3EpOjLBXfBl1R_8sLjGVo1iIYbkXbRZpYMaPHV0TtJ8,21728
+qlib/rl/order_execution/utils.py,sha256=iSZ_XPyeOHUcssOjfHTKcS6MOV9JLVPtAvAppmJyB08,1683
 qlib/rl/strategy/__init__.py,sha256=ocvADP5YmkFQo-gDraKgGI2ZbD6XHt4-B45Cz02WElI,159
-qlib/rl/strategy/single_order.py,sha256=mWuJKQD5euPoHzZqOo_1QXBPnIhhqOJFvj0ZrEp6xek,1030
+qlib/rl/strategy/single_order.py,sha256=kEATByqUe3KXAFzNJLP2mdjxwWuV5HJZhgmgYqrlZnU,1082
 qlib/rl/trainer/__init__.py,sha256=Cxtr3smPraMbX82dMIJ4FH9rXHPE6dwead4f-l-5NY8,483
 qlib/rl/trainer/api.py,sha256=EIYKzOyEdxkG6sZF8Yj9vBsRbcD6rQnmB5FZ8Z0HpB8,3768
 qlib/rl/trainer/callbacks.py,sha256=z6TdUjt3MuOQ0bzwpsexAfQVHkaDKdb3vgzm9bh6GOI,11867
 qlib/rl/trainer/trainer.py,sha256=6VHuWIiZrP91PSoGf6HSLp1QqUYAMAP-mp-_uJKH7lc,13831
 qlib/rl/trainer/vessel.py,sha256=y6tOVS3PHMjvHtJqcLfE681wyC3dLY2FJ0EGFdCv_YU,10103
 qlib/rl/utils/__init__.py,sha256=HXkTy-nDy6OHiwVGKwDBbTvdW2ECeaFPl278wePVgPs,548
-qlib/rl/utils/data_queue.py,sha256=TlEimixieGCp5r2uoHbDrv9hN-Ivw0jR2UWhGRJr9P4,6534
-qlib/rl/utils/env_wrapper.py,sha256=YshijO9rbYyrq_6ZtUwQJtComgteBh3zC22L0E7GBtM,10073
+qlib/rl/utils/data_queue.py,sha256=n7afyWB2WEeyiYzDTDEQw2JTw0aUMmUltJiSbV2pRJ0,6785
+qlib/rl/utils/env_wrapper.py,sha256=_wN0hHhjgYTM0Zb-766qStiJ0o8U75u05UwWmBo_oio,10094
 qlib/rl/utils/finite_env.py,sha256=YH3729mMOLC0BHPo6sn3oc6VhuAf-hgqsoSU0ZUXEyI,13736
-qlib/rl/utils/log.py,sha256=epnK71ES0FmVH1Y2rwW2JVMRuF8VHB4FmqfHflINS4M,19057
+qlib/rl/utils/log.py,sha256=RjdNEJbiI8R3JbeU2M_z1_G_c7e3nRyYfQdwZMi_LX8,19064
 qlib/run/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 qlib/run/get_data.py,sha256=_v_-H5iMalXUVn8hJhyKygpUhJFM_vUev0O3l7_Q4AY,185
 qlib/strategy/__init__.py,sha256=aCOr6sEsQpv9z4cJgWFA4qOs4xJqclqYYnxOVcxiK2Q,75
 qlib/strategy/base.py,sha256=jYBFiS1rUvsHQAJaL-RG7kMxCsHAzEOEzRRr63z9Pwg,11451
 qlib/tests/__init__.py,sha256=SxH9liQfb_0zJwxgxWNRnYKW6Ze25VpfWQREQy988ns,12511
 qlib/tests/config.py,sha256=3hzC5qMEFpGLUSvDQ57K41a2_d0HI88aw5zTBgoPbiw,5001
-qlib/tests/data.py,sha256=VS3vXJvnJRwoBEWQEsndvAlCGwVrWWkHWeDOFN3XXpw,6901
-qlib/utils/__init__.py,sha256=_aCI-Rup_giVf3vEyAocFXz_wWUcqYI4W3BAgKnTaDU,35054
-qlib/utils/data.py,sha256=yn3h-PXaLgicyde7MIKZF7q5HHuAlAw4108g96YQnFU,1456
+qlib/tests/data.py,sha256=T6GzOVXNeUabG_5sF-pmks-NSf3PATPNjWwZOxFm_0c,8192
+qlib/utils/__init__.py,sha256=WBn2-hUjPN-XbAFU0p3Ezb5YLCtmqbJTFF_Nq49V9EU,35251
+qlib/utils/data.py,sha256=UYlAnNI8MW4mPhaa9-ovNoGTmj-DennPaBweaWJg4iE,3190
 qlib/utils/exceptions.py,sha256=ZU8DxuFaGxPmWFljQ3cX3xPh2isRkY9oKoFpLlHVkGc,457
 qlib/utils/file.py,sha256=5XNW4D9e9JKa4FOSGwV6xvdN5lKVGPVLJyELRjEWnXw,5928
 qlib/utils/index_data.py,sha256=Il8FBAXWUPxTb1joBkDVm9gexWHGZ4KL1OIQZ1A9R8M,22537
 qlib/utils/objm.py,sha256=x8pzFJgJABmAdQWcpYvhBfF6gV6ckbroc0TWcKu7Hwg,3435
 qlib/utils/paral.py,sha256=Yez9UkLr08691xohG7T3CaVjnl6MYc0EwW6GIbiRz3c,9433
 qlib/utils/resam.py,sha256=n6g7ZY7MwGniSi_H3MxZYfUTysOGp7oBD1H97kSf7C8,9626
 qlib/utils/serial.py,sha256=lSl_3YVqFk82DKMXYkhhvOy8T1bVcg9ZaWPz3T3njis,6263
 qlib/utils/time.py,sha256=cLKB_hhcxJr8BBZcSC0W64wIXgmyJLsFdnhs8xumGkw,12155
 qlib/workflow/__init__.py,sha256=RXAWBqSzfFR8fz7Y1iTSansBhWQbMk99otClH9AaOsE,25250
-qlib/workflow/cli.py,sha256=Sf8P8N8-NCJc2d6VTrJn2nY5zWCHh4vaniW2jwBHvtM,2035
-qlib/workflow/exp.py,sha256=VT5T7HPLF4eWOm85UL5OB0HBXfO9DPlc7PTUEZm2lTQ,15191
-qlib/workflow/expm.py,sha256=l0Opa6eMnddXNgo5hHZKet7Knrs7T0sFWayfq6ZRSe8,18007
+qlib/workflow/cli.py,sha256=qU6hI2834JUghU24odU5DE3YjB3eERgwWczz0I5NGck,3809
+qlib/workflow/exp.py,sha256=vomFDMJW38e6STx396swFBEEmxCBgeHj7a7982ricFA,15192
+qlib/workflow/expm.py,sha256=GNIyaBvAao2pJTJhEZ6CPXdxWV57VESaCmoFakiiZw8,18008
 qlib/workflow/record_temp.py,sha256=DdLWVogajtFlYN51l1yTnu7ni6dpcINAnWtkuBe8SX4,22416
-qlib/workflow/recorder.py,sha256=AFR7WBkFZyx5XQ0b8bAB60w85eEw5cLgjWBS38c0DI4,18255
+qlib/workflow/recorder.py,sha256=VGcOxMut49FeiUeaB2RCAzNbGGR4ekBz3Q5z1KzzqYQ,18257
 qlib/workflow/utils.py,sha256=-G6VMOO7V2mXUtm01lclB_jnC6B6Jr-QsCDvMfumBuk,1663
 qlib/workflow/online/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 qlib/workflow/online/manager.py,sha256=W3pOdrx8WsBh5WrqoInLeGx3BDQ6hK9hXGeR1y2uPRM,17748
 qlib/workflow/online/strategy.py,sha256=ZIjV4gUpyyzvwhcHKFcmc1crvjfI9VjXoAabxTLL_ao,8620
 qlib/workflow/online/update.py,sha256=qL0iR4BVK597yMWKZjG9RtWqah88ysBOqji27_b9ITs,10884
 qlib/workflow/online/utils.py,sha256=AmvMTq7BlzoioPs8UVZF8RbyzebQ_RA_U3AIGoMeIOs,6662
 qlib/workflow/task/__init__.py,sha256=7DnkUttb90Qmrl6CT2Usad5SUd-vicF0KIDDj5_fV6Y,548
 qlib/workflow/task/collect.py,sha256=XFWBC1LTPj90jGP8ffcmqYqZJAR2vsp7b9PvjSGMRdo,10315
 qlib/workflow/task/gen.py,sha256=JOPngqTj7MkkhF8BtoXre8RvgVz5Mom-ya9Sc2GnPB4,12119
 qlib/workflow/task/manage.py,sha256=W1VLYwz6jIrK5AIpTcHF0mmU5PjCxSJnH5esEPUTgcw,18968
 qlib/workflow/task/utils.py,sha256=cjzzA9PhzpERtqBUUDhF4Aj8ahtrSM9UGS3oZ-DlQNM,8890
-pyqlib-0.9.1.dist-info/LICENSE,sha256=mQaUD2Gx8LUz-n2ZuvVReLKAj74RPqUd-_rYVyzNXys,1162
-pyqlib-0.9.1.dist-info/METADATA,sha256=Yr8S6fg5QRSyj91fOjD05DOWyhMXhmduHj-6A_gAGCY,38902
-pyqlib-0.9.1.dist-info/WHEEL,sha256=b_PH-i_F2xFYDXcROE5vpDbWUcY020I0eLFvEIYA9Pc,100
-pyqlib-0.9.1.dist-info/entry_points.txt,sha256=dwunoTIpn4pHpIdn84W_tcL4QhuvJu2Ui-fnfa540uQ,48
-pyqlib-0.9.1.dist-info/top_level.txt,sha256=NEJ4tcoKeCgATGsti9esDbItwLBkgN8elNBsfGExUlE,5
-pyqlib-0.9.1.dist-info/RECORD,,
+pyqlib-0.9.2.dist-info/LICENSE,sha256=mQaUD2Gx8LUz-n2ZuvVReLKAj74RPqUd-_rYVyzNXys,1162
+pyqlib-0.9.2.dist-info/METADATA,sha256=GcxzvhjXFZncF_c9MYP3g_hUDmzkyZJL_uVCf3J4dmY,39966
+pyqlib-0.9.2.dist-info/WHEEL,sha256=F2aDWtWrilNM3-px1vBtJOEd7aRVTetQfFuDYmYbAUQ,100
+pyqlib-0.9.2.dist-info/entry_points.txt,sha256=dwunoTIpn4pHpIdn84W_tcL4QhuvJu2Ui-fnfa540uQ,48
+pyqlib-0.9.2.dist-info/top_level.txt,sha256=NEJ4tcoKeCgATGsti9esDbItwLBkgN8elNBsfGExUlE,5
+pyqlib-0.9.2.dist-info/RECORD,,
```

